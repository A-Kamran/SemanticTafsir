{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "164cc3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.resources import path\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "from rdflib import Graph\n",
    "from shutil import copy\n",
    "import xml.etree.ElementTree as ET\n",
    "from matplotlib import lines\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pyarabic.araby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d6dad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S000A MuqadimaA\\\\sure_0_section.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S001M\\\\sure_1_section_1.1-1.14.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.1-2.42.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.113-2.165.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.166-2.210.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.211-2.263.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.264-2.297.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.298-2.344.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.345-2.395.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.396-2.439.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.43-2.77.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.440-2.482.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.483-2.498.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.499-2.523.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.524-2.532.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.533-2.548.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.549-2.582.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.583-2.613.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.614-2.626.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.627-2.648.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.649-2.672.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.673-2.685.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.686-2.717.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.718-2.750.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.751-2.797.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.78-2.112.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.798-2.830.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.1-3.48.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.139-3.171.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.172-3.210.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.211-3.249.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.250-3.290.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.291-3.301.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.49-3.95.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.96-3.138.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.1-4.27.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.115-4.148.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.149-4.197.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.198-4.208.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.209-4.241.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.242-4.277.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.278-4.302.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.28-4.57.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.58-4.84.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.85-4.114.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.1-5.35.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.106-5.125.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.126-5.137.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.138-5.178.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.179-5.205.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.206-5.224.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.225-5.256.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.36-5.52.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.53-5.105.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S006N\\\\sure_6_section_6.1-6.63.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S006N\\\\sure_6_section_6.101-6.144.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S006N\\\\sure_6_section_6.145-6.199.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S006N\\\\sure_6_section_6.200-6.234.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S006N\\\\sure_6_section_6.235-6.241.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S006N\\\\sure_6_section_6.64-6.100.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S007A\\\\sure_7_section_7.1-7.48.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S007A\\\\sure_7_section_7.151-7.191.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S007A\\\\sure_7_section_7.192-7.232.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S007A\\\\sure_7_section_7.233-7.235.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S007A\\\\sure_7_section_7.49-7.89.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S007A\\\\sure_7_section_7.90-7.150.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S008CatA\\\\sure_8_section_8.1-8.26.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S008CatA\\\\sure_8_section_8.27-8.56.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S008CatA\\\\sure_8_section_8.57-8.83.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S009CatA\\\\sure_9_section_9.1-9.28.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S009CatA\\\\sure_9_section_9.116-9.131.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S009CatA\\\\sure_9_section_9.29-9.62.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S009CatA\\\\sure_9_section_9.63-9.92.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S009CatA\\\\sure_9_section_9.93-9.115.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S010M\\\\sure_10_section_10.1-10.68.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S010M\\\\sure_10_section_10.69-10.108.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S011CatA\\\\sure_11_section_11.1-11.42.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S011CatA\\\\sure_11_section_11.43-11.81.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S011CatA\\\\sure_11_section_11.82-11.112.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S012CatA\\\\sure_12_section_12.1-12.22.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S012CatA\\\\sure_12_section_12.23-12.60.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S012CatA\\\\sure_12_section_12.61-12.84.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S012CatA\\\\sure_12_section_12.85-12.91.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S013M\\\\sure_13_section_13.1-13.21.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S013M\\\\sure_13_section_13.22-13.39.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S014M\\\\sure_14_section_14.1-14.34.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S014M\\\\sure_14_section_14.35-14.44.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S015M\\\\sure_15_section_15.1-15.44.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S015M\\\\sure_15_section_15.45-15.51.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S016A\\\\sure_16_section_16.1-16.61.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S016A\\\\sure_16_section_16.112-16.112.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S016A\\\\sure_16_section_16.62-16.111.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S017N\\\\sure_17_section_17.1-17.13.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S017N\\\\sure_17_section_17.14-17.53.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S017N\\\\sure_17_section_17.54-17.86.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S017N\\\\sure_17_section_17.87-17.102.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S018CatA\\\\sure_18_section_18.1-18.26.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S018CatA\\\\sure_18_section_18.27-18.64.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S018CatA\\\\sure_18_section_18.65-18.77.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S019N\\\\sure_19_section_19.1-19.38.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S019N\\\\sure_19_section_19.39-19.67.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S020CatA\\\\sure_20_section_20.1-20.42.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S020CatA\\\\sure_20_section_20.43-20.71.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S021CatA\\\\sure_21_section_21.1-21.61.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S021CatA\\\\sure_21_section_21.62-21.85.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S022CatA\\\\sure_22_section_22.1-22.27.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S022CatA\\\\sure_22_section_22.28-22.63.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S023CatN\\\\sure_23_section_23.1-23.67.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S024CatA\\\\sure_22_section_22.24-22.63.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S024CatA\\\\sure_24_section_24.1-24.27.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S024CatA\\\\sure_24_section_24.28-24.52.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S024CatA\\\\sure_24_section_24.53-22.23.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S025N\\\\sure_25_section_25.1-25.46.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S025N\\\\sure_25_section_25.47-25.52.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S026N\\\\sure_26_section_26.1-26.67.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S026N\\\\sure_26_section_26.68-26.69.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S027N\\\\sure_27_section_27.1-27.50.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S027N\\\\sure_27_section_27.51-27.56.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S028N\\\\sure_28_section_28.1-28.38.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S028N\\\\sure_28_section_28.39-28.73.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S029M\\\\sure_29_section_29.1-29.60.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S030M\\\\sure_30_section_30.1-30.50.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S031M\\\\sure_31_section_31.1-31.28.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S032M\\\\sure_32_section_32.1-32.21.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S033N\\\\sure_33_section_33.1-33.26.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S033N\\\\sure_33_section_33.27-33.51.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S034N\\\\sure_34_section_34.1-34.45.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S035N\\\\sure_35_section_35.1-35.31.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S036N\\\\sure_36_section_36.1-36.39.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S037CatA\\\\sure_37_section_37.1-37.32.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S037CatA\\\\sure_37_section_37.33-37.47.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S038A\\\\sure_38_section_38.1-38.23.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S038A\\\\sure_38_section_38.24-38.34.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S039A\\\\sure_39_section_39.1-39.51.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S040A\\\\sure_40_section_40.1-40.53.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S041CatN\\\\sure_41_section_41.1-41.39.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S042A\\\\sure_42_section_42.1-42.36.xml',\n",
       " \"D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S043A\\\\sure_43_section_43.1-43.50 (Raqmi 333's conflicted copy 2022-06-07).xml\",\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S043A\\\\sure_43_section_43.1-43.50.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S044M\\\\sure_44_section_44.1-44.20.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S045M\\\\sure_45_section_45.1-45.30.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S046CatN\\\\sure_46_section_46.1-46.28.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S047M\\\\sure_47_section_47.1-47.24.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S048A\\\\sure_48_section_48.1-48.20.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S049CatN\\\\sure_49_section_49.1-49.16.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S050CatN\\\\sure_50_section_50.1-50.22.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S051A\\\\sure_51_section_51.1-51.25.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S052A\\\\sure_52_section_52.1-52.19.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S053AN\\\\sure_53_section_53.1-53.18.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S054A\\\\sure_54_section_54.1-54.20.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S055N\\\\sure_55_section_55.1-55.20.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S056N\\\\sure_56_section_56.1-56.20.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S057N\\\\sure_57_section_57.1-57.23.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S058N\\\\sure_58_section_58.1-58.20.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S059N\\\\sure_59_section_59.1-59.17.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S060A\\\\sure_60_section_60.1-60.12.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S061M\\\\sure_61_section_61.1-61.10.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S062M\\\\sure_62_section_62.1-62.10.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S063CatM\\\\sure_63_section_63.1-63.10.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S064M\\\\sure_64_section_64.1-64.14.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S065A\\\\sure_65_section_65.1-65.8.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S066A\\\\sure_66_section_66.1-66.12.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S067M\\\\sure_67_section_67.1-67.19.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S068N\\\\sure_68_section_68.1-68.19.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S069N\\\\sure_69_section_69.1-69.13.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S070N\\\\sure_70_section_70.1-70.11.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S071M\\\\sure_71_section_71.1-71.9.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S072CatN\\\\sure_72_section_72.1-72.11.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S073N\\\\sure_73_section_73.1-73.8.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S074N\\\\sure_74_section_74.1-74.10.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S074N\\\\TT_s074_div010.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S075N\\\\sure_75_section_75.1-75.8.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S076M\\\\sure_76_section_76.1-76.14.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S077N\\\\sure_77_section_77.1-77.10.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S078N\\\\sure_78_section_78.1-78.9.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S079N\\\\sure_79_section_79.1-79.9.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S080M\\\\sure_80_section_80.1-80.6.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S081M\\\\sure_81_section_81.1-81.6.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S082M\\\\sure_82_section_82.1-82.4.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S083M\\\\sure_83_section_83.1-83.9.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S084M\\\\sure_84_section_84.1-84.5.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S085CatM\\\\sure_85_section_85.1-85.6.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S086M\\\\sure_86_section_86.1-86.2.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S087M\\\\sure_87_section_87.1-87.3.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S088M\\\\sure_88_section_88.1-88.4.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S089CatN\\\\sure_89_section_89.1-89.6.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S090A\\\\sure_90_section_90.1-90.3.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S091M\\\\sure_91_section_91.1-91.2.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S092CatM\\\\sure_92_section_92.1-92.3.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S093M\\\\sure_93_section_93.1-93.2.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S094M\\\\sure_94_section_94.1-94.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S095M\\\\sure_95_section_95.1-95.2.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S096M\\\\sure_96_section_96.1-96.5.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S097M\\\\sure_97_section_97.1-97.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S098M\\\\sure_98_section_98.1-98.4.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S099M\\\\sure_99_section_99.1-99.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S100M\\\\sure_100_section_100.1-100.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S101M\\\\sure_101_section_101.1-101.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S102M\\\\sure_102_section_102.1-102.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S103CatM\\\\sure_103_section_103.1-103.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S104CatM\\\\sure_104_section_104.1-104.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S105CatM\\\\sure_105_section_105.1-105.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S106CatM\\\\sure_106_section_106.1-106.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S107M\\\\sure_107_section_107.1-107.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S108NM\\\\sure_108_section_108.1-108.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S109CatM\\\\sure_109_section_109.1-109.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S110M\\\\sure_110_section_110.1-110.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S111CatM\\\\sure_111_section_111.1-111.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S112CatM\\\\sure_112_section_112.1-112.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S113CatM\\\\sure_113_section_113.1-113.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S114٘M\\\\sure_114_section_114.1-114.1.xml']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\**\\\\*.xml', recursive = True)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77ead6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sections(inputfile):\n",
    "    import xml.etree.ElementTree as ET\n",
    "    with open(inputfile, 'r',encoding='utf8') as f:\n",
    "        xml_file_as_string = f.read()\n",
    "    ET.register_namespace(\"\", \"http://www.tei-c.org/ns/1.0\")\n",
    "    xml_root = ET.fromstring(xml_file_as_string)\n",
    "    ns = xml_root.tag\n",
    "    ns = ns[:ns.rfind('}') + 1]\n",
    "    ana_tags_statistics = dict()\n",
    "    # text/body/div/p/ana\n",
    "    sections_list = []\n",
    "    try:\n",
    "        for paragraph_node in xml_root.find(ns + \"text\").find(ns + \"body\").iter(ns + \"div\"):\n",
    "            # for div_node in paragraph_node.findall(ns+\"p\"):\n",
    "            paragraph_type_attrib = paragraph_node.get(\"type\")\n",
    "            paragraph_n_attrib = paragraph_node.get(\"n\")\n",
    "\n",
    "            if paragraph_type_attrib == \"section\":\n",
    "                sections_list.append(paragraph_n_attrib)\n",
    "\n",
    "            if paragraph_type_attrib == \"chapter\" and \"section\" not in xml_file_as_string:\n",
    "                sections_list.append(paragraph_n_attrib)\n",
    "            # print(type(paragraph_n_attrib))\n",
    "    except:\n",
    "        try:\n",
    "            try:\n",
    "                for paragraph_node in xml_root.find(ns + \"text\").find(ns + \"body\").find(ns + \"div\").iter(ns + \"div\"):\n",
    "                    # for div_node in paragraph_node.findall(ns+\"p\"):\n",
    "                    paragraph_type_attrib = paragraph_node.get(\"type\")\n",
    "                    paragraph_n_attrib = paragraph_node.get(\"n\")\n",
    "\n",
    "                    if paragraph_type_attrib == \"section\":\n",
    "                        sections_list.append(paragraph_n_attrib)\n",
    "                    # print(type(paragraph_n_attrib))\n",
    "            except:\n",
    "                for paragraph_node in xml_root.find(ns + \"text\").find(ns + \"body\").iter(ns + \"div\"):\n",
    "                    # for div_node in paragraph_node.findall(ns+\"p\"):\n",
    "                    paragraph_type_attrib = paragraph_node.get(\"type\")\n",
    "                    paragraph_n_attrib = paragraph_node.get(\"n\")\n",
    "\n",
    "                    if paragraph_type_attrib == \"chapter\":\n",
    "                        print(\"found\")\n",
    "                        sections_list.append(paragraph_n_attrib)\n",
    "        except:\n",
    "            print(\"Error1\")\n",
    "\n",
    "    # print(sections_list[0],sections_list[-1])\n",
    "    # sure = sections_list[0]\n",
    "    # print(sure[0])\n",
    "    return sections_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "53086abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  extract_paragraphs_2(input_file, list_of_list_for_cvs):\n",
    "    #header = parse_paragraph_quran_n(input_file)\n",
    "    sura_number = sections(input_file)\n",
    "    with open(input_file, 'r',encoding='utf8') as f: \n",
    "        data = f.read() \n",
    "        Bs_data = BeautifulSoup(data, \"xml\") \n",
    "\n",
    "        # find all sub_sections\n",
    "        sub_sections = Bs_data.find_all('div')\n",
    "        sub_sections_liste = []\n",
    "        for i in sub_sections:\n",
    "            try:\n",
    "                an = (i['n'])\n",
    "                sub_sections_liste.append(an)\n",
    "            except: \n",
    "                #print(i)\n",
    "                sub_sections_liste.append(\"no ayat number\")\n",
    "                continue\n",
    "       # print(sub_sections_liste)\n",
    "        #sub_sections = list(sub_sections)\n",
    "        #print(sub_sections)\n",
    "\n",
    "        \n",
    "        list_of_list_paragraphs = []\n",
    "        list_of_list_topics = []\n",
    "        list_of_list_subtopics = []\n",
    "        list_of_list_xml_id = []\n",
    "        list_of_list_HadithNotHadith = []\n",
    "        list_of_list_ayat_numbers = []\n",
    "        list_of_list_rawis = []\n",
    "        list_of_list_sahabis = []\n",
    "        list_of_list_shaykhs = []\n",
    "        list_of_list_persons = []\n",
    "        list_of_list_locations = []\n",
    "        list_of_list_organizations = []\n",
    "        list_of_list_times = []\n",
    "        list_of_list_others = []\n",
    "        list_of_list_heads_30 = []\n",
    "        \n",
    "\n",
    "        # zahler \n",
    "        zaehler_topics = 0\n",
    "        zaehler_subtopics = 0\n",
    "\n",
    "\n",
    "        previous_ayat = \"\"\n",
    "\n",
    "\n",
    "\n",
    "        # delte double sub_sections\n",
    "        '''\n",
    "        list_of_sub_sections_reverse = sub_sections[::-1]\n",
    "        list_of_subsections_clear = [] \n",
    "        liste_new = []\n",
    "        end_flag = 0\n",
    "        for element in list_of_sub_sections_reverse:\n",
    "            element_str = str(element)\n",
    "            check_line = element_str\n",
    "            for element_new in list_of_subsections_clear:\n",
    "                if check_line == str(element_new):\n",
    "                    end_flag = 1\n",
    "                    break\n",
    "            if end_flag == 1:\n",
    "                end_flag = 0\n",
    "                continue\n",
    "            else:\n",
    "                list_of_subsections_clear.append(element)\n",
    "        list_of_subsections_clear_reverse = list_of_subsections_clear[::-1]\n",
    "\n",
    "        #print(list_of_subsections_clear_reverse)\n",
    "        \n",
    "        #print(sub_sections)\n",
    "        #print(type(sub_sections))\n",
    "        '''\n",
    "\n",
    "        for i in sub_sections: #sub_sections\n",
    "            #print(\"Hallo\")\n",
    "            #print(i)\n",
    "\n",
    "            # head = 30\n",
    "            heads_30 = [] \n",
    "            a_numb = \"No Ayat Number\"\n",
    "            \n",
    "            heads_first = i.find_all('head', {'type' : '30'})\n",
    "            for head in heads_first:\n",
    "                ayat_number = head.find('quote')\n",
    "                try:\n",
    "                    a_numb = (ayat_number['n'])\n",
    "                    previous_ayat = a_numb\n",
    "                except:\n",
    "                    a_numb = \"No Ayat Number\"\n",
    "                    continue\n",
    "              \n",
    "            if a_numb == \"No Ayat Number\":\n",
    "                a_numb =  a_numb + \" (\" + previous_ayat + \")\"\n",
    "\n",
    "        \n",
    "            # paragraphs\n",
    "            paragraphs = [] \n",
    "            paragraphs_first = i.find_all('p')\n",
    "            for i in paragraphs_first:\n",
    "                if i == \"\\n\":\n",
    "                    paragraphs.append(\"No Text\") \n",
    "                else:\n",
    "                    paragraphs.append(i) \n",
    "            list_of_list_paragraphs.append(paragraphs)\n",
    "\n",
    "\n",
    "            # ayat numbers add to paragraph\n",
    "            for i in range(len(paragraphs)):\n",
    "                heads_30.append(a_numb)\n",
    "            list_of_list_heads_30.append(heads_30)\n",
    "\n",
    "\n",
    "\n",
    "            # topics\n",
    "            liste_topics = []\n",
    "            for i in paragraphs:\n",
    "                try:\n",
    "                    a = (i['ana'])\n",
    "                    zaehler_topics = zaehler_topics + 1\n",
    "                    liste_topics.append(a)\n",
    "                except: \n",
    "                    #print(i)\n",
    "                    liste_topics.append(\"no topic\")\n",
    "                    continue\n",
    "            list_of_list_topics.append(liste_topics)\n",
    "\n",
    "            # subtopics\n",
    "            subtopics = []\n",
    "            subtopics_of_paragraph = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_subtopics = i.find_all('seg')\n",
    "                for a in paragraph_subtopics:\n",
    "                    try:\n",
    "                        s = (a['ana'])\n",
    "                        zaehler_subtopics = zaehler_subtopics + 1\n",
    "                        subtopics_of_paragraph.append(s)\n",
    "                    except: \n",
    "                        subtopics_of_paragraph.append(\"no subtopic\")\n",
    "                    continue\n",
    "                subtopics.append(subtopics_of_paragraph)\n",
    "                subtopics_of_paragraph = []\n",
    "            list_of_list_subtopics.append(subtopics)\n",
    "            #print(list_of_list_subtopics)\n",
    "\n",
    "            # ayat numbers in paragraph\n",
    "            ayat_numbers = []\n",
    "            ayat_numbers_of_paragraph = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_ayat_numbers = i.find_all('quote', {'type': 'quran'})\n",
    "                for a in paragraph_ayat_numbers:\n",
    "                    try:\n",
    "                        s = (a['n'])\n",
    "                        ayat_numbers_of_paragraph.append(s)\n",
    "                    except: \n",
    "                        ayat_numbers_of_paragraph.append(\"no ayat number\")\n",
    "                    continue\n",
    "                ayat_numbers.append(ayat_numbers_of_paragraph)\n",
    "                ayat_numbers_of_paragraph = []\n",
    "            list_of_list_ayat_numbers.append(ayat_numbers)\n",
    "\n",
    "\n",
    "            # shaykh\n",
    "            shaykhs_of_paragraph = []\n",
    "            shaykhs = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_shaykhs = i.find_all('persName', {'ana' : 'shaykh'} )\n",
    "                shaykhs_of_paragraph = []\n",
    "                for a in paragraph_shaykhs:\n",
    "                    if paragraph_shaykhs == []:\n",
    "                        shaykhs_of_paragraph.append(\"no shaykh\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        shaykhs_of_paragraph.append(a)\n",
    "                shaykhs.append(shaykhs_of_paragraph)\n",
    "            list_of_list_shaykhs.append(shaykhs)\n",
    "\n",
    "\n",
    "            # rawi \n",
    "            rawis_of_paragraph = []\n",
    "            rawis = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_rawis = i.find_all('persName', {'ana' : 'rawi'} )\n",
    "                rawis_of_paragraph = []\n",
    "                for a in paragraph_rawis:\n",
    "                    if paragraph_rawis == []:\n",
    "                        rawis_of_paragraph.append(\"no rawi\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        rawis_of_paragraph.append(a)\n",
    "                rawis.append(rawis_of_paragraph)\n",
    "            list_of_list_rawis.append(rawis)\n",
    "        \n",
    "\n",
    "\n",
    "            # sahabi\n",
    "            sahabis_of_paragraph = []\n",
    "            sahabis = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_sahabis = i.find_all('persName', {'ana' : 'sahabi'} )\n",
    "                sahabis_of_paragraph = []\n",
    "                for a in paragraph_sahabis:\n",
    "                    if paragraph_sahabis == []:\n",
    "                        sahabis_of_paragraph.append(\"no sahabi\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        sahabis_of_paragraph.append(a)\n",
    "                sahabis.append(sahabis_of_paragraph)\n",
    "            list_of_list_sahabis.append(sahabis)\n",
    "\n",
    "\n",
    "            # NER person\n",
    "            persons_of_paragraph = []\n",
    "            persons = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_persons = i.find_all('name', {'role' : 'person'} )\n",
    "                #print(paragraph_persons)\n",
    "                persons_of_paragraph = []\n",
    "                for a in paragraph_persons:\n",
    "                    if paragraph_persons == []:\n",
    "                        persons_of_paragraph.append(\"no person\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        persons_of_paragraph.append(a)\n",
    "                #zaehler_person = zaehler_person + len(persons_of_paragraph)\n",
    "                persons.append(persons_of_paragraph)\n",
    "            list_of_list_persons.append(persons)\n",
    "            #print(list_of_list_persons)\n",
    "            \n",
    "\n",
    "\n",
    "            # NER location\n",
    "            locations_of_paragraph = []\n",
    "            locations = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_locations = i.find_all('name', {'role' : 'location'} )\n",
    "                locations_of_paragraph = []\n",
    "                for a in paragraph_locations:\n",
    "                    if paragraph_locations == []:\n",
    "                        locations_of_paragraph.append(\"no location\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        locations_of_paragraph.append(a)\n",
    "                locations.append(locations_of_paragraph)\n",
    "            list_of_list_locations.append(locations)\n",
    "            \n",
    "\n",
    "            # NER organization\n",
    "            organizations_of_paragraph = []\n",
    "            organizations = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_organizations = i.find_all('name', {'role' : 'organization'} )\n",
    "                organizations_of_paragraph = []\n",
    "                for a in paragraph_organizations:\n",
    "                    if paragraph_organizations == []:\n",
    "                        organizations_of_paragraph.append(\"no organization\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        organizations_of_paragraph.append(a)\n",
    "                organizations.append(organizations_of_paragraph)\n",
    "            list_of_list_organizations.append(organizations)\n",
    "            \n",
    "\n",
    "\n",
    "            # NER time\n",
    "            times_of_paragraph = []\n",
    "            times = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_times = i.find_all('name', {'role' : 'time'} )\n",
    "                times_of_paragraph = []\n",
    "                for a in paragraph_times:\n",
    "                    if paragraph_times == []:\n",
    "                        times_of_paragraph.append(\"no times\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        times_of_paragraph.append(a)\n",
    "                times.append(times_of_paragraph)\n",
    "            list_of_list_times.append(times)\n",
    "            \n",
    "\n",
    "            # NER other\n",
    "            others_of_paragraph = []\n",
    "            others = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_others = i.find_all('name', {'role' : 'other'} )\n",
    "                others_of_paragraph = []\n",
    "                for a in paragraph_others:\n",
    "                    if paragraph_others == []:\n",
    "                        others_of_paragraph.append(\"no others\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        others_of_paragraph.append(a)\n",
    "                others.append(others_of_paragraph)\n",
    "            list_of_list_others.append(others)\n",
    "            #print(list_of_list_others)\n",
    "\n",
    "\n",
    "            # xml:id\n",
    "            xml_id = []\n",
    "            for i in paragraphs:\n",
    "                try:\n",
    "                    b = i['xml:id']\n",
    "                    xml_id.append(b)\n",
    "                except: \n",
    "                    #print(i)\n",
    "                    xml_id.append(\"no paragraph id\")\n",
    "                    continue\n",
    "            list_of_list_xml_id.append(xml_id)\n",
    "            #print(list_of_list_xml_id)\n",
    "\n",
    "            # hadith / nothadith\n",
    "            hadith_list = []\n",
    "            for i in paragraphs:\n",
    "                try:\n",
    "                    c = i['n']\n",
    "                    hadith_list.append(c)\n",
    "                except:\n",
    "                    hadith_list.append(\"Hadith Not Defined\")\n",
    "            list_of_list_HadithNotHadith.append(hadith_list)\n",
    "\n",
    "\n",
    "            \n",
    "                \n",
    "\n",
    "        #print(zaehler_person)\n",
    "        # sura number\n",
    "        #sura_number = Bs_data.find_all('div', {'type':'chapter' })\n",
    "        suraNumber = sura_number[0]\n",
    "        sura_number_list = suraNumber.split(\".\")\n",
    "        suraNumber = sura_number_list[0]\n",
    "        #print(suraNumber)\n",
    "\n",
    "\n",
    "       # print(len(sub_sections))\n",
    "        #print(len(list_of_list_paragraphs))\n",
    "        #print(len(list_of_list_topics))\n",
    "        #print(len(list_of_list_xml_id))\n",
    "        #print(len(list_of_list_rawis))\n",
    "\n",
    "\n",
    "        with open (\"file2\", \"a+\",encoding='utf8') as target:\n",
    "            \n",
    "            \n",
    "            for a in range(len(sub_sections_liste)):\n",
    "                paragraphs_li = list_of_list_paragraphs[a]\n",
    "                liste_topics_li = list_of_list_topics[a]\n",
    "                subtopics_li = list_of_list_subtopics[a]\n",
    "                xml_id_li = list_of_list_xml_id[a]\n",
    "                list_ofHadith_li = list_of_list_HadithNotHadith[a]\n",
    "                ayat_numbers_li = list_of_list_ayat_numbers[a]\n",
    "                rawis_li = list_of_list_rawis[a]\n",
    "                sahabis_li = list_of_list_sahabis[a]\n",
    "                shaykhs_li = list_of_list_shaykhs[a]\n",
    "                persons_li = list_of_list_persons[a]\n",
    "                locations_li = list_of_list_locations[a]\n",
    "                organisations_li = list_of_list_organizations[a]\n",
    "                times_li = list_of_list_times[a]\n",
    "                others_li = list_of_list_others[a]\n",
    "                ayat_heads_li = list_of_list_heads_30[a]\n",
    "\n",
    "                #row_cvs = []\n",
    "\n",
    "                for i in range(len(list_of_list_paragraphs[a])):\n",
    "                    row_cvs = []\n",
    "\n",
    "                    # output xml:id\n",
    "                    xmliD = str(xml_id_li[i])\n",
    "                    #print(xmliD)\n",
    "                    target.write(xmliD)\n",
    "                    target.write(\"\\n\")\n",
    "                    row_cvs.append(xmliD)\n",
    "\n",
    "                    # output paragraphes\n",
    "                    paragraph = str(paragraphs_li[i].text)\n",
    "                    if paragraph == \"\":\n",
    "                        paragraph = \"No Text\"\n",
    "                    paragraph = paragraph.replace('\\t', '')\n",
    "                    paragraph = paragraph.replace('\\n', ' ')\n",
    "                    paragraph = re.sub('\\s+',' ',paragraph)\n",
    "                    paragraph = paragraph.strip()\n",
    "                    target.write(paragraph)\n",
    "                    row_cvs.append(paragraph)\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "                    # (sub-)section:\n",
    "                    target.write(sub_sections_liste[a])\n",
    "                    row_cvs.append(sub_sections_liste[a])\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "                    # sura number:\n",
    "                    target.write(suraNumber)\n",
    "                    target.write(\"\\n\")\n",
    "                    row_cvs.append(suraNumber)\n",
    "\n",
    "                    # ayat number\n",
    "                    ayat_n = ayat_heads_li[i]\n",
    "                    \n",
    "                    print(ayat_n)\n",
    "                    target.write(ayat_n)\n",
    "                    row_cvs.append(ayat_n)\n",
    "                    \n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "                    # ayat numbers in paragraphs \n",
    "                    ayat = ayat_numbers_li[i]\n",
    "                    aya_row = []\n",
    "                    if ayat == []:\n",
    "                        aya = \"no aya\"\n",
    "                        target.write(aya)\n",
    "                        row_cvs.append(aya)\n",
    "                    else:\n",
    "                        ayat = set(ayat)\n",
    "                        for aya in ayat:\n",
    "                            aya = aya.replace(\"yes\", \"\")\n",
    "                            aya = aya.replace(\",\", \" \")\n",
    "                            if aya == \"\":\n",
    "                                aya = \"no aya\"\n",
    "                            target.write(aya)\n",
    "                            aya_row.append(aya+\" \")\n",
    "                            target.write(\" \")\n",
    "                        aya_row = set(aya_row)\n",
    "                        aya_row = list(aya_row)\n",
    "                        aya_row.sort()\n",
    "                        aya_row_string = \" \".join(aya_row)\n",
    "                        row_cvs.append(aya_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # output topics\n",
    "                    topics = str(liste_topics_li[i])\n",
    "                    topics = topics.replace(\"yes\", \"\")\n",
    "                    if topics == \"\":\n",
    "                            topics = \"no topic\"\n",
    "                    target.write(topics)\n",
    "                    row_cvs.append(topics)\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "                    # output subtopics\n",
    "                    subt = subtopics_li[i]\n",
    "                    subtopic_row = []\n",
    "                    if subt == []:\n",
    "                        subtopic = \"no subtopic\"\n",
    "                        target.write(subtopic)\n",
    "                        row_cvs.append(subtopic)\n",
    "                    else:\n",
    "                        subt = set(subt)\n",
    "                        for subtopic in subt:\n",
    "                            subtopic = subtopic.replace(\"yes\", \"\")\n",
    "                            subtopic = subtopic.replace(\",\", \" \")\n",
    "                            if subtopic == \"\":\n",
    "                                subtopic = \"no subtopic\"\n",
    "                            target.write(subtopic)\n",
    "                            subtopic_row.append(subtopic+\",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        subtopic_row = set(subtopic_row)\n",
    "                        subtopic_row = list(subtopic_row)\n",
    "                        subtopic_row.sort()\n",
    "                        subtopic_row_string = \" \".join(subtopic_row)\n",
    "                        row_cvs.append(subtopic_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "                    # output sahabi\n",
    "                    sah = sahabis_li[i]\n",
    "                    #print(sah)\n",
    "                    sahabi_row = []\n",
    "                    if sah == []:\n",
    "                        sahabi = \"no sahabi\"\n",
    "                        target.write(sahabi)\n",
    "                        row_cvs.append(sahabi)\n",
    "                    else:\n",
    "                        #sah = set(sah)\n",
    "                        for sahabi in sah:\n",
    "                            #print(sahabi)\n",
    "                            #rawi = rawi.replace(\"yes\", \"\")\n",
    "                            #rawi = rawi.replace(\",\", \" \")\n",
    "                            if sahabi == \"\":\n",
    "                                sahabi = \"no sahabi\"\n",
    "                            target.write(sahabi)\n",
    "                            sahabi_row.append(sahabi+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        #sahabi_row = set(sahabi_row)\n",
    "                        #sahabi_row = list(sahabi_row)\n",
    "                        #sahabi_row.sort()\n",
    "                        sahabi_row_string = \" \".join(sahabi_row)\n",
    "                        row_cvs.append(sahabi_row_string)\n",
    "                        #print(sahabi_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "                    # output rawis\n",
    "                    raw = rawis_li[i]\n",
    "                    rawi_row = []\n",
    "                    if raw == []:\n",
    "                        rawi = \"no rawi\"\n",
    "                        target.write(rawi)\n",
    "                        row_cvs.append(rawi)\n",
    "                    else:\n",
    "                        #raw = set(raw)\n",
    "                        for rawi in raw:\n",
    "                            #rawi = rawi.replace(\"yes\", \"\")\n",
    "                            #rawi = rawi.replace(\",\", \" \")\n",
    "                            if rawi == \"\":\n",
    "                                rawi = \"no rawi\"\n",
    "                            target.write(rawi)\n",
    "                            rawi_row.append(rawi+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        #rawi_row = set(rawi_row)\n",
    "                        #rawi_row = list(rawi_row)\n",
    "                        #rawi_row.sort()\n",
    "                        rawi_row_string = \" \".join(rawi_row)\n",
    "                        row_cvs.append(rawi_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # output shaykhs\n",
    "                    sahay = shaykhs_li[i]\n",
    "                    #print(sah)\n",
    "                    shaykh_row = []\n",
    "                    if sahay == []:\n",
    "                        shaykh = \"no shaykh\"\n",
    "                        target.write(shaykh)\n",
    "                        row_cvs.append(shaykh)\n",
    "                    else:\n",
    "                        for shaykh in sahay:\n",
    "                            if shaykh == \"\":\n",
    "                                shaykh = \"no shaykh\"\n",
    "                            target.write(shaykh)\n",
    "                            shaykh_row.append(shaykh+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        shaykh_row_string = \" \".join(shaykh_row)\n",
    "                        row_cvs.append(shaykh_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "                    # output Ner persons\n",
    "                    per = persons_li[i]\n",
    "                    person_row = []\n",
    "                    if per == []:\n",
    "                        person = \"no person\"\n",
    "                        target.write(person)\n",
    "                        row_cvs.append(person)\n",
    "                    else:\n",
    "                        for person in per:\n",
    "                            if person == \"\":\n",
    "                                person = \"no person\"\n",
    "                            target.write(person)\n",
    "                            person_row.append(person+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        person_row = set(person_row)\n",
    "                        person_row = list(person_row)\n",
    "                        person_row.sort()\n",
    "                        person_row_string = \" \".join(person_row)\n",
    "                        row_cvs.append(person_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "                    # output Ner locations\n",
    "                    loc = locations_li[i]\n",
    "                    location_row = []\n",
    "                    if loc == []:\n",
    "                        location = \"no location\"\n",
    "                        target.write(location)\n",
    "                        row_cvs.append(location)\n",
    "                    else:\n",
    "                        for location in loc:\n",
    "                            if location == \"\":\n",
    "                                location = \"no location\"\n",
    "                            target.write(location)\n",
    "                            location_row.append(location+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        location_row = set(location_row)\n",
    "                        location_row = list(location_row)\n",
    "                        location_row.sort()\n",
    "                        location_row_string = \" \".join(location_row)\n",
    "                        row_cvs.append(location_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "                    # output Ner organisations\n",
    "                    org = organisations_li[i]\n",
    "                    organisation_row = []\n",
    "                    if org == []:\n",
    "                        organisation = \"no organisation\"\n",
    "                        target.write(organisation)\n",
    "                        row_cvs.append(organisation)\n",
    "                    else:\n",
    "                        for organisation in org:\n",
    "                            if organisation == \"\":\n",
    "                                organisation = \"no organisation\"\n",
    "                            target.write(organisation)\n",
    "                            organisation_row.append(organisation+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        organisation_row = set(organisation_row)\n",
    "                        organisation_row = list(organisation_row)\n",
    "                        organisation_row.sort()\n",
    "                        organisation_row_string = \" \".join(organisation_row)\n",
    "                        row_cvs.append(organisation_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "                    # output Ner time\n",
    "                    tim = times_li[i]\n",
    "                    time_row = []\n",
    "                    if tim == []:\n",
    "                        time = \"no time\"\n",
    "                        target.write(time)\n",
    "                        row_cvs.append(time)\n",
    "                    else:\n",
    "                        for time in tim:\n",
    "                            if time == \"\":\n",
    "                                time = \"no time\"\n",
    "                            target.write(time)\n",
    "                            time_row.append(time+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        time_row = set(time_row)\n",
    "                        time_row = list(time_row)\n",
    "                        time_row.sort()\n",
    "                        time_row_string = \" \".join(time_row)\n",
    "                        row_cvs.append(time_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "                    # output Ner other\n",
    "                    oth = others_li[i]\n",
    "                    other_row = []\n",
    "                    if oth == []:\n",
    "                        other = \"no others\"\n",
    "                        target.write(other)\n",
    "                        row_cvs.append(other)\n",
    "                    else:\n",
    "                        for other in oth:\n",
    "                            if other == \"\":\n",
    "                                other = \"no others\"\n",
    "                            target.write(other)\n",
    "                            other_row.append(other+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        other_row = set(other_row)\n",
    "                        other_row = list(other_row)\n",
    "                        other_row.sort()\n",
    "                        other_row_string = \" \".join(other_row)\n",
    "                        row_cvs.append(other_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "                    # hadith / not Hadith\n",
    "                    hadith = str(list_ofHadith_li[i])\n",
    "                    if hadith == \"\":\n",
    "                        hadith = \"No Hadith definition\"\n",
    "                    hadith = hadith.replace('\\t', '')\n",
    "                    hadith = hadith.replace('\\n', ' ')\n",
    "                    hadith = re.sub('\\s+',' ',hadith)\n",
    "                    hadith = hadith.strip()\n",
    "                    target.write(hadith)\n",
    "                    row_cvs.append(hadith)\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "                    list_of_list_for_cvs.append(row_cvs)\n",
    "\n",
    "    # clear result/ delete double paragraphs\n",
    "    list_of_list_for_cvs_reverse = list_of_list_for_cvs[::-1]\n",
    "    list_of_list_for_cvs_clear = []\n",
    "    end_flag = 0\n",
    "    for liste in list_of_list_for_cvs_reverse:\n",
    "        if list_of_list_for_cvs_clear == []:\n",
    "            list_of_list_for_cvs_clear.append(liste)\n",
    "        else:\n",
    "            check_line = liste[1]\n",
    "            for liste_new in list_of_list_for_cvs_clear:\n",
    "                if check_line == liste_new[1]:\n",
    "                    end_flag = 1\n",
    "                    break\n",
    "            if end_flag == 1:\n",
    "                end_flag = 0\n",
    "                continue\n",
    "            else:\n",
    "                list_of_list_for_cvs_clear.append(liste)\n",
    "    list_of_list_for_cvs_clear_reverse = list_of_list_for_cvs_clear[::-1]\n",
    "\n",
    "    return list_of_list_for_cvs_clear_reverse, zaehler_topics, zaehler_subtopics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aeeb7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S100M\\\\sure_100_section_100.1-100.1.xml'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.index('D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S100M\\\\sure_100_section_100.1-100.1.xml')\n",
    "files[196]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868014b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(files[196], 'r', encoding='utf8') as f: \n",
    "    xml_file_as_string = f.read()\n",
    "Bs_data = BeautifulSoup(xml_file_as_string, \"xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c09e2907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_i = []\n",
    "# import re\n",
    "\n",
    "# tag = Bs_data.find_all(\"name\")\n",
    "# root = ET.fromstring(xml_file_as_string)\n",
    "# for i in tag:\n",
    "# #     print(i)\n",
    "#     O = i.get(\"role\")\n",
    "#     if O in \"organiztion\":\n",
    "# #         print(i)\n",
    "#         list_i.append(i)\n",
    "# #         xml_data = str(i)\n",
    "\n",
    "# s = set(list_i)\n",
    "# len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f450eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6de57ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_i = []\n",
    "import re\n",
    "\n",
    "tag = Bs_data.find_all(\"name\")\n",
    "root = ET.fromstring(xml_file_as_string)\n",
    "for i in tag:\n",
    "#     print(i)\n",
    "#     O = i.get(\"role\")\n",
    "#     if O == \"organization\":\n",
    "#         print(i)\n",
    "    list_i.append(i)\n",
    "#         xml_data = str(i)\n",
    "\n",
    "s = set(list_i)\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848a1a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<name role=\"location\">الكوفة</name>,\n",
       " <name role=\"location\">الْحِجْرِ</name>,\n",
       " <name role=\"location\">الْمُزْدَلِفَةِ</name>,\n",
       " <name role=\"location\">بَدْرٌ</name>,\n",
       " <name role=\"location\">سِقَايَةِ\n",
       " \t\t\t\t\t\t\t\tزَمْزَمَ</name>,\n",
       " <name role=\"location\">عَرَفَةَ</name>,\n",
       " <name role=\"location\">مُزْدَلِفَةَ</name>,\n",
       " <name role=\"location\">مِنًى</name>,\n",
       " <name role=\"organization\"><seg ana=\"kalamterm yes\">الْكُفَّارِ</seg></name>,\n",
       " <name role=\"organization\">أهل التأويل</name>,\n",
       " <name role=\"organization\">أهل العربية</name>,\n",
       " <name role=\"organization\">أَهْلُ\n",
       " \t\t\t\t\t\t\tالتَّأْوِيلِ</name>,\n",
       " <name role=\"organization\">أَهْلُ التَّأْوِيلِ</name>,\n",
       " <name role=\"organization\">البصريين</name>,\n",
       " <name role=\"organization\">عَرَبِ</name>,\n",
       " <name role=\"organization\">نحويي</name>,\n",
       " <name role=\"other\">سورة ( إبراهيم</name>,\n",
       " <name role=\"other\">سورة والعاديات</name>,\n",
       " <name role=\"person\"><seg ana=\"qari\">عَبْدِ اللَّهِ</seg></name>,\n",
       " <name role=\"person\">ابن عباس</name>,\n",
       " <name role=\"person\">ابْنَ عَبَّاسٍ</name>,\n",
       " <name role=\"person\">ابْنُ عَبَّاسٍ</name>,\n",
       " <name role=\"person\">ابْنِ\n",
       " \t\t\t\t\t\t\t\tعَبَّاسٍ</name>,\n",
       " <name role=\"person\">الأعشى</name>,\n",
       " <name role=\"person\">الْكَلْبِيُّ</name>,\n",
       " <name role=\"person\">بِشْرٍ</name>,\n",
       " <name role=\"person\">زَيْدُ بْنُ أَسْلَمَ</name>,\n",
       " <name role=\"person\">زُّبَيْ</name>,\n",
       " <name role=\"person\">سَعِيدٍ</name>,\n",
       " <name role=\"person\">طَرَفَةَ بْنِ\n",
       " \t\t\t\t\t\t\tالْعَبْدِ الْيَشْكُرِيُّ</name>,\n",
       " <name role=\"person\">عَلِيٌّ</name>,\n",
       " <name role=\"person\">كِنْدَةَ</name>,\n",
       " <name role=\"person\">كِنْدَةُ</name>,\n",
       " <name role=\"person\">مِقْدَادِ</name>,\n",
       " <name role=\"person\">يَزِيدَ</name>,\n",
       " <name role=\"time\"><seg ana=\"hajj yes\">الْحَجِّ</seg></name>,\n",
       " <name role=\"time\">اللَّيْلِ</name>,\n",
       " <name role=\"time\">صبحا</name>,\n",
       " <name role=\"time\">صُبْحًا</name>,\n",
       " <name role=\"time\">يَوْمَ النَّحْرِ</name>,\n",
       " <name type=\"sura\">والعاديات </name>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c01b301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_i = []\n",
    "import re\n",
    "a = ['']\n",
    "\n",
    "\n",
    "\n",
    "for xml_f in a:\n",
    "    # extracting info from xml\n",
    "    with open(xml_f, 'r', encoding='utf8') as f:     \n",
    "        xml_file_as_string = f.read()\n",
    "\n",
    "    Bs_data = BeautifulSoup(xml_file_as_string, \"xml\")\n",
    "    tag = Bs_data.find_all(\"head\")\n",
    "    root = ET.fromstring(xml_file_as_string)\n",
    "    for i in tag:\n",
    "    #     print(i)\n",
    "        O = i.get(\"quote\")\n",
    "        if O == \"quran\":\n",
    "            print(processText(removeDiacritics(str(i))))\n",
    "            print('\\n\\n')\n",
    "            list_i.append(processText(removeDiacritics(i.text)))\n",
    "\n",
    "s = set(list_i)\n",
    "s = list(s)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9f8ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(list_i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a48d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_numbers_and_slashes(text):\n",
    "    # Use regular expression to remove numbers and slashes\n",
    "    cleaned_text = re.sub(r'[0-9/-]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_leading_spaces_list(input_list):\n",
    "    return [item.strip() for item in input_list]\n",
    "\n",
    "# text processing function\n",
    "def removeDiacritics(text):\n",
    "    for a in pyarabic.araby.DIACRITICS:\n",
    "        text = text.replace(a, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def processText(text):\n",
    "    text = str(text)\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    while text.find('  ') != -1:\n",
    "        text = text.replace('  ', ' ')\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def removeSpace(text: str) -> str:\n",
    "    while text.find(' ') != -1:\n",
    "        text = text.replace(' ', '')\n",
    "\n",
    "    return text\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def lists_have_same_elements(list1, list2):\n",
    "    counter1 = Counter(list1)\n",
    "    counter2 = Counter(list2)\n",
    "    \n",
    "    return counter1 == counter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4b7cc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:003, t: خَلَقَ الْإِنْسَانَ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:004, t: عَلَّمَهُ الْبَيَانَ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:005, t: الشَّمْسُ وَالْقَمَرُ بِحُسْبَانٍ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:001, t: بِسْمِ اللَّهِ الرَّحْمَٰنِ الرَّحِيمِ الرَّحْمَٰنُ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:002, t: عَلَّمَ الْقُرْآنَ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:006, t: وَالنَّجْمُ وَالشَّجَرُ يَسْجُدَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:007, t: وَالسَّمَاءَ رَفَعَهَا وَوَضَعَ الْمِيزَانَ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:008, t: أَلَّا تَطْغَوْا فِي الْمِيزَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:009, t: وَأَقِيمُوا الْوَزْنَ بِالْقِسْطِ وَلَا تُخْسِرُوا الْمِيزَانَ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:010, t: وَالْأَرْضَ وَضَعَهَا لِلْأَنَامِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:011, t: فِيهَا فَاكِهَةٌ وَالنَّخْلُ ذَاتُ الْأَكْمَامِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:012, t: وَالْحَبُّ ذُو الْعَصْفِ وَالرَّيْحَانُ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:013, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:014, t: خَلَقَ الْإِنْسَانَ مِنْ صَلْصَالٍ كَالْفَخَّارِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:015, t: وَخَلَقَ الْجَانَّ مِنْ مَارِجٍ مِنْ نَارٍ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:016, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:017, t: رَبُّ الْمَشْرِقَيْنِ وَرَبُّ الْمَغْرِبَيْنِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:018, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:019, t: مَرَجَ الْبَحْرَيْنِ يَلْتَقِيَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:020, t: بَيْنَهُمَا بَرْزَخٌ لَا يَبْغِيَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:021, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:022, t: يَخْرُجُ مِنْهُمَا اللُّؤْلُؤُ وَالْمَرْجَانُ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:023, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:024, t: وَلَهُ الْجَوَارِ الْمُنْشَآتُ فِي الْبَحْرِ كَالْأَعْلَامِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:025, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:026, t: كُلُّ مَنْ عَلَيْهَا فَانٍ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:027, t: وَيَبْقَىٰ وَجْهُ رَبِّكَ ذُو الْجَلَالِ وَالْإِكْرَامِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:028, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:030, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:029, t: يَسْأَلُهُ مَنْ فِي السَّمَاوَاتِ وَالْأَرْضِ ۚ كُلَّ يَوْمٍ هُوَ فِي شَأْنٍ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:032, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:031, t: سَنَفْرُغُ لَكُمْ أَيُّهَ الثَّقَلَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:034, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:033, t: يَا مَعْشَرَ الْجِنِّ وَالْإِنْسِ إِنِ اسْتَطَعْتُمْ أَنْ تَنْفُذُوا مِنْ أَقْطَارِ السَّمَاوَاتِ وَالْأَرْضِ فَانْفُذُوا ۚ لَا تَنْفُذُونَ إِلَّا بِسُلْطَانٍ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:035, t: يُرْسَلُ عَلَيْكُمَا شُوَاظٌ مِنْ نَارٍ وَنُحَاسٌ فَلَا تَنْتَصِرَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:036, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:037, t: فَإِذَا انْشَقَّتِ السَّمَاءُ فَكَانَتْ وَرْدَةً كَالدِّهَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:038, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:039, t: فَيَوْمَئِذٍ لَا يُسْأَلُ عَنْ ذَنْبِهِ إِنْسٌ وَلَا جَانٌّ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:040, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:041, t: يُعْرَفُ الْمُجْرِمُونَ بِسِيمَاهُمْ فَيُؤْخَذُ بِالنَّوَاصِي وَالْأَقْدَامِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:042, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:043, t: هَٰذِهِ جَهَنَّمُ الَّتِي يُكَذِّبُ بِهَا الْمُجْرِمُونَ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:044, t: يَطُوفُونَ بَيْنَهَا وَبَيْنَ حَمِيمٍ آنٍ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:045, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:046, t: وَلِمَنْ خَافَ مَقَامَ رَبِّهِ جَنَّتَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:047, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:048, t: ذَوَاتَا أَفْنَانٍ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:049, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:050, t: فِيهِمَا عَيْنَانِ تَجْرِيَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:051, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:052, t: فِيهِمَا مِنْ كُلِّ فَاكِهَةٍ زَوْجَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:053, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:054, t: مُتَّكِئِينَ عَلَىٰ فُرُشٍ بَطَائِنُهَا مِنْ إِسْتَبْرَقٍ ۚ وَجَنَى الْجَنَّتَيْنِ دَانٍ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:055, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:056, t: فِيهِنَّ قَاصِرَاتُ الطَّرْفِ لَمْ يَطْمِثْهُنَّ إِنْسٌ قَبْلَهُمْ وَلَا جَانٌّ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:057, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:058, t: كَأَنَّهُنَّ الْيَاقُوتُ وَالْمَرْجَانُ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:059, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:060, t: هَلْ جَزَاءُ الْإِحْسَانِ إِلَّا الْإِحْسَانُ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:061, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:062, t: وَمِنْ دُونِهِمَا جَنَّتَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:063, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:064, t: مُدْهَامَّتَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:065, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:066, t: فِيهِمَا عَيْنَانِ نَضَّاخَتَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:067, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:068, t: فِيهِمَا فَاكِهَةٌ وَنَخْلٌ وَرُمَّانٌ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:069, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:070, t: فِيهِنَّ خَيْرَاتٌ حِسَانٌ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:071, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:072, t: حُورٌ مَقْصُورَاتٌ فِي الْخِيَامِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:073, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:074, t: لَمْ يَطْمِثْهُنَّ إِنْسٌ قَبْلَهُمْ وَلَا جَانٌّ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:075, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:076, t: مُتَّكِئِينَ عَلَىٰ رَفْرَفٍ خُضْرٍ وَعَبْقَرِيٍّ حِسَانٍ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:077, t: فَبِأَيِّ آلَاءِ رَبِّكُمَا تُكَذِّبَانِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:078, t: تَبَارَكَ اسْمُ رَبِّكَ ذِي الْجَلَالِ وَالْإِكْرَامِ\n"
     ]
    }
   ],
   "source": [
    "# querying from graph\n",
    "    \n",
    "# Create a new RDF graph\n",
    "graph = Graph()\n",
    "\n",
    "graph_file = 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\A-Box\\\\Tafsir Al-Tabari A-Box S55.owl'\n",
    "\n",
    "\n",
    "# Parse the RDF data from a file or URL\n",
    "graph.parse(graph_file)\n",
    "\n",
    "# Define the SPARQL query\n",
    "query = \"\"\"\n",
    "PREFIX : <http://www.tafsirtabari.com/ontology#>\n",
    "PREFIX W3:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "SELECT DISTINCT *\n",
    "WHERE {\n",
    "\n",
    " ?k rdf:type :Chapter.\n",
    "?k :containsVerse ?v.\n",
    "?v :hasText ?t.\n",
    "}\n",
    "\"\"\"\n",
    "R = []\n",
    "# Execute the SPARQL query\n",
    "results = graph.query(query)\n",
    "# Process the query results\n",
    "for row in results:\n",
    "    # Access the query variables\n",
    "    s = row[\"k\"]\n",
    "    n = row[\"v\"]\n",
    "    t = row[\"t\"]\n",
    "    \n",
    "    print(f\"s: {s}, n: {n}, t: {t}\")\n",
    "    R.append(str(t))\n",
    "\n",
    "hadiths_from_graph = list(set(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7973887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning of xml and graph\n",
    "\n",
    "#graph\n",
    "cleaned_hadiths_from_graph = []\n",
    "for i in hadiths_from_graph:\n",
    "    a = remove_numbers_and_slashes(i)\n",
    "    cleaned_hadiths_from_graph.append(removeDiacritics(a))\n",
    "\n",
    "# xml\n",
    "list_i_h = []\n",
    "for i in list_i:\n",
    "    filtered_list = remove_numbers_and_slashes(i)\n",
    "    list_i_h.append(filtered_list)\n",
    "\n",
    "# removing leading spaces\n",
    "Hadiths_from_xml = remove_leading_spaces_list(list_i_h)\n",
    "\n",
    "cleaned_hadiths_from_graph.sort()\n",
    "Hadiths_from_xml.sort()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4d580e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ألا تطغوا في الميزان\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# manual comparing\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_hadiths_from_graph[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mHadiths_from_xml\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_hadiths_from_graph[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m Hadiths_from_xml[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# manual comparing\n",
    "print(cleaned_hadiths_from_graph[0])\n",
    "print(Hadiths_from_xml[1])\n",
    "print(cleaned_hadiths_from_graph[0] == Hadiths_from_xml[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9cdf28b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(cleaned_hadiths_from_graph),len(Hadiths_from_xml))\n",
    "lists_have_same_elements(cleaned_hadiths_from_graph,Hadiths_from_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ea6d7d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "إن كاد ليضلنا عن آلهتنا لولا أن صبرنا عليها  وسوف يعلمون حين يرون العذاب من أضل سبيلا\n",
      "----------------\n",
      "حنت إلي النخلة القصوى فقلت لها حجر حرام ألا تلك الدهاريس\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_hadiths_from_graph[8])\n",
    "print('----------------')\n",
    "print(Hadiths_from_xml[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b84369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d37a2033",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [80]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ff \u001b[38;5;241m=\u001b[39m \u001b[43mcleaned_hadiths_from_graph\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m295\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m Hadiths_from_xml:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ff:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ff = cleaned_hadiths_from_graph[295]\n",
    "for i in Hadiths_from_xml:\n",
    "    if i in ff:\n",
    "        print('here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8ad2d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "f = []\n",
    "for index,i in enumerate(cleaned_hadiths_from_graph):\n",
    "    for j in Hadiths_from_xml:\n",
    "        if i == j:\n",
    "            c+=1\n",
    "            f.append(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3643441a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "298"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(set(f))\n",
    "print(len(set(f)))\n",
    "print(c)\n",
    "298 - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e1b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = str(s[3])\n",
    "# print(a)\n",
    "# A = removeDiacritics(a)\n",
    "# B  = processText(A)\n",
    "# regex pattern to get the text inside tags\n",
    "pattern = \"<[^>]*>(.*?)<\\/[^>]*>\"\n",
    "count = 0\n",
    "Nat = []\n",
    "for i in s:\n",
    "    T = processText(i)\n",
    "    T = removeDiacritics(T)\n",
    "#     T = removeSpace(T)\n",
    "#     print(T)\n",
    "    Nat.append(T)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67caa712",
   "metadata": {},
   "source": [
    "### Graph Validation Functions\n",
    "1. Chapters (suras)\n",
    "2. Verses\n",
    "3. Commentary Sections \n",
    "4. Hadiths\n",
    "5. Not Hadiths\n",
    "6. narrator\n",
    "7. Topic/ SubTopic\n",
    "8. Person,Location,Organization & Time\n",
    "9. Verses\n",
    "10. Poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d5287",
   "metadata": {},
   "source": [
    "# Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text processing function\n",
    "def removeDiacritics(text):\n",
    "    for a in pyarabic.araby.DIACRITICS:\n",
    "        text = text.replace(a, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def processText(text):\n",
    "    text = str(text)\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    while text.find('  ') != -1:\n",
    "        text = text.replace('  ', ' ')\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def removeSpace(text: str) -> str:\n",
    "    while text.find(' ') != -1:\n",
    "        text = text.replace(' ', '')\n",
    "\n",
    "    return text\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def lists_have_same_elements(list1, list2):\n",
    "    counter1 = Counter(list1)\n",
    "    counter2 = Counter(list2)\n",
    "    \n",
    "    return counter1 == counter2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d79509d",
   "metadata": {},
   "source": [
    "## narrator validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe72d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validate_Narrator(graph_file,xml_file,nat_type):\n",
    "    \n",
    "    # Extracting Graph info (Narrators)\n",
    "    \n",
    "    # Create a new RDF graph\n",
    "    graph = Graph()\n",
    "\n",
    "    # Parse the RDF data from a file or URL\n",
    "    graph.parse(graph_file)\n",
    "\n",
    "    # Define the SPARQL query\n",
    "    query = f\"\"\"\n",
    "    PREFIX : <http://www.tafsirtabari.com/ontology#>\n",
    "    PREFIX W3:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "    SELECT DISTINCT *\n",
    "    WHERE {{\n",
    "      {{\n",
    "      ?Person rdf:type :Narrator.\n",
    "        ?Person :hasName ?name.\n",
    "      ?Person :hasNarratorType :{nat_type}.\n",
    "      }}\n",
    "      union\n",
    "      {{\n",
    "          ?Person rdf:type :RootNarrator.\n",
    "        ?Person :hasName ?name.\n",
    "      ?Person :hasNarratorType :{nat_type}.\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    R = []\n",
    "    # Execute the SPARQL query\n",
    "    results = graph.query(query)\n",
    "    # Process the query results\n",
    "    for row in results:\n",
    "        # Access the query variables\n",
    "        person = row[\"Person\"]\n",
    "        name = row[\"name\"]\n",
    "#         print(f\"Person: {person}, Name: {name}\")\n",
    "        R.append(str(name))\n",
    "\n",
    "    narrators_from_graph = list(set(R))\n",
    "    list_i = []\n",
    "\n",
    "    for xml_f in xml_file:\n",
    "        # extracting info from xml\n",
    "        with open(xml_f, 'r', encoding='utf8') as f:     \n",
    "            xml_file_as_string = f.read()\n",
    "\n",
    "        Bs_data = BeautifulSoup(xml_file_as_string, \"xml\")\n",
    "\n",
    "        tag = Bs_data.find_all(\"persName\")\n",
    "        root = ET.fromstring(xml_file_as_string)\n",
    "        for i in tag:\n",
    "        #     print(i)\n",
    "            O = i.get(\"ana\")\n",
    "            if O == nat_type: # set narrator type\n",
    "    #             print(i)\n",
    "                list_i.append(i)\n",
    "\n",
    "    s = set(list_i)\n",
    "    S = list(s)\n",
    "        \n",
    "    # regex pattern to get the text inside tags\n",
    "    pattern = \"<[^>]*>(.*?)<\\/[^>]*>\"\n",
    "    count = 0\n",
    "    Nat = []\n",
    "    for i in S:\n",
    "        T = processText(i)\n",
    "        T = removeDiacritics(T)\n",
    "    #     T = removeSpace(T)\n",
    "        output = re.findall(pattern, T)\n",
    "    #     print(output)\n",
    "        if len(output) != 0:\n",
    "            count+=1\n",
    "#             print(output[0])\n",
    "            Nat.append(output[0])\n",
    "        else:\n",
    "            print(\"something wrong \",i)\n",
    "    \n",
    "    \n",
    "    narrators_from_xml = list(set(Nat))\n",
    "\n",
    "    if lists_have_same_elements(narrators_from_graph, narrators_from_xml):\n",
    "        print(\"The lists have the same elements.\")\n",
    "        print(\"Narrators test passed! Validated! ✅ \")\n",
    "    else:\n",
    "        print(\"The lists do not have the same elements.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d87ab1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lists have the same elements.\n",
      "Narrators test passed! Validated! ✅ \n"
     ]
    }
   ],
   "source": [
    "a = ['D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\Dataset\\\\S025N\\\\sure_25_section_25.1-25.46.xml','D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\Dataset\\\\S025N\\\\sure_25_section_25.47-25.52.xml']\n",
    "# g_file = \"D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-9f4906f3913e4c1e2353d282ee31f7b4aafcab4b\\\\A-Box\\\\Tafsir Al-Tabari A-Box S100.owl\"\n",
    "g_file = 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-9f4906f3913e4c1e2353d282ee31f7b4aafcab4b\\\\A-Box\\\\Tafsir Al-Tabari A-Box S25.owl'\n",
    "\n",
    "xml_file = a\n",
    "\n",
    "Validate_Narrator(g_file,xml_file,\"rawi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e9d426",
   "metadata": {},
   "source": [
    "## NER Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "688764b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validate_NER(graph_file,xml_file,NER):\n",
    "    \n",
    "    # Extracting Graph info (Narrators)\n",
    "    \n",
    "    # Create a new RDF graph\n",
    "    graph = Graph()\n",
    "\n",
    "    # Parse the RDF data from a file or URL\n",
    "    graph.parse(graph_file)\n",
    "    \n",
    "    NER_type = NER\n",
    "    text = 'hasName'\n",
    "    if NER == 'Time':\n",
    "        text = 'hasTime'\n",
    "    # Define the SPARQL query\n",
    "    query = f\"\"\"\n",
    "    PREFIX : <http://www.tafsirtabari.com/ontology#>\n",
    "    PREFIX W3:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "    SELECT DISTINCT ?individual ?Name\n",
    "    WHERE {{\n",
    "      \n",
    "          ?individual rdf:type :{NER_type}.\n",
    "            ?individual :{text} ?Name.\n",
    "    }}\n",
    "    \n",
    "    \"\"\"\n",
    "    R = []\n",
    "    # Execute the SPARQL query\n",
    "    results = graph.query(query)\n",
    "    # Process the query results\n",
    "    for row in results:\n",
    "        # Access the query variables\n",
    "        org = row[\"individual\"]\n",
    "        name = row[\"Name\"]\n",
    "#         print(f\"Organization: {org}, Name: {name}\")\n",
    "        R.append(str(name))\n",
    "\n",
    "    NER_from_graph = list(set(R))\n",
    "\n",
    "    # extracting info from xml\n",
    "    for xml_f in xml_file:\n",
    "        with open(xml_f, 'r', encoding='utf8') as f:     \n",
    "            xml_file_as_string = f.read()\n",
    "\n",
    "        Bs_data = BeautifulSoup(xml_file_as_string, \"xml\")\n",
    "\n",
    "        tag = Bs_data.find_all(\"name\")\n",
    "        root = ET.fromstring(xml_file_as_string)\n",
    "        list_i = []\n",
    "        for i in tag:\n",
    "        #     print(i)\n",
    "            O = i.get(\"role\")\n",
    "            if O == NER_type.lower(): # set narrator type\n",
    "    #             print(i)\n",
    "                list_i.append(i.text)\n",
    "\n",
    "    s = set(list_i)\n",
    "    S = list(s)\n",
    "#     print(len(S),S)\n",
    "    print('\\n\\n')\n",
    "    # regex pattern to get the text inside tags\n",
    "    pattern = \"<[^>]*>(.*?)<\\/[^>]*>\"\n",
    "    count = 0\n",
    "    Nat = []\n",
    "    for i in S:\n",
    "        \n",
    "    #     T = removeSpace(T)\n",
    "        T = processText(i)\n",
    "        T = removeDiacritics(T)\n",
    "        Nat.append(T)\n",
    "    \n",
    "    \n",
    "    NER_from_xml = list(set(Nat))\n",
    "\n",
    "    if lists_have_same_elements(NER_from_graph, NER_from_xml):\n",
    "        print(\"The lists have the same elements.\")\n",
    "        print(f\"{NER_type} test passed! Validated! ✅ \")\n",
    "    else:\n",
    "        print(\"The lists do not have the same elements.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e3f95b28",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [71]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m g_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUniversity\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msemester6\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mKRR_WORK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFYP-AUTOKG-9f4906f3913e4c1e2353d282ee31f7b4aafcab4b\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mA-Box\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mTafsir Al-Tabari A-Box S100.owl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m xml_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUniversity\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msemester6\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mKRR_WORK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFYP-AUTOKG-Graph-Generator\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mA-Box\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mS100M\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msure_100_section_100.1-100.1.xml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mValidate_NER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43mxml_file\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36mValidate_NER\u001b[1;34m(graph_file, xml_file, NER)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# extracting info from xml\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xml_f \u001b[38;5;129;01min\u001b[39;00m xml_file:\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxml_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:     \n\u001b[0;32m     45\u001b[0m         xml_file_as_string \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     47\u001b[0m     Bs_data \u001b[38;5;241m=\u001b[39m BeautifulSoup(xml_file_as_string, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D'"
     ]
    }
   ],
   "source": [
    "g_file = \"D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-9f4906f3913e4c1e2353d282ee31f7b4aafcab4b\\\\A-Box\\\\Tafsir Al-Tabari A-Box S100.owl\"\n",
    "xml_file = 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S100M\\\\sure_100_section_100.1-100.1.xml'\n",
    "Validate_NER(g_file,xml_file,'Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5818c795",
   "metadata": {},
   "source": [
    "## Poetry Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e1e08244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validate_Poetry(graph_file,xml_file):\n",
    "    \n",
    "    # Extracting Graph info (Narrators)\n",
    "    \n",
    "    # Create a new RDF graph\n",
    "    graph = Graph()\n",
    "\n",
    "    # Parse the RDF data from a file or URL\n",
    "    graph.parse(graph_file)\n",
    "    \n",
    "    # Define the SPARQL query\n",
    "    query = \"\"\"\n",
    "    PREFIX : <http://www.tafsirtabari.com/ontology#>\n",
    "    PREFIX W3:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "    SELECT DISTINCT *\n",
    "    WHERE {\n",
    "\n",
    "    ?s rdf:type :Poetry.\n",
    "    ?s :hasText ?n.\n",
    "\n",
    "    }\n",
    "    \"\"\"\n",
    "    R = []\n",
    "    # Execute the SPARQL query\n",
    "    results = graph.query(query)\n",
    "    # Process the query results\n",
    "    for row in results:\n",
    "        # Access the query variables\n",
    "        s = row[\"s\"]\n",
    "        n = row[\"n\"]\n",
    "        print(f\"s: {s}, n: {n}\")\n",
    "        R.append(str(n))\n",
    "\n",
    "    poetry_from_graph = list(set(R))\n",
    "        \n",
    "    for xml_f in xml_file:\n",
    "        # extracting info from xml\n",
    "        with open(xml_f, 'r', encoding='utf8') as f:     \n",
    "            xml_file_as_string = f.read()\n",
    "\n",
    "        Bs_data = BeautifulSoup(xml_file_as_string, \"xml\")\n",
    "        tag = Bs_data.find_all(\"quote\")\n",
    "        root = ET.fromstring(xml_file_as_string)\n",
    "        for i in tag:\n",
    "        #     print(i)\n",
    "            O = i.get(\"type\")\n",
    "            if O == \"poetry\":\n",
    "                print(processText(removeDiacritics(str(i))))\n",
    "                print('\\n\\n')\n",
    "                list_i.append(processText(removeDiacritics(i.text)))\n",
    "\n",
    "    s = set(list_i)\n",
    "    S = list(s)\n",
    "    \n",
    "\n",
    "    print('\\n\\n')\n",
    "    # regex pattern to get the text inside tags\n",
    "    pattern = \"<[^>]*>(.*?)<\\/[^>]*>\"\n",
    "    count = 0\n",
    "    Nat = []\n",
    "    for i in S:\n",
    "        \n",
    "    #     T = removeSpace(T)\n",
    "        T = processText(i)\n",
    "        T = removeDiacritics(T)\n",
    "        Nat.append(T)\n",
    "        \n",
    "        \n",
    "    # Further text cleaning (needed for pieces of text)\n",
    "    #graph\n",
    "    cleaned_poetry_from_graph = []\n",
    "    for i in poetry_from_graph:\n",
    "        a = remove_numbers_and_slashes_hadith(i)\n",
    "        cleaned_poetry_from_graph.append(removeDiacritics(a))\n",
    "\n",
    "    # xml\n",
    "    list_i_p = []\n",
    "    for i in list_i:\n",
    "        filtered_list = remove_numbers_and_slashes(i)\n",
    "        list_i_p.append(filtered_list)\n",
    "\n",
    "    # removing leading spaces\n",
    "    poetry_from_xml = remove_leading_spaces_list(list_i_p)\n",
    "\n",
    "    cleaned_poetry_from_graph.sort()\n",
    "    poetry_from_xml.sort()\n",
    "\n",
    "    \n",
    "    \n",
    "    poetry_from_xml = list(set(Nat))\n",
    "\n",
    "    if lists_have_same_elements(cleaned_poetry_from_graph, poetry_from_xml):\n",
    "        print(\"The lists have the same elements.\")\n",
    "        print(f\"Poetry test passed! Validated! ✅ \")\n",
    "    else:\n",
    "        print(\"The lists do not have the same elements.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ada6acfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: http://www.tafsirtabari.com/ontology#P_001, n: إِذْ أُجَارِي الشيطان فِي سَنَنِ الْغَسيِّ وَمَنْ مَالَ مَيْلَهُ مَثْبُورُ\n",
      "s: http://www.tafsirtabari.com/ontology#P_002, n: يَا رَسُولَ الْمَلِيكِ إِنَّ لِسَانِي رَاتِقٌ مَا فَتَّقْتُ إِذْ أَنَا بُورُ\n",
      "s: http://www.tafsirtabari.com/ontology#P_003, n: حنت إلي النخلة القصوى فقلت لها حجر حرام ألا تلك الدهاريس\n",
      "s: http://www.tafsirtabari.com/ontology#P_004, n: فهممت أن ألقي إليها محجرا فلمثلها يلقي إليه المحجر\n",
      "s: http://www.tafsirtabari.com/ontology#P_005, n: وقدم الخوارج الضلال إلى عباد ربهم وقالوا إن دماءكم لنا حلال\n",
      "s: http://www.tafsirtabari.com/ontology#P_006, n: سَبَقْتُ إِلَى فَرَطٍ ناهِلٍ تَنَابِلَةً يَحْفُرُونَ الرِّسَاسَا\n",
      "s: http://www.tafsirtabari.com/ontology#P_007, n: حتى يقول الناس مما رأوا يا عجبا للميت الناشر\n",
      "s: http://www.tafsirtabari.com/ontology#P_008, n: رعى بها مرج ربيع ممرجا\n",
      "s: http://www.tafsirtabari.com/ontology#P_009, n: ألم يحزنك أن حبال قيس وتغلب قد تباينتا انقطاعا\n",
      "s: http://www.tafsirtabari.com/ontology#P_010, n: كَأَنَّهَا بُرْجُ رومي يُشَيِّدُهُ بَانٍ بِجِصٍّ وَآجُرٍّ وَأَحْجَارِ\n",
      "s: http://www.tafsirtabari.com/ontology#P_011, n: وَلَهَا بِالْمَاطَرُونِ إِذَا أَكَلَ النَّمْلُ الَّذِي جَمَعَا\n",
      "s: http://www.tafsirtabari.com/ontology#P_012, n: خِلْفَةٌ حَتَّى إِذَا ارْتَبَعَتْ سَكَنَتْ مِنْ جِلِّقٍ بِيَعَا\n",
      "s: http://www.tafsirtabari.com/ontology#P_013, n: بِهَا الْعَيْنُ وَالآرَامُ يَمْشِينَ خِلْفَةً وَأَطْلاؤُهَا يَنْهَضْنَ مِنْ كُلِّ مَجْثَمِ\n",
      "s: http://www.tafsirtabari.com/ontology#P_014, n: إن يعاقب يكن غراما وإن يع ط جزيلا فإنه لا يبالى\n",
      "s: http://www.tafsirtabari.com/ontology#P_015, n: ويوم النسار ويوم الجفا ر كانا عقابا وكانا غراما\n",
      "s: http://www.tafsirtabari.com/ontology#P_016, n: يومان يوم مقامات وأندية ويوم سير إلى الأعداء تأويب\n",
      "s: http://www.tafsirtabari.com/ontology#P_017, n: فأيي ما وأيك كان شرا فقيد إلى المقامة لا يراها\n",
      "s: http://www.tafsirtabari.com/ontology#P_018, n: طَافَتْ أمامة بِالرُّكْبَانِ آوِنَةً يَا حُسْنَهُ مِنْ قَوَامٍ مَا وَمُنْتَقَبَا\n",
      "s: http://www.tafsirtabari.com/ontology#P_019, n: جزى الله ابن عروة حيث أمسى عقوقا والعقوق له أثام\n",
      "s: http://www.tafsirtabari.com/ontology#P_020, n: متى تأته تعشو إلى ضوء ناره تجد خير نار عندها خير موقد\n",
      "s: http://www.tafsirtabari.com/ontology#P_021, n: لا يُقْنِعُ الْجَارِيَةَ الْخِضَابُ وَلا الْوِشَاحَانِ وَلا الْجِلْبَابُ مِنْ دُونِ أَنْ تَلْتَقِيَ الأَرْكَابُ وَيَقْعُدَ الأَيْرُ لَهُ لُعَابُ\n",
      "s: http://www.tafsirtabari.com/ontology#P_022, n: وَيَقْعُدُ الْهَنُ لَهُ لُعَابُ\n",
      "s: http://www.tafsirtabari.com/ontology#P_023, n: يَا عَاذِلاتِي لا تُرِدْنَ مَلامَتِي إِنَّ الْعَوَاذِلَ لَسْنَ لِي بِأَمِيرِ\n",
      "s: http://www.tafsirtabari.com/ontology#P_024, n: كأن بنحره وبمنكبيه عبيرا بات يعبؤه عروس\n",
      "s: http://www.tafsirtabari.com/ontology#P_025, n: ففاجأة بعادية لِزامٍ كما يتفجر الحوض اللقيف\n",
      "s: http://www.tafsirtabari.com/ontology#P_026, n: إِذَا كَانَ طَعْنًا بَيْنَهُمْ وَقِتَالا\n",
      "<quote type=\"poetry\"> <lg> <l>إذ أجاري <seg ana=\"samiyat\"><name role=\"other\">الشيطان</name></seg> في سنن الغسي </l> <l>ومن مال ميله مثبور </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>يا رسول المليك إن لساني </l> <l>راتق ما فتقت إذ أنا بور </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>حنت إلي النخلة القصوى فقلت لها </l> <l>حجر حرام ألا تلك <name role=\"time\">الدهاريس</name> </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>فهممت أن ألقي إليها محجرا </l> <l>فلمثلها يلقي إليه المحجر </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <l> وقدم <seg ana=\"yes firaq\">الخوارج</seg> <seg ana=\"kalamterm\">الضلال</seg> </l> <l> إلى عباد ربهم وقالوا </l> <l> إن دماءكم لنا حلال </l> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>سبقت إلى فرط ناهل </l> <l>تنابلة يحفرون الرساسا </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>حتى يقول الناس مما رأوا </l> <l>يا عجبا للميت الناشر </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <l> رعى بها مرج ربيع ممرجا </l> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>ألم يحزنك أن حبال قيس </l> <l>وتغلب قد تباينتا انقطاعا </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>كأنها برج <name role=\"organization\">رومي</name> يشيده </l> <l>بان بجص وآجر وأحجار </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>ولها بالماطرون إذا </l> <l>أكل النمل الذي جمعا </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>خلفة حتى إذا ارتبعت </l> <l>سكنت من جلق بيعا </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>بها العين والآرام يمشين خلفة </l> <l>وأطلاؤها ينهضن من كل مجثم </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>إن يعاقب يكن غراما وإن يع </l> <l>ط جزيلا فإنه لا يبالى </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>و<name role=\"time\">يوم النسار</name> و<name role=\"time\">يوم الجفا</name> </l> <l>ر كانا عقابا وكانا غراما </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>يومان يوم مقامات وأندية </l> <l>ويوم سير إلى الأعداء تأويب </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>فأيي ما وأيك كان شرا </l> <l>فقيد إلى المقامة لا يراها </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>طافت <name role=\"person\">أمامة</name> بالركبان آونة </l> <l>يا حسنه من قوام ما ومنتقبا </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>جزى الله <name role=\"person\">ابن عروة</name> حيث أمسى </l> <l>عقوقا والعقوق له أثام </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>متى تأته تعشو إلى ضوء ناره </l> <l>تجد خير نار عندها خير موقد </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <l> لا يقنع الجارية الخضاب </l> <l> ولا الوشاحان ولا الجلباب </l> <l> من دون أن تلتقي الأركاب </l> <l> ويقعد الأير له لعاب </l> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <l> ويقعد الهن له لعاب </l> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>يا عاذلاتي لا تردن ملامتي </l> <l>إن العواذل لسن لي بأمير </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>كأن بنحره وبمنكبيه </l> <l>عبيرا بات يعبؤه عروس </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>ففاجأة بعادية لزام </l> <l>كما يتفجر الحوض اللقيف </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <l> إذا كان طعنا بينهم وقتالا </l> </quote>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'remove_numbers_and_slashes_hadith' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [73]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUniversity\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msemester6\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mKRR_WORK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFYP-AUTOKG-Narrator_fixed\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mS025N\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msure_25_section_25.1-25.46.xml\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUniversity\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msemester6\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mKRR_WORK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFYP-AUTOKG-Narrator_fixed\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mS025N\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msure_25_section_25.47-25.52.xml\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUniversity\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msemester6\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mKRR_WORK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFYP-AUTOKG-Narrator_fixed\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mA-Box\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mTafsir Al-Tabari A-Box S25.owl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mValidate_Poetry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36mValidate_Poetry\u001b[1;34m(graph_file, xml_file)\u001b[0m\n\u001b[0;32m     73\u001b[0m cleaned_poetry_from_graph \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m poetry_from_graph:\n\u001b[1;32m---> 75\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mremove_numbers_and_slashes_hadith\u001b[49m(i)\n\u001b[0;32m     76\u001b[0m     cleaned_poetry_from_graph\u001b[38;5;241m.\u001b[39mappend(removeDiacritics(a))\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# xml\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'remove_numbers_and_slashes_hadith' is not defined"
     ]
    }
   ],
   "source": [
    "a = ['D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\Dataset\\\\S025N\\\\sure_25_section_25.1-25.46.xml','D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\Dataset\\\\S025N\\\\sure_25_section_25.47-25.52.xml']\n",
    "g = 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\A-Box\\\\Tafsir Al-Tabari A-Box S25.owl'\n",
    "Validate_Poetry(g,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e41e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9ae1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab31ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f02ce05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "30e222c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total verses: 6\n",
      "['قُلْ أَعُوذُ بِرَبِّ النَّاسِ ', ' مَلِكِ النَّاسِ ', ' إِلَهِ النَّاسِ ', ' مِنْ شَرِّ الْوَسْوَاسِ الْخَنَّاسِ ', ' الَّذِي يُوَسْوِسُ فِي صُدُورِ النَّاسِ ', ' مِنَ الْجِنَّةِ وَالنَّاسِ ']\n"
     ]
    }
   ],
   "source": [
    "list_i = []\n",
    "import re\n",
    "# D:\\University\\semester6\\KRR_WORK\\FYP-AUTOKG-Narrator_fixed\\Dataset\\S114٘M\n",
    "# a = ['D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\Dataset\\\\S025N\\\\sure_25_section_25.1-25.46.xml','D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\Dataset\\\\S025N\\\\sure_25_section_25.47-25.52.xml']\n",
    "a = ['D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\Dataset\\\\S114٘M\\\\sure_114_section_114.1-114.1.xml']\n",
    "verses = []\n",
    "\n",
    "for xml_f in a:\n",
    "    # extracting info from xml\n",
    "    with open(xml_f, 'r', encoding='utf8') as f:     \n",
    "        xml_file_as_string = f.read()\n",
    "        Bs_data = BeautifulSoup(xml_file_as_string, 'xml')\n",
    "\n",
    "    tags_with_quotes = Bs_data.find_all(\"head\")\n",
    "    \n",
    "    for tag in tags_with_quotes:\n",
    "        quote_tag = tag.find(\"quote\")\n",
    "        if quote_tag:\n",
    "            l_tags = quote_tag.find_all(\"l\")  # Find nested <l> tags\n",
    "            if l_tags:\n",
    "                for l_tag in l_tags:\n",
    "                    l_text = l_tag.get_text()\n",
    "                    verses.append(l_text)    \n",
    "            else:        \n",
    "                quote_text = quote_tag.get_text()  # Get the text within the quote tag\n",
    "                verses.append(quote_text)\n",
    "print(\"Total verses:\", len(verses))\n",
    "\n",
    "            \n",
    "print(verses)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5735c090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7058e0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: http://www.tafsirtabari.com/ontology#C_114, n: http://www.tafsirtabari.com/ontology#V114:002, t: مَلِكِ النَّاسِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_114, n: http://www.tafsirtabari.com/ontology#V114:003, t: إِلَٰهِ النَّاسِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_114, n: http://www.tafsirtabari.com/ontology#V114:004, t: مِنْ شَرِّ الْوَسْوَاسِ الْخَنَّاسِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_114, n: http://www.tafsirtabari.com/ontology#V114:005, t: الَّذِي يُوَسْوِسُ فِي صُدُورِ النَّاسِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_114, n: http://www.tafsirtabari.com/ontology#V114:006, t: مِنَ الْجِنَّةِ وَالنَّاسِ\n",
      "s: http://www.tafsirtabari.com/ontology#C_114, n: http://www.tafsirtabari.com/ontology#V114:001, t: بِسْمِ اللَّهِ الرَّحْمَٰنِ الرَّحِيمِ قُلْ أَعُوذُ بِرَبِّ النَّاسِ\n"
     ]
    }
   ],
   "source": [
    "# querying from graph\n",
    "    \n",
    "# Create a new RDF graph\n",
    "graph = Graph()\n",
    "\n",
    "graph_file = 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\A-Box\\\\Tafsir Al-Tabari A-Box S114.owl'\n",
    "\n",
    "\n",
    "# Parse the RDF data from a file or URL\n",
    "graph.parse(graph_file)\n",
    "\n",
    "# Define the SPARQL query\n",
    "query = \"\"\"\n",
    "PREFIX : <http://www.tafsirtabari.com/ontology#>\n",
    "PREFIX W3:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "SELECT DISTINCT *\n",
    "WHERE {\n",
    "\n",
    " ?k rdf:type :Chapter.\n",
    "?k :containsVerse ?v.\n",
    "?v :hasText ?t.\n",
    "}\n",
    "\"\"\"\n",
    "R = []\n",
    "# Execute the SPARQL query\n",
    "results = graph.query(query)\n",
    "# Process the query results\n",
    "for row in results:\n",
    "    # Access the query variables\n",
    "    s = row[\"k\"]\n",
    "    n = row[\"v\"]\n",
    "    t = row[\"t\"]\n",
    "    \n",
    "    print(f\"s: {s}, n: {n}, t: {t}\")\n",
    "    R.append(str(t))\n",
    "\n",
    "hadiths_from_graph = list(set(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d048d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8da357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d182f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
