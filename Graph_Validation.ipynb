{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "164cc3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.resources import path\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "from rdflib import Graph\n",
    "from shutil import copy\n",
    "import xml.etree.ElementTree as ET\n",
    "from matplotlib import lines\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pyarabic.araby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d6dad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S000A MuqadimaA\\\\sure_0_section.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S001M\\\\sure_1_section_1.1-1.14.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.1-2.42.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.113-2.165.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.166-2.210.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.211-2.263.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.264-2.297.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.298-2.344.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.345-2.395.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.396-2.439.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.43-2.77.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.440-2.482.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.483-2.498.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.499-2.523.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.524-2.532.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.533-2.548.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.549-2.582.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.583-2.613.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.614-2.626.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.627-2.648.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.649-2.672.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.673-2.685.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.686-2.717.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.718-2.750.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.751-2.797.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.78-2.112.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S002A\\\\sure_2_section_2.798-2.830.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.1-3.48.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.139-3.171.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.172-3.210.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.211-3.249.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.250-3.290.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.291-3.301.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.49-3.95.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S003A\\\\sure_3_section_3.96-3.138.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.1-4.27.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.115-4.148.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.149-4.197.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.198-4.208.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.209-4.241.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.242-4.277.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.278-4.302.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.28-4.57.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.58-4.84.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S004N\\\\sure_4_section_4.85-4.114.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.1-5.35.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.106-5.125.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.126-5.137.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.138-5.178.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.179-5.205.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.206-5.224.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.225-5.256.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.36-5.52.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S005Cat NER MA\\\\sure_5_section_5.53-5.105.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S006N\\\\sure_6_section_6.1-6.63.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S006N\\\\sure_6_section_6.101-6.144.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S006N\\\\sure_6_section_6.145-6.199.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S006N\\\\sure_6_section_6.200-6.234.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S006N\\\\sure_6_section_6.235-6.241.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S006N\\\\sure_6_section_6.64-6.100.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S007A\\\\sure_7_section_7.1-7.48.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S007A\\\\sure_7_section_7.151-7.191.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S007A\\\\sure_7_section_7.192-7.232.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S007A\\\\sure_7_section_7.233-7.235.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S007A\\\\sure_7_section_7.49-7.89.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S007A\\\\sure_7_section_7.90-7.150.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S008CatA\\\\sure_8_section_8.1-8.26.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S008CatA\\\\sure_8_section_8.27-8.56.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S008CatA\\\\sure_8_section_8.57-8.83.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S009CatA\\\\sure_9_section_9.1-9.28.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S009CatA\\\\sure_9_section_9.116-9.131.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S009CatA\\\\sure_9_section_9.29-9.62.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S009CatA\\\\sure_9_section_9.63-9.92.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S009CatA\\\\sure_9_section_9.93-9.115.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S010M\\\\sure_10_section_10.1-10.68.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S010M\\\\sure_10_section_10.69-10.108.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S011CatA\\\\sure_11_section_11.1-11.42.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S011CatA\\\\sure_11_section_11.43-11.81.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S011CatA\\\\sure_11_section_11.82-11.112.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S012CatA\\\\sure_12_section_12.1-12.22.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S012CatA\\\\sure_12_section_12.23-12.60.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S012CatA\\\\sure_12_section_12.61-12.84.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S012CatA\\\\sure_12_section_12.85-12.91.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S013M\\\\sure_13_section_13.1-13.21.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S013M\\\\sure_13_section_13.22-13.39.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S014M\\\\sure_14_section_14.1-14.34.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S014M\\\\sure_14_section_14.35-14.44.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S015M\\\\sure_15_section_15.1-15.44.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S015M\\\\sure_15_section_15.45-15.51.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S016A\\\\sure_16_section_16.1-16.61.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S016A\\\\sure_16_section_16.112-16.112.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S016A\\\\sure_16_section_16.62-16.111.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S017N\\\\sure_17_section_17.1-17.13.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S017N\\\\sure_17_section_17.14-17.53.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S017N\\\\sure_17_section_17.54-17.86.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S017N\\\\sure_17_section_17.87-17.102.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S018CatA\\\\sure_18_section_18.1-18.26.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S018CatA\\\\sure_18_section_18.27-18.64.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S018CatA\\\\sure_18_section_18.65-18.77.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S019N\\\\sure_19_section_19.1-19.38.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S019N\\\\sure_19_section_19.39-19.67.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S020CatA\\\\sure_20_section_20.1-20.42.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S020CatA\\\\sure_20_section_20.43-20.71.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S021CatA\\\\sure_21_section_21.1-21.61.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S021CatA\\\\sure_21_section_21.62-21.85.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S022CatA\\\\sure_22_section_22.1-22.27.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S022CatA\\\\sure_22_section_22.28-22.63.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S023CatN\\\\sure_23_section_23.1-23.67.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S024CatA\\\\sure_22_section_22.24-22.63.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S024CatA\\\\sure_24_section_24.1-24.27.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S024CatA\\\\sure_24_section_24.28-24.52.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S024CatA\\\\sure_24_section_24.53-22.23.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S025N\\\\sure_25_section_25.1-25.46.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S025N\\\\sure_25_section_25.47-25.52.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S026N\\\\sure_26_section_26.1-26.67.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S026N\\\\sure_26_section_26.68-26.69.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S027N\\\\sure_27_section_27.1-27.50.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S027N\\\\sure_27_section_27.51-27.56.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S028N\\\\sure_28_section_28.1-28.38.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S028N\\\\sure_28_section_28.39-28.73.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S029M\\\\sure_29_section_29.1-29.60.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S030M\\\\sure_30_section_30.1-30.50.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S031M\\\\sure_31_section_31.1-31.28.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S032M\\\\sure_32_section_32.1-32.21.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S033N\\\\sure_33_section_33.1-33.26.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S033N\\\\sure_33_section_33.27-33.51.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S034N\\\\sure_34_section_34.1-34.45.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S035N\\\\sure_35_section_35.1-35.31.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S036N\\\\sure_36_section_36.1-36.39.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S037CatA\\\\sure_37_section_37.1-37.32.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S037CatA\\\\sure_37_section_37.33-37.47.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S038A\\\\sure_38_section_38.1-38.23.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S038A\\\\sure_38_section_38.24-38.34.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S039A\\\\sure_39_section_39.1-39.51.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S040A\\\\sure_40_section_40.1-40.53.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S041CatN\\\\sure_41_section_41.1-41.39.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S042A\\\\sure_42_section_42.1-42.36.xml',\n",
       " \"D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S043A\\\\sure_43_section_43.1-43.50 (Raqmi 333's conflicted copy 2022-06-07).xml\",\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S043A\\\\sure_43_section_43.1-43.50.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S044M\\\\sure_44_section_44.1-44.20.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S045M\\\\sure_45_section_45.1-45.30.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S046CatN\\\\sure_46_section_46.1-46.28.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S047M\\\\sure_47_section_47.1-47.24.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S048A\\\\sure_48_section_48.1-48.20.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S049CatN\\\\sure_49_section_49.1-49.16.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S050CatN\\\\sure_50_section_50.1-50.22.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S051A\\\\sure_51_section_51.1-51.25.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S052A\\\\sure_52_section_52.1-52.19.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S053AN\\\\sure_53_section_53.1-53.18.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S054A\\\\sure_54_section_54.1-54.20.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S055N\\\\sure_55_section_55.1-55.20.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S056N\\\\sure_56_section_56.1-56.20.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S057N\\\\sure_57_section_57.1-57.23.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S058N\\\\sure_58_section_58.1-58.20.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S059N\\\\sure_59_section_59.1-59.17.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S060A\\\\sure_60_section_60.1-60.12.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S061M\\\\sure_61_section_61.1-61.10.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S062M\\\\sure_62_section_62.1-62.10.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S063CatM\\\\sure_63_section_63.1-63.10.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S064M\\\\sure_64_section_64.1-64.14.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S065A\\\\sure_65_section_65.1-65.8.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S066A\\\\sure_66_section_66.1-66.12.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S067M\\\\sure_67_section_67.1-67.19.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S068N\\\\sure_68_section_68.1-68.19.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S069N\\\\sure_69_section_69.1-69.13.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S070N\\\\sure_70_section_70.1-70.11.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S071M\\\\sure_71_section_71.1-71.9.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S072CatN\\\\sure_72_section_72.1-72.11.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S073N\\\\sure_73_section_73.1-73.8.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S074N\\\\sure_74_section_74.1-74.10.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S074N\\\\TT_s074_div010.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S075N\\\\sure_75_section_75.1-75.8.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S076M\\\\sure_76_section_76.1-76.14.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S077N\\\\sure_77_section_77.1-77.10.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S078N\\\\sure_78_section_78.1-78.9.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S079N\\\\sure_79_section_79.1-79.9.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S080M\\\\sure_80_section_80.1-80.6.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S081M\\\\sure_81_section_81.1-81.6.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S082M\\\\sure_82_section_82.1-82.4.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S083M\\\\sure_83_section_83.1-83.9.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S084M\\\\sure_84_section_84.1-84.5.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S085CatM\\\\sure_85_section_85.1-85.6.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S086M\\\\sure_86_section_86.1-86.2.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S087M\\\\sure_87_section_87.1-87.3.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S088M\\\\sure_88_section_88.1-88.4.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S089CatN\\\\sure_89_section_89.1-89.6.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S090A\\\\sure_90_section_90.1-90.3.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S091M\\\\sure_91_section_91.1-91.2.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S092CatM\\\\sure_92_section_92.1-92.3.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S093M\\\\sure_93_section_93.1-93.2.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S094M\\\\sure_94_section_94.1-94.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S095M\\\\sure_95_section_95.1-95.2.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S096M\\\\sure_96_section_96.1-96.5.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S097M\\\\sure_97_section_97.1-97.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S098M\\\\sure_98_section_98.1-98.4.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S099M\\\\sure_99_section_99.1-99.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S100M\\\\sure_100_section_100.1-100.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S101M\\\\sure_101_section_101.1-101.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S102M\\\\sure_102_section_102.1-102.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S103CatM\\\\sure_103_section_103.1-103.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S104CatM\\\\sure_104_section_104.1-104.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S105CatM\\\\sure_105_section_105.1-105.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S106CatM\\\\sure_106_section_106.1-106.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S107M\\\\sure_107_section_107.1-107.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S108NM\\\\sure_108_section_108.1-108.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S109CatM\\\\sure_109_section_109.1-109.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S110M\\\\sure_110_section_110.1-110.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S111CatM\\\\sure_111_section_111.1-111.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S112CatM\\\\sure_112_section_112.1-112.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S113CatM\\\\sure_113_section_113.1-113.1.xml',\n",
       " 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S114Ù˜M\\\\sure_114_section_114.1-114.1.xml']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\**\\\\*.xml', recursive = True)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77ead6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sections(inputfile):\n",
    "    import xml.etree.ElementTree as ET\n",
    "    with open(inputfile, 'r',encoding='utf8') as f:\n",
    "        xml_file_as_string = f.read()\n",
    "    ET.register_namespace(\"\", \"http://www.tei-c.org/ns/1.0\")\n",
    "    xml_root = ET.fromstring(xml_file_as_string)\n",
    "    ns = xml_root.tag\n",
    "    ns = ns[:ns.rfind('}') + 1]\n",
    "    ana_tags_statistics = dict()\n",
    "    # text/body/div/p/ana\n",
    "    sections_list = []\n",
    "    try:\n",
    "        for paragraph_node in xml_root.find(ns + \"text\").find(ns + \"body\").iter(ns + \"div\"):\n",
    "            # for div_node in paragraph_node.findall(ns+\"p\"):\n",
    "            paragraph_type_attrib = paragraph_node.get(\"type\")\n",
    "            paragraph_n_attrib = paragraph_node.get(\"n\")\n",
    "\n",
    "            if paragraph_type_attrib == \"section\":\n",
    "                sections_list.append(paragraph_n_attrib)\n",
    "\n",
    "            if paragraph_type_attrib == \"chapter\" and \"section\" not in xml_file_as_string:\n",
    "                sections_list.append(paragraph_n_attrib)\n",
    "            # print(type(paragraph_n_attrib))\n",
    "    except:\n",
    "        try:\n",
    "            try:\n",
    "                for paragraph_node in xml_root.find(ns + \"text\").find(ns + \"body\").find(ns + \"div\").iter(ns + \"div\"):\n",
    "                    # for div_node in paragraph_node.findall(ns+\"p\"):\n",
    "                    paragraph_type_attrib = paragraph_node.get(\"type\")\n",
    "                    paragraph_n_attrib = paragraph_node.get(\"n\")\n",
    "\n",
    "                    if paragraph_type_attrib == \"section\":\n",
    "                        sections_list.append(paragraph_n_attrib)\n",
    "                    # print(type(paragraph_n_attrib))\n",
    "            except:\n",
    "                for paragraph_node in xml_root.find(ns + \"text\").find(ns + \"body\").iter(ns + \"div\"):\n",
    "                    # for div_node in paragraph_node.findall(ns+\"p\"):\n",
    "                    paragraph_type_attrib = paragraph_node.get(\"type\")\n",
    "                    paragraph_n_attrib = paragraph_node.get(\"n\")\n",
    "\n",
    "                    if paragraph_type_attrib == \"chapter\":\n",
    "                        print(\"found\")\n",
    "                        sections_list.append(paragraph_n_attrib)\n",
    "        except:\n",
    "            print(\"Error1\")\n",
    "\n",
    "    # print(sections_list[0],sections_list[-1])\n",
    "    # sure = sections_list[0]\n",
    "    # print(sure[0])\n",
    "    return sections_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "53086abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  extract_paragraphs_2(input_file, list_of_list_for_cvs):\n",
    "    #header = parse_paragraph_quran_n(input_file)\n",
    "    sura_number = sections(input_file)\n",
    "    with open(input_file, 'r',encoding='utf8') as f: \n",
    "        data = f.read() \n",
    "        Bs_data = BeautifulSoup(data, \"xml\") \n",
    "\n",
    "        # find all sub_sections\n",
    "        sub_sections = Bs_data.find_all('div')\n",
    "        sub_sections_liste = []\n",
    "        for i in sub_sections:\n",
    "            try:\n",
    "                an = (i['n'])\n",
    "                sub_sections_liste.append(an)\n",
    "            except: \n",
    "                #print(i)\n",
    "                sub_sections_liste.append(\"no ayat number\")\n",
    "                continue\n",
    "       # print(sub_sections_liste)\n",
    "        #sub_sections = list(sub_sections)\n",
    "        #print(sub_sections)\n",
    "\n",
    "        \n",
    "        list_of_list_paragraphs = []\n",
    "        list_of_list_topics = []\n",
    "        list_of_list_subtopics = []\n",
    "        list_of_list_xml_id = []\n",
    "        list_of_list_HadithNotHadith = []\n",
    "        list_of_list_ayat_numbers = []\n",
    "        list_of_list_rawis = []\n",
    "        list_of_list_sahabis = []\n",
    "        list_of_list_shaykhs = []\n",
    "        list_of_list_persons = []\n",
    "        list_of_list_locations = []\n",
    "        list_of_list_organizations = []\n",
    "        list_of_list_times = []\n",
    "        list_of_list_others = []\n",
    "        list_of_list_heads_30 = []\n",
    "        \n",
    "\n",
    "        # zahler \n",
    "        zaehler_topics = 0\n",
    "        zaehler_subtopics = 0\n",
    "\n",
    "\n",
    "        previous_ayat = \"\"\n",
    "\n",
    "\n",
    "\n",
    "        # delte double sub_sections\n",
    "        '''\n",
    "        list_of_sub_sections_reverse = sub_sections[::-1]\n",
    "        list_of_subsections_clear = [] \n",
    "        liste_new = []\n",
    "        end_flag = 0\n",
    "        for element in list_of_sub_sections_reverse:\n",
    "            element_str = str(element)\n",
    "            check_line = element_str\n",
    "            for element_new in list_of_subsections_clear:\n",
    "                if check_line == str(element_new):\n",
    "                    end_flag = 1\n",
    "                    break\n",
    "            if end_flag == 1:\n",
    "                end_flag = 0\n",
    "                continue\n",
    "            else:\n",
    "                list_of_subsections_clear.append(element)\n",
    "        list_of_subsections_clear_reverse = list_of_subsections_clear[::-1]\n",
    "\n",
    "        #print(list_of_subsections_clear_reverse)\n",
    "        \n",
    "        #print(sub_sections)\n",
    "        #print(type(sub_sections))\n",
    "        '''\n",
    "\n",
    "        for i in sub_sections: #sub_sections\n",
    "            #print(\"Hallo\")\n",
    "            #print(i)\n",
    "\n",
    "            # head = 30\n",
    "            heads_30 = [] \n",
    "            a_numb = \"No Ayat Number\"\n",
    "            \n",
    "            heads_first = i.find_all('head', {'type' : '30'})\n",
    "            for head in heads_first:\n",
    "                ayat_number = head.find('quote')\n",
    "                try:\n",
    "                    a_numb = (ayat_number['n'])\n",
    "                    previous_ayat = a_numb\n",
    "                except:\n",
    "                    a_numb = \"No Ayat Number\"\n",
    "                    continue\n",
    "              \n",
    "            if a_numb == \"No Ayat Number\":\n",
    "                a_numb =  a_numb + \" (\" + previous_ayat + \")\"\n",
    "\n",
    "        \n",
    "            # paragraphs\n",
    "            paragraphs = [] \n",
    "            paragraphs_first = i.find_all('p')\n",
    "            for i in paragraphs_first:\n",
    "                if i == \"\\n\":\n",
    "                    paragraphs.append(\"No Text\") \n",
    "                else:\n",
    "                    paragraphs.append(i) \n",
    "            list_of_list_paragraphs.append(paragraphs)\n",
    "\n",
    "\n",
    "            # ayat numbers add to paragraph\n",
    "            for i in range(len(paragraphs)):\n",
    "                heads_30.append(a_numb)\n",
    "            list_of_list_heads_30.append(heads_30)\n",
    "\n",
    "\n",
    "\n",
    "            # topics\n",
    "            liste_topics = []\n",
    "            for i in paragraphs:\n",
    "                try:\n",
    "                    a = (i['ana'])\n",
    "                    zaehler_topics = zaehler_topics + 1\n",
    "                    liste_topics.append(a)\n",
    "                except: \n",
    "                    #print(i)\n",
    "                    liste_topics.append(\"no topic\")\n",
    "                    continue\n",
    "            list_of_list_topics.append(liste_topics)\n",
    "\n",
    "            # subtopics\n",
    "            subtopics = []\n",
    "            subtopics_of_paragraph = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_subtopics = i.find_all('seg')\n",
    "                for a in paragraph_subtopics:\n",
    "                    try:\n",
    "                        s = (a['ana'])\n",
    "                        zaehler_subtopics = zaehler_subtopics + 1\n",
    "                        subtopics_of_paragraph.append(s)\n",
    "                    except: \n",
    "                        subtopics_of_paragraph.append(\"no subtopic\")\n",
    "                    continue\n",
    "                subtopics.append(subtopics_of_paragraph)\n",
    "                subtopics_of_paragraph = []\n",
    "            list_of_list_subtopics.append(subtopics)\n",
    "            #print(list_of_list_subtopics)\n",
    "\n",
    "            # ayat numbers in paragraph\n",
    "            ayat_numbers = []\n",
    "            ayat_numbers_of_paragraph = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_ayat_numbers = i.find_all('quote', {'type': 'quran'})\n",
    "                for a in paragraph_ayat_numbers:\n",
    "                    try:\n",
    "                        s = (a['n'])\n",
    "                        ayat_numbers_of_paragraph.append(s)\n",
    "                    except: \n",
    "                        ayat_numbers_of_paragraph.append(\"no ayat number\")\n",
    "                    continue\n",
    "                ayat_numbers.append(ayat_numbers_of_paragraph)\n",
    "                ayat_numbers_of_paragraph = []\n",
    "            list_of_list_ayat_numbers.append(ayat_numbers)\n",
    "\n",
    "\n",
    "            # shaykh\n",
    "            shaykhs_of_paragraph = []\n",
    "            shaykhs = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_shaykhs = i.find_all('persName', {'ana' : 'shaykh'} )\n",
    "                shaykhs_of_paragraph = []\n",
    "                for a in paragraph_shaykhs:\n",
    "                    if paragraph_shaykhs == []:\n",
    "                        shaykhs_of_paragraph.append(\"no shaykh\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        shaykhs_of_paragraph.append(a)\n",
    "                shaykhs.append(shaykhs_of_paragraph)\n",
    "            list_of_list_shaykhs.append(shaykhs)\n",
    "\n",
    "\n",
    "            # rawi \n",
    "            rawis_of_paragraph = []\n",
    "            rawis = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_rawis = i.find_all('persName', {'ana' : 'rawi'} )\n",
    "                rawis_of_paragraph = []\n",
    "                for a in paragraph_rawis:\n",
    "                    if paragraph_rawis == []:\n",
    "                        rawis_of_paragraph.append(\"no rawi\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        rawis_of_paragraph.append(a)\n",
    "                rawis.append(rawis_of_paragraph)\n",
    "            list_of_list_rawis.append(rawis)\n",
    "        \n",
    "\n",
    "\n",
    "            # sahabi\n",
    "            sahabis_of_paragraph = []\n",
    "            sahabis = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_sahabis = i.find_all('persName', {'ana' : 'sahabi'} )\n",
    "                sahabis_of_paragraph = []\n",
    "                for a in paragraph_sahabis:\n",
    "                    if paragraph_sahabis == []:\n",
    "                        sahabis_of_paragraph.append(\"no sahabi\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        sahabis_of_paragraph.append(a)\n",
    "                sahabis.append(sahabis_of_paragraph)\n",
    "            list_of_list_sahabis.append(sahabis)\n",
    "\n",
    "\n",
    "            # NER person\n",
    "            persons_of_paragraph = []\n",
    "            persons = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_persons = i.find_all('name', {'role' : 'person'} )\n",
    "                #print(paragraph_persons)\n",
    "                persons_of_paragraph = []\n",
    "                for a in paragraph_persons:\n",
    "                    if paragraph_persons == []:\n",
    "                        persons_of_paragraph.append(\"no person\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        persons_of_paragraph.append(a)\n",
    "                #zaehler_person = zaehler_person + len(persons_of_paragraph)\n",
    "                persons.append(persons_of_paragraph)\n",
    "            list_of_list_persons.append(persons)\n",
    "            #print(list_of_list_persons)\n",
    "            \n",
    "\n",
    "\n",
    "            # NER location\n",
    "            locations_of_paragraph = []\n",
    "            locations = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_locations = i.find_all('name', {'role' : 'location'} )\n",
    "                locations_of_paragraph = []\n",
    "                for a in paragraph_locations:\n",
    "                    if paragraph_locations == []:\n",
    "                        locations_of_paragraph.append(\"no location\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        locations_of_paragraph.append(a)\n",
    "                locations.append(locations_of_paragraph)\n",
    "            list_of_list_locations.append(locations)\n",
    "            \n",
    "\n",
    "            # NER organization\n",
    "            organizations_of_paragraph = []\n",
    "            organizations = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_organizations = i.find_all('name', {'role' : 'organization'} )\n",
    "                organizations_of_paragraph = []\n",
    "                for a in paragraph_organizations:\n",
    "                    if paragraph_organizations == []:\n",
    "                        organizations_of_paragraph.append(\"no organization\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        organizations_of_paragraph.append(a)\n",
    "                organizations.append(organizations_of_paragraph)\n",
    "            list_of_list_organizations.append(organizations)\n",
    "            \n",
    "\n",
    "\n",
    "            # NER time\n",
    "            times_of_paragraph = []\n",
    "            times = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_times = i.find_all('name', {'role' : 'time'} )\n",
    "                times_of_paragraph = []\n",
    "                for a in paragraph_times:\n",
    "                    if paragraph_times == []:\n",
    "                        times_of_paragraph.append(\"no times\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        times_of_paragraph.append(a)\n",
    "                times.append(times_of_paragraph)\n",
    "            list_of_list_times.append(times)\n",
    "            \n",
    "\n",
    "            # NER other\n",
    "            others_of_paragraph = []\n",
    "            others = []\n",
    "            for i in paragraphs:\n",
    "                paragraph_others = i.find_all('name', {'role' : 'other'} )\n",
    "                others_of_paragraph = []\n",
    "                for a in paragraph_others:\n",
    "                    if paragraph_others == []:\n",
    "                        others_of_paragraph.append(\"no others\")\n",
    "                    else:\n",
    "                        a = str(a.text)\n",
    "                        a = a.replace('\\t', '')\n",
    "                        a = a.replace('\\n', ' ')\n",
    "                        a = re.sub('\\s+',' ',a)\n",
    "                        a = a.strip()\n",
    "                        others_of_paragraph.append(a)\n",
    "                others.append(others_of_paragraph)\n",
    "            list_of_list_others.append(others)\n",
    "            #print(list_of_list_others)\n",
    "\n",
    "\n",
    "            # xml:id\n",
    "            xml_id = []\n",
    "            for i in paragraphs:\n",
    "                try:\n",
    "                    b = i['xml:id']\n",
    "                    xml_id.append(b)\n",
    "                except: \n",
    "                    #print(i)\n",
    "                    xml_id.append(\"no paragraph id\")\n",
    "                    continue\n",
    "            list_of_list_xml_id.append(xml_id)\n",
    "            #print(list_of_list_xml_id)\n",
    "\n",
    "            # hadith / nothadith\n",
    "            hadith_list = []\n",
    "            for i in paragraphs:\n",
    "                try:\n",
    "                    c = i['n']\n",
    "                    hadith_list.append(c)\n",
    "                except:\n",
    "                    hadith_list.append(\"Hadith Not Defined\")\n",
    "            list_of_list_HadithNotHadith.append(hadith_list)\n",
    "\n",
    "\n",
    "            \n",
    "                \n",
    "\n",
    "        #print(zaehler_person)\n",
    "        # sura number\n",
    "        #sura_number = Bs_data.find_all('div', {'type':'chapter' })\n",
    "        suraNumber = sura_number[0]\n",
    "        sura_number_list = suraNumber.split(\".\")\n",
    "        suraNumber = sura_number_list[0]\n",
    "        #print(suraNumber)\n",
    "\n",
    "\n",
    "       # print(len(sub_sections))\n",
    "        #print(len(list_of_list_paragraphs))\n",
    "        #print(len(list_of_list_topics))\n",
    "        #print(len(list_of_list_xml_id))\n",
    "        #print(len(list_of_list_rawis))\n",
    "\n",
    "\n",
    "        with open (\"file2\", \"a+\",encoding='utf8') as target:\n",
    "            \n",
    "            \n",
    "            for a in range(len(sub_sections_liste)):\n",
    "                paragraphs_li = list_of_list_paragraphs[a]\n",
    "                liste_topics_li = list_of_list_topics[a]\n",
    "                subtopics_li = list_of_list_subtopics[a]\n",
    "                xml_id_li = list_of_list_xml_id[a]\n",
    "                list_ofHadith_li = list_of_list_HadithNotHadith[a]\n",
    "                ayat_numbers_li = list_of_list_ayat_numbers[a]\n",
    "                rawis_li = list_of_list_rawis[a]\n",
    "                sahabis_li = list_of_list_sahabis[a]\n",
    "                shaykhs_li = list_of_list_shaykhs[a]\n",
    "                persons_li = list_of_list_persons[a]\n",
    "                locations_li = list_of_list_locations[a]\n",
    "                organisations_li = list_of_list_organizations[a]\n",
    "                times_li = list_of_list_times[a]\n",
    "                others_li = list_of_list_others[a]\n",
    "                ayat_heads_li = list_of_list_heads_30[a]\n",
    "\n",
    "                #row_cvs = []\n",
    "\n",
    "                for i in range(len(list_of_list_paragraphs[a])):\n",
    "                    row_cvs = []\n",
    "\n",
    "                    # output xml:id\n",
    "                    xmliD = str(xml_id_li[i])\n",
    "                    #print(xmliD)\n",
    "                    target.write(xmliD)\n",
    "                    target.write(\"\\n\")\n",
    "                    row_cvs.append(xmliD)\n",
    "\n",
    "                    # output paragraphes\n",
    "                    paragraph = str(paragraphs_li[i].text)\n",
    "                    if paragraph == \"\":\n",
    "                        paragraph = \"No Text\"\n",
    "                    paragraph = paragraph.replace('\\t', '')\n",
    "                    paragraph = paragraph.replace('\\n', ' ')\n",
    "                    paragraph = re.sub('\\s+',' ',paragraph)\n",
    "                    paragraph = paragraph.strip()\n",
    "                    target.write(paragraph)\n",
    "                    row_cvs.append(paragraph)\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "                    # (sub-)section:\n",
    "                    target.write(sub_sections_liste[a])\n",
    "                    row_cvs.append(sub_sections_liste[a])\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "                    # sura number:\n",
    "                    target.write(suraNumber)\n",
    "                    target.write(\"\\n\")\n",
    "                    row_cvs.append(suraNumber)\n",
    "\n",
    "                    # ayat number\n",
    "                    ayat_n = ayat_heads_li[i]\n",
    "                    \n",
    "                    print(ayat_n)\n",
    "                    target.write(ayat_n)\n",
    "                    row_cvs.append(ayat_n)\n",
    "                    \n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "                    # ayat numbers in paragraphs \n",
    "                    ayat = ayat_numbers_li[i]\n",
    "                    aya_row = []\n",
    "                    if ayat == []:\n",
    "                        aya = \"no aya\"\n",
    "                        target.write(aya)\n",
    "                        row_cvs.append(aya)\n",
    "                    else:\n",
    "                        ayat = set(ayat)\n",
    "                        for aya in ayat:\n",
    "                            aya = aya.replace(\"yes\", \"\")\n",
    "                            aya = aya.replace(\",\", \" \")\n",
    "                            if aya == \"\":\n",
    "                                aya = \"no aya\"\n",
    "                            target.write(aya)\n",
    "                            aya_row.append(aya+\" \")\n",
    "                            target.write(\" \")\n",
    "                        aya_row = set(aya_row)\n",
    "                        aya_row = list(aya_row)\n",
    "                        aya_row.sort()\n",
    "                        aya_row_string = \" \".join(aya_row)\n",
    "                        row_cvs.append(aya_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # output topics\n",
    "                    topics = str(liste_topics_li[i])\n",
    "                    topics = topics.replace(\"yes\", \"\")\n",
    "                    if topics == \"\":\n",
    "                            topics = \"no topic\"\n",
    "                    target.write(topics)\n",
    "                    row_cvs.append(topics)\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "                    # output subtopics\n",
    "                    subt = subtopics_li[i]\n",
    "                    subtopic_row = []\n",
    "                    if subt == []:\n",
    "                        subtopic = \"no subtopic\"\n",
    "                        target.write(subtopic)\n",
    "                        row_cvs.append(subtopic)\n",
    "                    else:\n",
    "                        subt = set(subt)\n",
    "                        for subtopic in subt:\n",
    "                            subtopic = subtopic.replace(\"yes\", \"\")\n",
    "                            subtopic = subtopic.replace(\",\", \" \")\n",
    "                            if subtopic == \"\":\n",
    "                                subtopic = \"no subtopic\"\n",
    "                            target.write(subtopic)\n",
    "                            subtopic_row.append(subtopic+\",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        subtopic_row = set(subtopic_row)\n",
    "                        subtopic_row = list(subtopic_row)\n",
    "                        subtopic_row.sort()\n",
    "                        subtopic_row_string = \" \".join(subtopic_row)\n",
    "                        row_cvs.append(subtopic_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "                    # output sahabi\n",
    "                    sah = sahabis_li[i]\n",
    "                    #print(sah)\n",
    "                    sahabi_row = []\n",
    "                    if sah == []:\n",
    "                        sahabi = \"no sahabi\"\n",
    "                        target.write(sahabi)\n",
    "                        row_cvs.append(sahabi)\n",
    "                    else:\n",
    "                        #sah = set(sah)\n",
    "                        for sahabi in sah:\n",
    "                            #print(sahabi)\n",
    "                            #rawi = rawi.replace(\"yes\", \"\")\n",
    "                            #rawi = rawi.replace(\",\", \" \")\n",
    "                            if sahabi == \"\":\n",
    "                                sahabi = \"no sahabi\"\n",
    "                            target.write(sahabi)\n",
    "                            sahabi_row.append(sahabi+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        #sahabi_row = set(sahabi_row)\n",
    "                        #sahabi_row = list(sahabi_row)\n",
    "                        #sahabi_row.sort()\n",
    "                        sahabi_row_string = \" \".join(sahabi_row)\n",
    "                        row_cvs.append(sahabi_row_string)\n",
    "                        #print(sahabi_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "                    # output rawis\n",
    "                    raw = rawis_li[i]\n",
    "                    rawi_row = []\n",
    "                    if raw == []:\n",
    "                        rawi = \"no rawi\"\n",
    "                        target.write(rawi)\n",
    "                        row_cvs.append(rawi)\n",
    "                    else:\n",
    "                        #raw = set(raw)\n",
    "                        for rawi in raw:\n",
    "                            #rawi = rawi.replace(\"yes\", \"\")\n",
    "                            #rawi = rawi.replace(\",\", \" \")\n",
    "                            if rawi == \"\":\n",
    "                                rawi = \"no rawi\"\n",
    "                            target.write(rawi)\n",
    "                            rawi_row.append(rawi+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        #rawi_row = set(rawi_row)\n",
    "                        #rawi_row = list(rawi_row)\n",
    "                        #rawi_row.sort()\n",
    "                        rawi_row_string = \" \".join(rawi_row)\n",
    "                        row_cvs.append(rawi_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # output shaykhs\n",
    "                    sahay = shaykhs_li[i]\n",
    "                    #print(sah)\n",
    "                    shaykh_row = []\n",
    "                    if sahay == []:\n",
    "                        shaykh = \"no shaykh\"\n",
    "                        target.write(shaykh)\n",
    "                        row_cvs.append(shaykh)\n",
    "                    else:\n",
    "                        for shaykh in sahay:\n",
    "                            if shaykh == \"\":\n",
    "                                shaykh = \"no shaykh\"\n",
    "                            target.write(shaykh)\n",
    "                            shaykh_row.append(shaykh+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        shaykh_row_string = \" \".join(shaykh_row)\n",
    "                        row_cvs.append(shaykh_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "                    # output Ner persons\n",
    "                    per = persons_li[i]\n",
    "                    person_row = []\n",
    "                    if per == []:\n",
    "                        person = \"no person\"\n",
    "                        target.write(person)\n",
    "                        row_cvs.append(person)\n",
    "                    else:\n",
    "                        for person in per:\n",
    "                            if person == \"\":\n",
    "                                person = \"no person\"\n",
    "                            target.write(person)\n",
    "                            person_row.append(person+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        person_row = set(person_row)\n",
    "                        person_row = list(person_row)\n",
    "                        person_row.sort()\n",
    "                        person_row_string = \" \".join(person_row)\n",
    "                        row_cvs.append(person_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "                    # output Ner locations\n",
    "                    loc = locations_li[i]\n",
    "                    location_row = []\n",
    "                    if loc == []:\n",
    "                        location = \"no location\"\n",
    "                        target.write(location)\n",
    "                        row_cvs.append(location)\n",
    "                    else:\n",
    "                        for location in loc:\n",
    "                            if location == \"\":\n",
    "                                location = \"no location\"\n",
    "                            target.write(location)\n",
    "                            location_row.append(location+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        location_row = set(location_row)\n",
    "                        location_row = list(location_row)\n",
    "                        location_row.sort()\n",
    "                        location_row_string = \" \".join(location_row)\n",
    "                        row_cvs.append(location_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "                    # output Ner organisations\n",
    "                    org = organisations_li[i]\n",
    "                    organisation_row = []\n",
    "                    if org == []:\n",
    "                        organisation = \"no organisation\"\n",
    "                        target.write(organisation)\n",
    "                        row_cvs.append(organisation)\n",
    "                    else:\n",
    "                        for organisation in org:\n",
    "                            if organisation == \"\":\n",
    "                                organisation = \"no organisation\"\n",
    "                            target.write(organisation)\n",
    "                            organisation_row.append(organisation+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        organisation_row = set(organisation_row)\n",
    "                        organisation_row = list(organisation_row)\n",
    "                        organisation_row.sort()\n",
    "                        organisation_row_string = \" \".join(organisation_row)\n",
    "                        row_cvs.append(organisation_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "                    # output Ner time\n",
    "                    tim = times_li[i]\n",
    "                    time_row = []\n",
    "                    if tim == []:\n",
    "                        time = \"no time\"\n",
    "                        target.write(time)\n",
    "                        row_cvs.append(time)\n",
    "                    else:\n",
    "                        for time in tim:\n",
    "                            if time == \"\":\n",
    "                                time = \"no time\"\n",
    "                            target.write(time)\n",
    "                            time_row.append(time+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        time_row = set(time_row)\n",
    "                        time_row = list(time_row)\n",
    "                        time_row.sort()\n",
    "                        time_row_string = \" \".join(time_row)\n",
    "                        row_cvs.append(time_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "\n",
    "                    # output Ner other\n",
    "                    oth = others_li[i]\n",
    "                    other_row = []\n",
    "                    if oth == []:\n",
    "                        other = \"no others\"\n",
    "                        target.write(other)\n",
    "                        row_cvs.append(other)\n",
    "                    else:\n",
    "                        for other in oth:\n",
    "                            if other == \"\":\n",
    "                                other = \"no others\"\n",
    "                            target.write(other)\n",
    "                            other_row.append(other+ \",\"+\" \")\n",
    "                            target.write(\" \")\n",
    "                        other_row = set(other_row)\n",
    "                        other_row = list(other_row)\n",
    "                        other_row.sort()\n",
    "                        other_row_string = \" \".join(other_row)\n",
    "                        row_cvs.append(other_row_string)\n",
    "                    target.write(\"\\n\")\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "                    # hadith / not Hadith\n",
    "                    hadith = str(list_ofHadith_li[i])\n",
    "                    if hadith == \"\":\n",
    "                        hadith = \"No Hadith definition\"\n",
    "                    hadith = hadith.replace('\\t', '')\n",
    "                    hadith = hadith.replace('\\n', ' ')\n",
    "                    hadith = re.sub('\\s+',' ',hadith)\n",
    "                    hadith = hadith.strip()\n",
    "                    target.write(hadith)\n",
    "                    row_cvs.append(hadith)\n",
    "                    target.write(\"\\n\")\n",
    "\n",
    "                    list_of_list_for_cvs.append(row_cvs)\n",
    "\n",
    "    # clear result/ delete double paragraphs\n",
    "    list_of_list_for_cvs_reverse = list_of_list_for_cvs[::-1]\n",
    "    list_of_list_for_cvs_clear = []\n",
    "    end_flag = 0\n",
    "    for liste in list_of_list_for_cvs_reverse:\n",
    "        if list_of_list_for_cvs_clear == []:\n",
    "            list_of_list_for_cvs_clear.append(liste)\n",
    "        else:\n",
    "            check_line = liste[1]\n",
    "            for liste_new in list_of_list_for_cvs_clear:\n",
    "                if check_line == liste_new[1]:\n",
    "                    end_flag = 1\n",
    "                    break\n",
    "            if end_flag == 1:\n",
    "                end_flag = 0\n",
    "                continue\n",
    "            else:\n",
    "                list_of_list_for_cvs_clear.append(liste)\n",
    "    list_of_list_for_cvs_clear_reverse = list_of_list_for_cvs_clear[::-1]\n",
    "\n",
    "    return list_of_list_for_cvs_clear_reverse, zaehler_topics, zaehler_subtopics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aeeb7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S100M\\\\sure_100_section_100.1-100.1.xml'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.index('D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S100M\\\\sure_100_section_100.1-100.1.xml')\n",
    "files[196]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868014b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(files[196], 'r', encoding='utf8') as f: \n",
    "    xml_file_as_string = f.read()\n",
    "Bs_data = BeautifulSoup(xml_file_as_string, \"xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c09e2907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_i = []\n",
    "# import re\n",
    "\n",
    "# tag = Bs_data.find_all(\"name\")\n",
    "# root = ET.fromstring(xml_file_as_string)\n",
    "# for i in tag:\n",
    "# #     print(i)\n",
    "#     O = i.get(\"role\")\n",
    "#     if O in \"organiztion\":\n",
    "# #         print(i)\n",
    "#         list_i.append(i)\n",
    "# #         xml_data = str(i)\n",
    "\n",
    "# s = set(list_i)\n",
    "# len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f450eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6de57ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_i = []\n",
    "import re\n",
    "\n",
    "tag = Bs_data.find_all(\"name\")\n",
    "root = ET.fromstring(xml_file_as_string)\n",
    "for i in tag:\n",
    "#     print(i)\n",
    "#     O = i.get(\"role\")\n",
    "#     if O == \"organization\":\n",
    "#         print(i)\n",
    "    list_i.append(i)\n",
    "#         xml_data = str(i)\n",
    "\n",
    "s = set(list_i)\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848a1a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<name role=\"location\">Ø§Ù„ÙƒÙˆÙØ©</name>,\n",
       " <name role=\"location\">Ø§Ù„Ù’Ø­ÙØ¬Ù’Ø±Ù</name>,\n",
       " <name role=\"location\">Ø§Ù„Ù’Ù…ÙØ²Ù’Ø¯ÙŽÙ„ÙÙÙŽØ©Ù</name>,\n",
       " <name role=\"location\">Ø¨ÙŽØ¯Ù’Ø±ÙŒ</name>,\n",
       " <name role=\"location\">Ø³ÙÙ‚ÙŽØ§ÙŠÙŽØ©Ù\n",
       " \t\t\t\t\t\t\t\tØ²ÙŽÙ…Ù’Ø²ÙŽÙ…ÙŽ</name>,\n",
       " <name role=\"location\">Ø¹ÙŽØ±ÙŽÙÙŽØ©ÙŽ</name>,\n",
       " <name role=\"location\">Ù…ÙØ²Ù’Ø¯ÙŽÙ„ÙÙÙŽØ©ÙŽ</name>,\n",
       " <name role=\"location\">Ù…ÙÙ†Ù‹Ù‰</name>,\n",
       " <name role=\"organization\"><seg ana=\"kalamterm yes\">Ø§Ù„Ù’ÙƒÙÙÙ‘ÙŽØ§Ø±Ù</seg></name>,\n",
       " <name role=\"organization\">Ø£Ù‡Ù„ Ø§Ù„ØªØ£ÙˆÙŠÙ„</name>,\n",
       " <name role=\"organization\">Ø£Ù‡Ù„ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</name>,\n",
       " <name role=\"organization\">Ø£ÙŽÙ‡Ù’Ù„Ù\n",
       " \t\t\t\t\t\t\tØ§Ù„ØªÙ‘ÙŽØ£Ù’ÙˆÙÙŠÙ„Ù</name>,\n",
       " <name role=\"organization\">Ø£ÙŽÙ‡Ù’Ù„Ù Ø§Ù„ØªÙ‘ÙŽØ£Ù’ÙˆÙÙŠÙ„Ù</name>,\n",
       " <name role=\"organization\">Ø§Ù„Ø¨ØµØ±ÙŠÙŠÙ†</name>,\n",
       " <name role=\"organization\">Ø¹ÙŽØ±ÙŽØ¨Ù</name>,\n",
       " <name role=\"organization\">Ù†Ø­ÙˆÙŠÙŠ</name>,\n",
       " <name role=\"other\">Ø³ÙˆØ±Ø© ( Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ…</name>,\n",
       " <name role=\"other\">Ø³ÙˆØ±Ø© ÙˆØ§Ù„Ø¹Ø§Ø¯ÙŠØ§Øª</name>,\n",
       " <name role=\"person\"><seg ana=\"qari\">Ø¹ÙŽØ¨Ù’Ø¯Ù Ø§Ù„Ù„Ù‘ÙŽÙ‡Ù</seg></name>,\n",
       " <name role=\"person\">Ø§Ø¨Ù† Ø¹Ø¨Ø§Ø³</name>,\n",
       " <name role=\"person\">Ø§Ø¨Ù’Ù†ÙŽ Ø¹ÙŽØ¨Ù‘ÙŽØ§Ø³Ù</name>,\n",
       " <name role=\"person\">Ø§Ø¨Ù’Ù†Ù Ø¹ÙŽØ¨Ù‘ÙŽØ§Ø³Ù</name>,\n",
       " <name role=\"person\">Ø§Ø¨Ù’Ù†Ù\n",
       " \t\t\t\t\t\t\t\tØ¹ÙŽØ¨Ù‘ÙŽØ§Ø³Ù</name>,\n",
       " <name role=\"person\">Ø§Ù„Ø£Ø¹Ø´Ù‰</name>,\n",
       " <name role=\"person\">Ø§Ù„Ù’ÙƒÙŽÙ„Ù’Ø¨ÙÙŠÙ‘Ù</name>,\n",
       " <name role=\"person\">Ø¨ÙØ´Ù’Ø±Ù</name>,\n",
       " <name role=\"person\">Ø²ÙŽÙŠÙ’Ø¯Ù Ø¨Ù’Ù†Ù Ø£ÙŽØ³Ù’Ù„ÙŽÙ…ÙŽ</name>,\n",
       " <name role=\"person\">Ø²Ù‘ÙØ¨ÙŽÙŠÙ’</name>,\n",
       " <name role=\"person\">Ø³ÙŽØ¹ÙÙŠØ¯Ù</name>,\n",
       " <name role=\"person\">Ø·ÙŽØ±ÙŽÙÙŽØ©ÙŽ Ø¨Ù’Ù†Ù\n",
       " \t\t\t\t\t\t\tØ§Ù„Ù’Ø¹ÙŽØ¨Ù’Ø¯Ù Ø§Ù„Ù’ÙŠÙŽØ´Ù’ÙƒÙØ±ÙÙŠÙ‘Ù</name>,\n",
       " <name role=\"person\">Ø¹ÙŽÙ„ÙÙŠÙ‘ÙŒ</name>,\n",
       " <name role=\"person\">ÙƒÙÙ†Ù’Ø¯ÙŽØ©ÙŽ</name>,\n",
       " <name role=\"person\">ÙƒÙÙ†Ù’Ø¯ÙŽØ©Ù</name>,\n",
       " <name role=\"person\">Ù…ÙÙ‚Ù’Ø¯ÙŽØ§Ø¯Ù</name>,\n",
       " <name role=\"person\">ÙŠÙŽØ²ÙÙŠØ¯ÙŽ</name>,\n",
       " <name role=\"time\"><seg ana=\"hajj yes\">Ø§Ù„Ù’Ø­ÙŽØ¬Ù‘Ù</seg></name>,\n",
       " <name role=\"time\">Ø§Ù„Ù„Ù‘ÙŽÙŠÙ’Ù„Ù</name>,\n",
       " <name role=\"time\">ØµØ¨Ø­Ø§</name>,\n",
       " <name role=\"time\">ØµÙØ¨Ù’Ø­Ù‹Ø§</name>,\n",
       " <name role=\"time\">ÙŠÙŽÙˆÙ’Ù…ÙŽ Ø§Ù„Ù†Ù‘ÙŽØ­Ù’Ø±Ù</name>,\n",
       " <name type=\"sura\">ÙˆØ§Ù„Ø¹Ø§Ø¯ÙŠØ§Øª </name>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c01b301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_i = []\n",
    "import re\n",
    "a = ['']\n",
    "\n",
    "\n",
    "\n",
    "for xml_f in a:\n",
    "    # extracting info from xml\n",
    "    with open(xml_f, 'r', encoding='utf8') as f:     \n",
    "        xml_file_as_string = f.read()\n",
    "\n",
    "    Bs_data = BeautifulSoup(xml_file_as_string, \"xml\")\n",
    "    tag = Bs_data.find_all(\"head\")\n",
    "    root = ET.fromstring(xml_file_as_string)\n",
    "    for i in tag:\n",
    "    #     print(i)\n",
    "        O = i.get(\"quote\")\n",
    "        if O == \"quran\":\n",
    "            print(processText(removeDiacritics(str(i))))\n",
    "            print('\\n\\n')\n",
    "            list_i.append(processText(removeDiacritics(i.text)))\n",
    "\n",
    "s = set(list_i)\n",
    "s = list(s)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9f8ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(list_i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a48d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_numbers_and_slashes(text):\n",
    "    # Use regular expression to remove numbers and slashes\n",
    "    cleaned_text = re.sub(r'[0-9/-]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_leading_spaces_list(input_list):\n",
    "    return [item.strip() for item in input_list]\n",
    "\n",
    "# text processing function\n",
    "def removeDiacritics(text):\n",
    "    for a in pyarabic.araby.DIACRITICS:\n",
    "        text = text.replace(a, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def processText(text):\n",
    "    text = str(text)\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    while text.find('  ') != -1:\n",
    "        text = text.replace('  ', ' ')\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def removeSpace(text: str) -> str:\n",
    "    while text.find(' ') != -1:\n",
    "        text = text.replace(' ', '')\n",
    "\n",
    "    return text\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def lists_have_same_elements(list1, list2):\n",
    "    counter1 = Counter(list1)\n",
    "    counter2 = Counter(list2)\n",
    "    \n",
    "    return counter1 == counter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4b7cc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:003, t: Ø®ÙŽÙ„ÙŽÙ‚ÙŽ Ø§Ù„Ù’Ø¥ÙÙ†Ù’Ø³ÙŽØ§Ù†ÙŽ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:004, t: Ø¹ÙŽÙ„Ù‘ÙŽÙ…ÙŽÙ‡Ù Ø§Ù„Ù’Ø¨ÙŽÙŠÙŽØ§Ù†ÙŽ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:005, t: Ø§Ù„Ø´Ù‘ÙŽÙ…Ù’Ø³Ù ÙˆÙŽØ§Ù„Ù’Ù‚ÙŽÙ…ÙŽØ±Ù Ø¨ÙØ­ÙØ³Ù’Ø¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:001, t: Ø¨ÙØ³Ù’Ù…Ù Ø§Ù„Ù„Ù‘ÙŽÙ‡Ù Ø§Ù„Ø±Ù‘ÙŽØ­Ù’Ù…ÙŽÙ°Ù†Ù Ø§Ù„Ø±Ù‘ÙŽØ­ÙÙŠÙ…Ù Ø§Ù„Ø±Ù‘ÙŽØ­Ù’Ù…ÙŽÙ°Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:002, t: Ø¹ÙŽÙ„Ù‘ÙŽÙ…ÙŽ Ø§Ù„Ù’Ù‚ÙØ±Ù’Ø¢Ù†ÙŽ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:006, t: ÙˆÙŽØ§Ù„Ù†Ù‘ÙŽØ¬Ù’Ù…Ù ÙˆÙŽØ§Ù„Ø´Ù‘ÙŽØ¬ÙŽØ±Ù ÙŠÙŽØ³Ù’Ø¬ÙØ¯ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:007, t: ÙˆÙŽØ§Ù„Ø³Ù‘ÙŽÙ…ÙŽØ§Ø¡ÙŽ Ø±ÙŽÙÙŽØ¹ÙŽÙ‡ÙŽØ§ ÙˆÙŽÙˆÙŽØ¶ÙŽØ¹ÙŽ Ø§Ù„Ù’Ù…ÙÙŠØ²ÙŽØ§Ù†ÙŽ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:008, t: Ø£ÙŽÙ„Ù‘ÙŽØ§ ØªÙŽØ·Ù’ØºÙŽÙˆÙ’Ø§ ÙÙÙŠ Ø§Ù„Ù’Ù…ÙÙŠØ²ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:009, t: ÙˆÙŽØ£ÙŽÙ‚ÙÙŠÙ…ÙÙˆØ§ Ø§Ù„Ù’ÙˆÙŽØ²Ù’Ù†ÙŽ Ø¨ÙØ§Ù„Ù’Ù‚ÙØ³Ù’Ø·Ù ÙˆÙŽÙ„ÙŽØ§ ØªÙØ®Ù’Ø³ÙØ±ÙÙˆØ§ Ø§Ù„Ù’Ù…ÙÙŠØ²ÙŽØ§Ù†ÙŽ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:010, t: ÙˆÙŽØ§Ù„Ù’Ø£ÙŽØ±Ù’Ø¶ÙŽ ÙˆÙŽØ¶ÙŽØ¹ÙŽÙ‡ÙŽØ§ Ù„ÙÙ„Ù’Ø£ÙŽÙ†ÙŽØ§Ù…Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:011, t: ÙÙÙŠÙ‡ÙŽØ§ ÙÙŽØ§ÙƒÙÙ‡ÙŽØ©ÙŒ ÙˆÙŽØ§Ù„Ù†Ù‘ÙŽØ®Ù’Ù„Ù Ø°ÙŽØ§ØªÙ Ø§Ù„Ù’Ø£ÙŽÙƒÙ’Ù…ÙŽØ§Ù…Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:012, t: ÙˆÙŽØ§Ù„Ù’Ø­ÙŽØ¨Ù‘Ù Ø°ÙÙˆ Ø§Ù„Ù’Ø¹ÙŽØµÙ’ÙÙ ÙˆÙŽØ§Ù„Ø±Ù‘ÙŽÙŠÙ’Ø­ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:013, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:014, t: Ø®ÙŽÙ„ÙŽÙ‚ÙŽ Ø§Ù„Ù’Ø¥ÙÙ†Ù’Ø³ÙŽØ§Ù†ÙŽ Ù…ÙÙ†Ù’ ØµÙŽÙ„Ù’ØµÙŽØ§Ù„Ù ÙƒÙŽØ§Ù„Ù’ÙÙŽØ®Ù‘ÙŽØ§Ø±Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:015, t: ÙˆÙŽØ®ÙŽÙ„ÙŽÙ‚ÙŽ Ø§Ù„Ù’Ø¬ÙŽØ§Ù†Ù‘ÙŽ Ù…ÙÙ†Ù’ Ù…ÙŽØ§Ø±ÙØ¬Ù Ù…ÙÙ†Ù’ Ù†ÙŽØ§Ø±Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:016, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:017, t: Ø±ÙŽØ¨Ù‘Ù Ø§Ù„Ù’Ù…ÙŽØ´Ù’Ø±ÙÙ‚ÙŽÙŠÙ’Ù†Ù ÙˆÙŽØ±ÙŽØ¨Ù‘Ù Ø§Ù„Ù’Ù…ÙŽØºÙ’Ø±ÙØ¨ÙŽÙŠÙ’Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:018, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:019, t: Ù…ÙŽØ±ÙŽØ¬ÙŽ Ø§Ù„Ù’Ø¨ÙŽØ­Ù’Ø±ÙŽÙŠÙ’Ù†Ù ÙŠÙŽÙ„Ù’ØªÙŽÙ‚ÙÙŠÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:020, t: Ø¨ÙŽÙŠÙ’Ù†ÙŽÙ‡ÙÙ…ÙŽØ§ Ø¨ÙŽØ±Ù’Ø²ÙŽØ®ÙŒ Ù„ÙŽØ§ ÙŠÙŽØ¨Ù’ØºÙÙŠÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:021, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:022, t: ÙŠÙŽØ®Ù’Ø±ÙØ¬Ù Ù…ÙÙ†Ù’Ù‡ÙÙ…ÙŽØ§ Ø§Ù„Ù„Ù‘ÙØ¤Ù’Ù„ÙØ¤Ù ÙˆÙŽØ§Ù„Ù’Ù…ÙŽØ±Ù’Ø¬ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:023, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:024, t: ÙˆÙŽÙ„ÙŽÙ‡Ù Ø§Ù„Ù’Ø¬ÙŽÙˆÙŽØ§Ø±Ù Ø§Ù„Ù’Ù…ÙÙ†Ù’Ø´ÙŽØ¢ØªÙ ÙÙÙŠ Ø§Ù„Ù’Ø¨ÙŽØ­Ù’Ø±Ù ÙƒÙŽØ§Ù„Ù’Ø£ÙŽØ¹Ù’Ù„ÙŽØ§Ù…Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:025, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:026, t: ÙƒÙÙ„Ù‘Ù Ù…ÙŽÙ†Ù’ Ø¹ÙŽÙ„ÙŽÙŠÙ’Ù‡ÙŽØ§ ÙÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:027, t: ÙˆÙŽÙŠÙŽØ¨Ù’Ù‚ÙŽÙ‰Ù° ÙˆÙŽØ¬Ù’Ù‡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙŽ Ø°ÙÙˆ Ø§Ù„Ù’Ø¬ÙŽÙ„ÙŽØ§Ù„Ù ÙˆÙŽØ§Ù„Ù’Ø¥ÙÙƒÙ’Ø±ÙŽØ§Ù…Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:028, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:030, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:029, t: ÙŠÙŽØ³Ù’Ø£ÙŽÙ„ÙÙ‡Ù Ù…ÙŽÙ†Ù’ ÙÙÙŠ Ø§Ù„Ø³Ù‘ÙŽÙ…ÙŽØ§ÙˆÙŽØ§ØªÙ ÙˆÙŽØ§Ù„Ù’Ø£ÙŽØ±Ù’Ø¶Ù Ûš ÙƒÙÙ„Ù‘ÙŽ ÙŠÙŽÙˆÙ’Ù…Ù Ù‡ÙÙˆÙŽ ÙÙÙŠ Ø´ÙŽØ£Ù’Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:032, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:031, t: Ø³ÙŽÙ†ÙŽÙÙ’Ø±ÙØºÙ Ù„ÙŽÙƒÙÙ…Ù’ Ø£ÙŽÙŠÙ‘ÙÙ‡ÙŽ Ø§Ù„Ø«Ù‘ÙŽÙ‚ÙŽÙ„ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:034, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:033, t: ÙŠÙŽØ§ Ù…ÙŽØ¹Ù’Ø´ÙŽØ±ÙŽ Ø§Ù„Ù’Ø¬ÙÙ†Ù‘Ù ÙˆÙŽØ§Ù„Ù’Ø¥ÙÙ†Ù’Ø³Ù Ø¥ÙÙ†Ù Ø§Ø³Ù’ØªÙŽØ·ÙŽØ¹Ù’ØªÙÙ…Ù’ Ø£ÙŽÙ†Ù’ ØªÙŽÙ†Ù’ÙÙØ°ÙÙˆØ§ Ù…ÙÙ†Ù’ Ø£ÙŽÙ‚Ù’Ø·ÙŽØ§Ø±Ù Ø§Ù„Ø³Ù‘ÙŽÙ…ÙŽØ§ÙˆÙŽØ§ØªÙ ÙˆÙŽØ§Ù„Ù’Ø£ÙŽØ±Ù’Ø¶Ù ÙÙŽØ§Ù†Ù’ÙÙØ°ÙÙˆØ§ Ûš Ù„ÙŽØ§ ØªÙŽÙ†Ù’ÙÙØ°ÙÙˆÙ†ÙŽ Ø¥ÙÙ„Ù‘ÙŽØ§ Ø¨ÙØ³ÙÙ„Ù’Ø·ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:035, t: ÙŠÙØ±Ù’Ø³ÙŽÙ„Ù Ø¹ÙŽÙ„ÙŽÙŠÙ’ÙƒÙÙ…ÙŽØ§ Ø´ÙÙˆÙŽØ§Ø¸ÙŒ Ù…ÙÙ†Ù’ Ù†ÙŽØ§Ø±Ù ÙˆÙŽÙ†ÙØ­ÙŽØ§Ø³ÙŒ ÙÙŽÙ„ÙŽØ§ ØªÙŽÙ†Ù’ØªÙŽØµÙØ±ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:036, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:037, t: ÙÙŽØ¥ÙØ°ÙŽØ§ Ø§Ù†Ù’Ø´ÙŽÙ‚Ù‘ÙŽØªÙ Ø§Ù„Ø³Ù‘ÙŽÙ…ÙŽØ§Ø¡Ù ÙÙŽÙƒÙŽØ§Ù†ÙŽØªÙ’ ÙˆÙŽØ±Ù’Ø¯ÙŽØ©Ù‹ ÙƒÙŽØ§Ù„Ø¯Ù‘ÙÙ‡ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:038, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:039, t: ÙÙŽÙŠÙŽÙˆÙ’Ù…ÙŽØ¦ÙØ°Ù Ù„ÙŽØ§ ÙŠÙØ³Ù’Ø£ÙŽÙ„Ù Ø¹ÙŽÙ†Ù’ Ø°ÙŽÙ†Ù’Ø¨ÙÙ‡Ù Ø¥ÙÙ†Ù’Ø³ÙŒ ÙˆÙŽÙ„ÙŽØ§ Ø¬ÙŽØ§Ù†Ù‘ÙŒ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:040, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:041, t: ÙŠÙØ¹Ù’Ø±ÙŽÙÙ Ø§Ù„Ù’Ù…ÙØ¬Ù’Ø±ÙÙ…ÙÙˆÙ†ÙŽ Ø¨ÙØ³ÙÙŠÙ…ÙŽØ§Ù‡ÙÙ…Ù’ ÙÙŽÙŠÙØ¤Ù’Ø®ÙŽØ°Ù Ø¨ÙØ§Ù„Ù†Ù‘ÙŽÙˆÙŽØ§ØµÙÙŠ ÙˆÙŽØ§Ù„Ù’Ø£ÙŽÙ‚Ù’Ø¯ÙŽØ§Ù…Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:042, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:043, t: Ù‡ÙŽÙ°Ø°ÙÙ‡Ù Ø¬ÙŽÙ‡ÙŽÙ†Ù‘ÙŽÙ…Ù Ø§Ù„Ù‘ÙŽØªÙÙŠ ÙŠÙÙƒÙŽØ°Ù‘ÙØ¨Ù Ø¨ÙÙ‡ÙŽØ§ Ø§Ù„Ù’Ù…ÙØ¬Ù’Ø±ÙÙ…ÙÙˆÙ†ÙŽ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:044, t: ÙŠÙŽØ·ÙÙˆÙÙÙˆÙ†ÙŽ Ø¨ÙŽÙŠÙ’Ù†ÙŽÙ‡ÙŽØ§ ÙˆÙŽØ¨ÙŽÙŠÙ’Ù†ÙŽ Ø­ÙŽÙ…ÙÙŠÙ…Ù Ø¢Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:045, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:046, t: ÙˆÙŽÙ„ÙÙ…ÙŽÙ†Ù’ Ø®ÙŽØ§ÙÙŽ Ù…ÙŽÙ‚ÙŽØ§Ù…ÙŽ Ø±ÙŽØ¨Ù‘ÙÙ‡Ù Ø¬ÙŽÙ†Ù‘ÙŽØªÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:047, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:048, t: Ø°ÙŽÙˆÙŽØ§ØªÙŽØ§ Ø£ÙŽÙÙ’Ù†ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:049, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:050, t: ÙÙÙŠÙ‡ÙÙ…ÙŽØ§ Ø¹ÙŽÙŠÙ’Ù†ÙŽØ§Ù†Ù ØªÙŽØ¬Ù’Ø±ÙÙŠÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:051, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:052, t: ÙÙÙŠÙ‡ÙÙ…ÙŽØ§ Ù…ÙÙ†Ù’ ÙƒÙÙ„Ù‘Ù ÙÙŽØ§ÙƒÙÙ‡ÙŽØ©Ù Ø²ÙŽÙˆÙ’Ø¬ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:053, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:054, t: Ù…ÙØªÙ‘ÙŽÙƒÙØ¦ÙÙŠÙ†ÙŽ Ø¹ÙŽÙ„ÙŽÙ‰Ù° ÙÙØ±ÙØ´Ù Ø¨ÙŽØ·ÙŽØ§Ø¦ÙÙ†ÙÙ‡ÙŽØ§ Ù…ÙÙ†Ù’ Ø¥ÙØ³Ù’ØªÙŽØ¨Ù’Ø±ÙŽÙ‚Ù Ûš ÙˆÙŽØ¬ÙŽÙ†ÙŽÙ‰ Ø§Ù„Ù’Ø¬ÙŽÙ†Ù‘ÙŽØªÙŽÙŠÙ’Ù†Ù Ø¯ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:055, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:056, t: ÙÙÙŠÙ‡ÙÙ†Ù‘ÙŽ Ù‚ÙŽØ§ØµÙØ±ÙŽØ§ØªÙ Ø§Ù„Ø·Ù‘ÙŽØ±Ù’ÙÙ Ù„ÙŽÙ…Ù’ ÙŠÙŽØ·Ù’Ù…ÙØ«Ù’Ù‡ÙÙ†Ù‘ÙŽ Ø¥ÙÙ†Ù’Ø³ÙŒ Ù‚ÙŽØ¨Ù’Ù„ÙŽÙ‡ÙÙ…Ù’ ÙˆÙŽÙ„ÙŽØ§ Ø¬ÙŽØ§Ù†Ù‘ÙŒ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:057, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:058, t: ÙƒÙŽØ£ÙŽÙ†Ù‘ÙŽÙ‡ÙÙ†Ù‘ÙŽ Ø§Ù„Ù’ÙŠÙŽØ§Ù‚ÙÙˆØªÙ ÙˆÙŽØ§Ù„Ù’Ù…ÙŽØ±Ù’Ø¬ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:059, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:060, t: Ù‡ÙŽÙ„Ù’ Ø¬ÙŽØ²ÙŽØ§Ø¡Ù Ø§Ù„Ù’Ø¥ÙØ­Ù’Ø³ÙŽØ§Ù†Ù Ø¥ÙÙ„Ù‘ÙŽØ§ Ø§Ù„Ù’Ø¥ÙØ­Ù’Ø³ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:061, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:062, t: ÙˆÙŽÙ…ÙÙ†Ù’ Ø¯ÙÙˆÙ†ÙÙ‡ÙÙ…ÙŽØ§ Ø¬ÙŽÙ†Ù‘ÙŽØªÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:063, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:064, t: Ù…ÙØ¯Ù’Ù‡ÙŽØ§Ù…Ù‘ÙŽØªÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:065, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:066, t: ÙÙÙŠÙ‡ÙÙ…ÙŽØ§ Ø¹ÙŽÙŠÙ’Ù†ÙŽØ§Ù†Ù Ù†ÙŽØ¶Ù‘ÙŽØ§Ø®ÙŽØªÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:067, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:068, t: ÙÙÙŠÙ‡ÙÙ…ÙŽØ§ ÙÙŽØ§ÙƒÙÙ‡ÙŽØ©ÙŒ ÙˆÙŽÙ†ÙŽØ®Ù’Ù„ÙŒ ÙˆÙŽØ±ÙÙ…Ù‘ÙŽØ§Ù†ÙŒ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:069, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:070, t: ÙÙÙŠÙ‡ÙÙ†Ù‘ÙŽ Ø®ÙŽÙŠÙ’Ø±ÙŽØ§ØªÙŒ Ø­ÙØ³ÙŽØ§Ù†ÙŒ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:071, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:072, t: Ø­ÙÙˆØ±ÙŒ Ù…ÙŽÙ‚Ù’ØµÙÙˆØ±ÙŽØ§ØªÙŒ ÙÙÙŠ Ø§Ù„Ù’Ø®ÙÙŠÙŽØ§Ù…Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:073, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:074, t: Ù„ÙŽÙ…Ù’ ÙŠÙŽØ·Ù’Ù…ÙØ«Ù’Ù‡ÙÙ†Ù‘ÙŽ Ø¥ÙÙ†Ù’Ø³ÙŒ Ù‚ÙŽØ¨Ù’Ù„ÙŽÙ‡ÙÙ…Ù’ ÙˆÙŽÙ„ÙŽØ§ Ø¬ÙŽØ§Ù†Ù‘ÙŒ\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:075, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:076, t: Ù…ÙØªÙ‘ÙŽÙƒÙØ¦ÙÙŠÙ†ÙŽ Ø¹ÙŽÙ„ÙŽÙ‰Ù° Ø±ÙŽÙÙ’Ø±ÙŽÙÙ Ø®ÙØ¶Ù’Ø±Ù ÙˆÙŽØ¹ÙŽØ¨Ù’Ù‚ÙŽØ±ÙÙŠÙ‘Ù Ø­ÙØ³ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:077, t: ÙÙŽØ¨ÙØ£ÙŽÙŠÙ‘Ù Ø¢Ù„ÙŽØ§Ø¡Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙÙ…ÙŽØ§ ØªÙÙƒÙŽØ°Ù‘ÙØ¨ÙŽØ§Ù†Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_055, n: http://www.tafsirtabari.com/ontology#V055:078, t: ØªÙŽØ¨ÙŽØ§Ø±ÙŽÙƒÙŽ Ø§Ø³Ù’Ù…Ù Ø±ÙŽØ¨Ù‘ÙÙƒÙŽ Ø°ÙÙŠ Ø§Ù„Ù’Ø¬ÙŽÙ„ÙŽØ§Ù„Ù ÙˆÙŽØ§Ù„Ù’Ø¥ÙÙƒÙ’Ø±ÙŽØ§Ù…Ù\n"
     ]
    }
   ],
   "source": [
    "# querying from graph\n",
    "    \n",
    "# Create a new RDF graph\n",
    "graph = Graph()\n",
    "\n",
    "graph_file = 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\A-Box\\\\Tafsir Al-Tabari A-Box S55.owl'\n",
    "\n",
    "\n",
    "# Parse the RDF data from a file or URL\n",
    "graph.parse(graph_file)\n",
    "\n",
    "# Define the SPARQL query\n",
    "query = \"\"\"\n",
    "PREFIX : <http://www.tafsirtabari.com/ontology#>\n",
    "PREFIX W3:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "SELECT DISTINCT *\n",
    "WHERE {\n",
    "\n",
    " ?k rdf:type :Chapter.\n",
    "?k :containsVerse ?v.\n",
    "?v :hasText ?t.\n",
    "}\n",
    "\"\"\"\n",
    "R = []\n",
    "# Execute the SPARQL query\n",
    "results = graph.query(query)\n",
    "# Process the query results\n",
    "for row in results:\n",
    "    # Access the query variables\n",
    "    s = row[\"k\"]\n",
    "    n = row[\"v\"]\n",
    "    t = row[\"t\"]\n",
    "    \n",
    "    print(f\"s: {s}, n: {n}, t: {t}\")\n",
    "    R.append(str(t))\n",
    "\n",
    "hadiths_from_graph = list(set(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7973887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning of xml and graph\n",
    "\n",
    "#graph\n",
    "cleaned_hadiths_from_graph = []\n",
    "for i in hadiths_from_graph:\n",
    "    a = remove_numbers_and_slashes(i)\n",
    "    cleaned_hadiths_from_graph.append(removeDiacritics(a))\n",
    "\n",
    "# xml\n",
    "list_i_h = []\n",
    "for i in list_i:\n",
    "    filtered_list = remove_numbers_and_slashes(i)\n",
    "    list_i_h.append(filtered_list)\n",
    "\n",
    "# removing leading spaces\n",
    "Hadiths_from_xml = remove_leading_spaces_list(list_i_h)\n",
    "\n",
    "cleaned_hadiths_from_graph.sort()\n",
    "Hadiths_from_xml.sort()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4d580e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø£Ù„Ø§ ØªØ·ØºÙˆØ§ ÙÙŠ Ø§Ù„Ù…ÙŠØ²Ø§Ù†\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# manual comparing\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_hadiths_from_graph[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mHadiths_from_xml\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_hadiths_from_graph[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m Hadiths_from_xml[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# manual comparing\n",
    "print(cleaned_hadiths_from_graph[0])\n",
    "print(Hadiths_from_xml[1])\n",
    "print(cleaned_hadiths_from_graph[0] == Hadiths_from_xml[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9cdf28b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(cleaned_hadiths_from_graph),len(Hadiths_from_xml))\n",
    "lists_have_same_elements(cleaned_hadiths_from_graph,Hadiths_from_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ea6d7d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¥Ù† ÙƒØ§Ø¯ Ù„ÙŠØ¶Ù„Ù†Ø§ Ø¹Ù† Ø¢Ù„Ù‡ØªÙ†Ø§ Ù„ÙˆÙ„Ø§ Ø£Ù† ØµØ¨Ø±Ù†Ø§ Ø¹Ù„ÙŠÙ‡Ø§  ÙˆØ³ÙˆÙ ÙŠØ¹Ù„Ù…ÙˆÙ† Ø­ÙŠÙ† ÙŠØ±ÙˆÙ† Ø§Ù„Ø¹Ø°Ø§Ø¨ Ù…Ù† Ø£Ø¶Ù„ Ø³Ø¨ÙŠÙ„Ø§\n",
      "----------------\n",
      "Ø­Ù†Øª Ø¥Ù„ÙŠ Ø§Ù„Ù†Ø®Ù„Ø© Ø§Ù„Ù‚ØµÙˆÙ‰ ÙÙ‚Ù„Øª Ù„Ù‡Ø§ Ø­Ø¬Ø± Ø­Ø±Ø§Ù… Ø£Ù„Ø§ ØªÙ„Ùƒ Ø§Ù„Ø¯Ù‡Ø§Ø±ÙŠØ³\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_hadiths_from_graph[8])\n",
    "print('----------------')\n",
    "print(Hadiths_from_xml[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b84369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d37a2033",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [80]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ff \u001b[38;5;241m=\u001b[39m \u001b[43mcleaned_hadiths_from_graph\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m295\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m Hadiths_from_xml:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ff:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ff = cleaned_hadiths_from_graph[295]\n",
    "for i in Hadiths_from_xml:\n",
    "    if i in ff:\n",
    "        print('here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8ad2d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "f = []\n",
    "for index,i in enumerate(cleaned_hadiths_from_graph):\n",
    "    for j in Hadiths_from_xml:\n",
    "        if i == j:\n",
    "            c+=1\n",
    "            f.append(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3643441a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "298"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(set(f))\n",
    "print(len(set(f)))\n",
    "print(c)\n",
    "298 - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e1b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = str(s[3])\n",
    "# print(a)\n",
    "# A = removeDiacritics(a)\n",
    "# B  = processText(A)\n",
    "# regex pattern to get the text inside tags\n",
    "pattern = \"<[^>]*>(.*?)<\\/[^>]*>\"\n",
    "count = 0\n",
    "Nat = []\n",
    "for i in s:\n",
    "    T = processText(i)\n",
    "    T = removeDiacritics(T)\n",
    "#     T = removeSpace(T)\n",
    "#     print(T)\n",
    "    Nat.append(T)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67caa712",
   "metadata": {},
   "source": [
    "### Graph Validation Functions\n",
    "1. Chapters (suras)\n",
    "2. Verses\n",
    "3. Commentary Sections \n",
    "4. Hadiths\n",
    "5. Not Hadiths\n",
    "6. narrator\n",
    "7. Topic/ SubTopic\n",
    "8. Person,Location,Organization & Time\n",
    "9. Verses\n",
    "10. Poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d5287",
   "metadata": {},
   "source": [
    "# Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text processing function\n",
    "def removeDiacritics(text):\n",
    "    for a in pyarabic.araby.DIACRITICS:\n",
    "        text = text.replace(a, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def processText(text):\n",
    "    text = str(text)\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    while text.find('  ') != -1:\n",
    "        text = text.replace('  ', ' ')\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def removeSpace(text: str) -> str:\n",
    "    while text.find(' ') != -1:\n",
    "        text = text.replace(' ', '')\n",
    "\n",
    "    return text\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def lists_have_same_elements(list1, list2):\n",
    "    counter1 = Counter(list1)\n",
    "    counter2 = Counter(list2)\n",
    "    \n",
    "    return counter1 == counter2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d79509d",
   "metadata": {},
   "source": [
    "## narrator validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe72d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validate_Narrator(graph_file,xml_file,nat_type):\n",
    "    \n",
    "    # Extracting Graph info (Narrators)\n",
    "    \n",
    "    # Create a new RDF graph\n",
    "    graph = Graph()\n",
    "\n",
    "    # Parse the RDF data from a file or URL\n",
    "    graph.parse(graph_file)\n",
    "\n",
    "    # Define the SPARQL query\n",
    "    query = f\"\"\"\n",
    "    PREFIX : <http://www.tafsirtabari.com/ontology#>\n",
    "    PREFIX W3:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "    SELECT DISTINCT *\n",
    "    WHERE {{\n",
    "      {{\n",
    "      ?Person rdf:type :Narrator.\n",
    "        ?Person :hasName ?name.\n",
    "      ?Person :hasNarratorType :{nat_type}.\n",
    "      }}\n",
    "      union\n",
    "      {{\n",
    "          ?Person rdf:type :RootNarrator.\n",
    "        ?Person :hasName ?name.\n",
    "      ?Person :hasNarratorType :{nat_type}.\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    R = []\n",
    "    # Execute the SPARQL query\n",
    "    results = graph.query(query)\n",
    "    # Process the query results\n",
    "    for row in results:\n",
    "        # Access the query variables\n",
    "        person = row[\"Person\"]\n",
    "        name = row[\"name\"]\n",
    "#         print(f\"Person: {person}, Name: {name}\")\n",
    "        R.append(str(name))\n",
    "\n",
    "    narrators_from_graph = list(set(R))\n",
    "    list_i = []\n",
    "\n",
    "    for xml_f in xml_file:\n",
    "        # extracting info from xml\n",
    "        with open(xml_f, 'r', encoding='utf8') as f:     \n",
    "            xml_file_as_string = f.read()\n",
    "\n",
    "        Bs_data = BeautifulSoup(xml_file_as_string, \"xml\")\n",
    "\n",
    "        tag = Bs_data.find_all(\"persName\")\n",
    "        root = ET.fromstring(xml_file_as_string)\n",
    "        for i in tag:\n",
    "        #     print(i)\n",
    "            O = i.get(\"ana\")\n",
    "            if O == nat_type: # set narrator type\n",
    "    #             print(i)\n",
    "                list_i.append(i)\n",
    "\n",
    "    s = set(list_i)\n",
    "    S = list(s)\n",
    "        \n",
    "    # regex pattern to get the text inside tags\n",
    "    pattern = \"<[^>]*>(.*?)<\\/[^>]*>\"\n",
    "    count = 0\n",
    "    Nat = []\n",
    "    for i in S:\n",
    "        T = processText(i)\n",
    "        T = removeDiacritics(T)\n",
    "    #     T = removeSpace(T)\n",
    "        output = re.findall(pattern, T)\n",
    "    #     print(output)\n",
    "        if len(output) != 0:\n",
    "            count+=1\n",
    "#             print(output[0])\n",
    "            Nat.append(output[0])\n",
    "        else:\n",
    "            print(\"something wrong \",i)\n",
    "    \n",
    "    \n",
    "    narrators_from_xml = list(set(Nat))\n",
    "\n",
    "    if lists_have_same_elements(narrators_from_graph, narrators_from_xml):\n",
    "        print(\"The lists have the same elements.\")\n",
    "        print(\"Narrators test passed! Validated! âœ… \")\n",
    "    else:\n",
    "        print(\"The lists do not have the same elements.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d87ab1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lists have the same elements.\n",
      "Narrators test passed! Validated! âœ… \n"
     ]
    }
   ],
   "source": [
    "a = ['D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\Dataset\\\\S025N\\\\sure_25_section_25.1-25.46.xml','D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\Dataset\\\\S025N\\\\sure_25_section_25.47-25.52.xml']\n",
    "# g_file = \"D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-9f4906f3913e4c1e2353d282ee31f7b4aafcab4b\\\\A-Box\\\\Tafsir Al-Tabari A-Box S100.owl\"\n",
    "g_file = 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-9f4906f3913e4c1e2353d282ee31f7b4aafcab4b\\\\A-Box\\\\Tafsir Al-Tabari A-Box S25.owl'\n",
    "\n",
    "xml_file = a\n",
    "\n",
    "Validate_Narrator(g_file,xml_file,\"rawi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e9d426",
   "metadata": {},
   "source": [
    "## NER Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "688764b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validate_NER(graph_file,xml_file,NER):\n",
    "    \n",
    "    # Extracting Graph info (Narrators)\n",
    "    \n",
    "    # Create a new RDF graph\n",
    "    graph = Graph()\n",
    "\n",
    "    # Parse the RDF data from a file or URL\n",
    "    graph.parse(graph_file)\n",
    "    \n",
    "    NER_type = NER\n",
    "    text = 'hasName'\n",
    "    if NER == 'Time':\n",
    "        text = 'hasTime'\n",
    "    # Define the SPARQL query\n",
    "    query = f\"\"\"\n",
    "    PREFIX : <http://www.tafsirtabari.com/ontology#>\n",
    "    PREFIX W3:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "    SELECT DISTINCT ?individual ?Name\n",
    "    WHERE {{\n",
    "      \n",
    "          ?individual rdf:type :{NER_type}.\n",
    "            ?individual :{text} ?Name.\n",
    "    }}\n",
    "    \n",
    "    \"\"\"\n",
    "    R = []\n",
    "    # Execute the SPARQL query\n",
    "    results = graph.query(query)\n",
    "    # Process the query results\n",
    "    for row in results:\n",
    "        # Access the query variables\n",
    "        org = row[\"individual\"]\n",
    "        name = row[\"Name\"]\n",
    "#         print(f\"Organization: {org}, Name: {name}\")\n",
    "        R.append(str(name))\n",
    "\n",
    "    NER_from_graph = list(set(R))\n",
    "\n",
    "    # extracting info from xml\n",
    "    for xml_f in xml_file:\n",
    "        with open(xml_f, 'r', encoding='utf8') as f:     \n",
    "            xml_file_as_string = f.read()\n",
    "\n",
    "        Bs_data = BeautifulSoup(xml_file_as_string, \"xml\")\n",
    "\n",
    "        tag = Bs_data.find_all(\"name\")\n",
    "        root = ET.fromstring(xml_file_as_string)\n",
    "        list_i = []\n",
    "        for i in tag:\n",
    "        #     print(i)\n",
    "            O = i.get(\"role\")\n",
    "            if O == NER_type.lower(): # set narrator type\n",
    "    #             print(i)\n",
    "                list_i.append(i.text)\n",
    "\n",
    "    s = set(list_i)\n",
    "    S = list(s)\n",
    "#     print(len(S),S)\n",
    "    print('\\n\\n')\n",
    "    # regex pattern to get the text inside tags\n",
    "    pattern = \"<[^>]*>(.*?)<\\/[^>]*>\"\n",
    "    count = 0\n",
    "    Nat = []\n",
    "    for i in S:\n",
    "        \n",
    "    #     T = removeSpace(T)\n",
    "        T = processText(i)\n",
    "        T = removeDiacritics(T)\n",
    "        Nat.append(T)\n",
    "    \n",
    "    \n",
    "    NER_from_xml = list(set(Nat))\n",
    "\n",
    "    if lists_have_same_elements(NER_from_graph, NER_from_xml):\n",
    "        print(\"The lists have the same elements.\")\n",
    "        print(f\"{NER_type} test passed! Validated! âœ… \")\n",
    "    else:\n",
    "        print(\"The lists do not have the same elements.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e3f95b28",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [71]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m g_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUniversity\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msemester6\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mKRR_WORK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFYP-AUTOKG-9f4906f3913e4c1e2353d282ee31f7b4aafcab4b\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mA-Box\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mTafsir Al-Tabari A-Box S100.owl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m xml_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUniversity\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msemester6\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mKRR_WORK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFYP-AUTOKG-Graph-Generator\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mA-Box\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mS100M\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msure_100_section_100.1-100.1.xml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mValidate_NER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43mxml_file\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36mValidate_NER\u001b[1;34m(graph_file, xml_file, NER)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# extracting info from xml\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xml_f \u001b[38;5;129;01min\u001b[39;00m xml_file:\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxml_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:     \n\u001b[0;32m     45\u001b[0m         xml_file_as_string \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     47\u001b[0m     Bs_data \u001b[38;5;241m=\u001b[39m BeautifulSoup(xml_file_as_string, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D'"
     ]
    }
   ],
   "source": [
    "g_file = \"D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-9f4906f3913e4c1e2353d282ee31f7b4aafcab4b\\\\A-Box\\\\Tafsir Al-Tabari A-Box S100.owl\"\n",
    "xml_file = 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Graph-Generator\\\\A-Box\\\\Dataset\\\\S100M\\\\sure_100_section_100.1-100.1.xml'\n",
    "Validate_NER(g_file,xml_file,'Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5818c795",
   "metadata": {},
   "source": [
    "## Poetry Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e1e08244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validate_Poetry(graph_file,xml_file):\n",
    "    \n",
    "    # Extracting Graph info (Narrators)\n",
    "    \n",
    "    # Create a new RDF graph\n",
    "    graph = Graph()\n",
    "\n",
    "    # Parse the RDF data from a file or URL\n",
    "    graph.parse(graph_file)\n",
    "    \n",
    "    # Define the SPARQL query\n",
    "    query = \"\"\"\n",
    "    PREFIX : <http://www.tafsirtabari.com/ontology#>\n",
    "    PREFIX W3:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "    SELECT DISTINCT *\n",
    "    WHERE {\n",
    "\n",
    "    ?s rdf:type :Poetry.\n",
    "    ?s :hasText ?n.\n",
    "\n",
    "    }\n",
    "    \"\"\"\n",
    "    R = []\n",
    "    # Execute the SPARQL query\n",
    "    results = graph.query(query)\n",
    "    # Process the query results\n",
    "    for row in results:\n",
    "        # Access the query variables\n",
    "        s = row[\"s\"]\n",
    "        n = row[\"n\"]\n",
    "        print(f\"s: {s}, n: {n}\")\n",
    "        R.append(str(n))\n",
    "\n",
    "    poetry_from_graph = list(set(R))\n",
    "        \n",
    "    for xml_f in xml_file:\n",
    "        # extracting info from xml\n",
    "        with open(xml_f, 'r', encoding='utf8') as f:     \n",
    "            xml_file_as_string = f.read()\n",
    "\n",
    "        Bs_data = BeautifulSoup(xml_file_as_string, \"xml\")\n",
    "        tag = Bs_data.find_all(\"quote\")\n",
    "        root = ET.fromstring(xml_file_as_string)\n",
    "        for i in tag:\n",
    "        #     print(i)\n",
    "            O = i.get(\"type\")\n",
    "            if O == \"poetry\":\n",
    "                print(processText(removeDiacritics(str(i))))\n",
    "                print('\\n\\n')\n",
    "                list_i.append(processText(removeDiacritics(i.text)))\n",
    "\n",
    "    s = set(list_i)\n",
    "    S = list(s)\n",
    "    \n",
    "\n",
    "    print('\\n\\n')\n",
    "    # regex pattern to get the text inside tags\n",
    "    pattern = \"<[^>]*>(.*?)<\\/[^>]*>\"\n",
    "    count = 0\n",
    "    Nat = []\n",
    "    for i in S:\n",
    "        \n",
    "    #     T = removeSpace(T)\n",
    "        T = processText(i)\n",
    "        T = removeDiacritics(T)\n",
    "        Nat.append(T)\n",
    "        \n",
    "        \n",
    "    # Further text cleaning (needed for pieces of text)\n",
    "    #graph\n",
    "    cleaned_poetry_from_graph = []\n",
    "    for i in poetry_from_graph:\n",
    "        a = remove_numbers_and_slashes_hadith(i)\n",
    "        cleaned_poetry_from_graph.append(removeDiacritics(a))\n",
    "\n",
    "    # xml\n",
    "    list_i_p = []\n",
    "    for i in list_i:\n",
    "        filtered_list = remove_numbers_and_slashes(i)\n",
    "        list_i_p.append(filtered_list)\n",
    "\n",
    "    # removing leading spaces\n",
    "    poetry_from_xml = remove_leading_spaces_list(list_i_p)\n",
    "\n",
    "    cleaned_poetry_from_graph.sort()\n",
    "    poetry_from_xml.sort()\n",
    "\n",
    "    \n",
    "    \n",
    "    poetry_from_xml = list(set(Nat))\n",
    "\n",
    "    if lists_have_same_elements(cleaned_poetry_from_graph, poetry_from_xml):\n",
    "        print(\"The lists have the same elements.\")\n",
    "        print(f\"Poetry test passed! Validated! âœ… \")\n",
    "    else:\n",
    "        print(\"The lists do not have the same elements.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ada6acfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: http://www.tafsirtabari.com/ontology#P_001, n: Ø¥ÙØ°Ù’ Ø£ÙØ¬ÙŽØ§Ø±ÙÙŠ Ø§Ù„Ø´ÙŠØ·Ø§Ù† ÙÙÙŠ Ø³ÙŽÙ†ÙŽÙ†Ù Ø§Ù„Ù’ØºÙŽØ³ÙŠÙ‘Ù ÙˆÙŽÙ…ÙŽÙ†Ù’ Ù…ÙŽØ§Ù„ÙŽ Ù…ÙŽÙŠÙ’Ù„ÙŽÙ‡Ù Ù…ÙŽØ«Ù’Ø¨ÙÙˆØ±Ù\n",
      "s: http://www.tafsirtabari.com/ontology#P_002, n: ÙŠÙŽØ§ Ø±ÙŽØ³ÙÙˆÙ„ÙŽ Ø§Ù„Ù’Ù…ÙŽÙ„ÙÙŠÙƒÙ Ø¥ÙÙ†Ù‘ÙŽ Ù„ÙØ³ÙŽØ§Ù†ÙÙŠ Ø±ÙŽØ§ØªÙÙ‚ÙŒ Ù…ÙŽØ§ ÙÙŽØªÙ‘ÙŽÙ‚Ù’ØªÙ Ø¥ÙØ°Ù’ Ø£ÙŽÙ†ÙŽØ§ Ø¨ÙÙˆØ±Ù\n",
      "s: http://www.tafsirtabari.com/ontology#P_003, n: Ø­Ù†Øª Ø¥Ù„ÙŠ Ø§Ù„Ù†Ø®Ù„Ø© Ø§Ù„Ù‚ØµÙˆÙ‰ ÙÙ‚Ù„Øª Ù„Ù‡Ø§ Ø­Ø¬Ø± Ø­Ø±Ø§Ù… Ø£Ù„Ø§ ØªÙ„Ùƒ Ø§Ù„Ø¯Ù‡Ø§Ø±ÙŠØ³\n",
      "s: http://www.tafsirtabari.com/ontology#P_004, n: ÙÙ‡Ù…Ù…Øª Ø£Ù† Ø£Ù„Ù‚ÙŠ Ø¥Ù„ÙŠÙ‡Ø§ Ù…Ø­Ø¬Ø±Ø§ ÙÙ„Ù…Ø«Ù„Ù‡Ø§ ÙŠÙ„Ù‚ÙŠ Ø¥Ù„ÙŠÙ‡ Ø§Ù„Ù…Ø­Ø¬Ø±\n",
      "s: http://www.tafsirtabari.com/ontology#P_005, n: ÙˆÙ‚Ø¯Ù… Ø§Ù„Ø®ÙˆØ§Ø±Ø¬ Ø§Ù„Ø¶Ù„Ø§Ù„ Ø¥Ù„Ù‰ Ø¹Ø¨Ø§Ø¯ Ø±Ø¨Ù‡Ù… ÙˆÙ‚Ø§Ù„ÙˆØ§ Ø¥Ù† Ø¯Ù…Ø§Ø¡ÙƒÙ… Ù„Ù†Ø§ Ø­Ù„Ø§Ù„\n",
      "s: http://www.tafsirtabari.com/ontology#P_006, n: Ø³ÙŽØ¨ÙŽÙ‚Ù’ØªÙ Ø¥ÙÙ„ÙŽÙ‰ ÙÙŽØ±ÙŽØ·Ù Ù†Ø§Ù‡ÙÙ„Ù ØªÙŽÙ†ÙŽØ§Ø¨ÙÙ„ÙŽØ©Ù‹ ÙŠÙŽØ­Ù’ÙÙØ±ÙÙˆÙ†ÙŽ Ø§Ù„Ø±Ù‘ÙØ³ÙŽØ§Ø³ÙŽØ§\n",
      "s: http://www.tafsirtabari.com/ontology#P_007, n: Ø­ØªÙ‰ ÙŠÙ‚ÙˆÙ„ Ø§Ù„Ù†Ø§Ø³ Ù…Ù…Ø§ Ø±Ø£ÙˆØ§ ÙŠØ§ Ø¹Ø¬Ø¨Ø§ Ù„Ù„Ù…ÙŠØª Ø§Ù„Ù†Ø§Ø´Ø±\n",
      "s: http://www.tafsirtabari.com/ontology#P_008, n: Ø±Ø¹Ù‰ Ø¨Ù‡Ø§ Ù…Ø±Ø¬ Ø±Ø¨ÙŠØ¹ Ù…Ù…Ø±Ø¬Ø§\n",
      "s: http://www.tafsirtabari.com/ontology#P_009, n: Ø£Ù„Ù… ÙŠØ­Ø²Ù†Ùƒ Ø£Ù† Ø­Ø¨Ø§Ù„ Ù‚ÙŠØ³ ÙˆØªØºÙ„Ø¨ Ù‚Ø¯ ØªØ¨Ø§ÙŠÙ†ØªØ§ Ø§Ù†Ù‚Ø·Ø§Ø¹Ø§\n",
      "s: http://www.tafsirtabari.com/ontology#P_010, n: ÙƒÙŽØ£ÙŽÙ†Ù‘ÙŽÙ‡ÙŽØ§ Ø¨ÙØ±Ù’Ø¬Ù Ø±ÙˆÙ…ÙŠ ÙŠÙØ´ÙŽÙŠÙ‘ÙØ¯ÙÙ‡Ù Ø¨ÙŽØ§Ù†Ù Ø¨ÙØ¬ÙØµÙ‘Ù ÙˆÙŽØ¢Ø¬ÙØ±Ù‘Ù ÙˆÙŽØ£ÙŽØ­Ù’Ø¬ÙŽØ§Ø±Ù\n",
      "s: http://www.tafsirtabari.com/ontology#P_011, n: ÙˆÙŽÙ„ÙŽÙ‡ÙŽØ§ Ø¨ÙØ§Ù„Ù’Ù…ÙŽØ§Ø·ÙŽØ±ÙÙˆÙ†Ù Ø¥ÙØ°ÙŽØ§ Ø£ÙŽÙƒÙŽÙ„ÙŽ Ø§Ù„Ù†Ù‘ÙŽÙ…Ù’Ù„Ù Ø§Ù„Ù‘ÙŽØ°ÙÙŠ Ø¬ÙŽÙ…ÙŽØ¹ÙŽØ§\n",
      "s: http://www.tafsirtabari.com/ontology#P_012, n: Ø®ÙÙ„Ù’ÙÙŽØ©ÙŒ Ø­ÙŽØªÙ‘ÙŽÙ‰ Ø¥ÙØ°ÙŽØ§ Ø§Ø±Ù’ØªÙŽØ¨ÙŽØ¹ÙŽØªÙ’ Ø³ÙŽÙƒÙŽÙ†ÙŽØªÙ’ Ù…ÙÙ†Ù’ Ø¬ÙÙ„Ù‘ÙÙ‚Ù Ø¨ÙÙŠÙŽØ¹ÙŽØ§\n",
      "s: http://www.tafsirtabari.com/ontology#P_013, n: Ø¨ÙÙ‡ÙŽØ§ Ø§Ù„Ù’Ø¹ÙŽÙŠÙ’Ù†Ù ÙˆÙŽØ§Ù„Ø¢Ø±ÙŽØ§Ù…Ù ÙŠÙŽÙ…Ù’Ø´ÙÙŠÙ†ÙŽ Ø®ÙÙ„Ù’ÙÙŽØ©Ù‹ ÙˆÙŽØ£ÙŽØ·Ù’Ù„Ø§Ø¤ÙÙ‡ÙŽØ§ ÙŠÙŽÙ†Ù’Ù‡ÙŽØ¶Ù’Ù†ÙŽ Ù…ÙÙ†Ù’ ÙƒÙÙ„Ù‘Ù Ù…ÙŽØ¬Ù’Ø«ÙŽÙ…Ù\n",
      "s: http://www.tafsirtabari.com/ontology#P_014, n: Ø¥Ù† ÙŠØ¹Ø§Ù‚Ø¨ ÙŠÙƒÙ† ØºØ±Ø§Ù…Ø§ ÙˆØ¥Ù† ÙŠØ¹ Ø· Ø¬Ø²ÙŠÙ„Ø§ ÙØ¥Ù†Ù‡ Ù„Ø§ ÙŠØ¨Ø§Ù„Ù‰\n",
      "s: http://www.tafsirtabari.com/ontology#P_015, n: ÙˆÙŠÙˆÙ… Ø§Ù„Ù†Ø³Ø§Ø± ÙˆÙŠÙˆÙ… Ø§Ù„Ø¬ÙØ§ Ø± ÙƒØ§Ù†Ø§ Ø¹Ù‚Ø§Ø¨Ø§ ÙˆÙƒØ§Ù†Ø§ ØºØ±Ø§Ù…Ø§\n",
      "s: http://www.tafsirtabari.com/ontology#P_016, n: ÙŠÙˆÙ…Ø§Ù† ÙŠÙˆÙ… Ù…Ù‚Ø§Ù…Ø§Øª ÙˆØ£Ù†Ø¯ÙŠØ© ÙˆÙŠÙˆÙ… Ø³ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¡ ØªØ£ÙˆÙŠØ¨\n",
      "s: http://www.tafsirtabari.com/ontology#P_017, n: ÙØ£ÙŠÙŠ Ù…Ø§ ÙˆØ£ÙŠÙƒ ÙƒØ§Ù† Ø´Ø±Ø§ ÙÙ‚ÙŠØ¯ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ù…Ø© Ù„Ø§ ÙŠØ±Ø§Ù‡Ø§\n",
      "s: http://www.tafsirtabari.com/ontology#P_018, n: Ø·ÙŽØ§ÙÙŽØªÙ’ Ø£Ù…Ø§Ù…Ø© Ø¨ÙØ§Ù„Ø±Ù‘ÙÙƒÙ’Ø¨ÙŽØ§Ù†Ù Ø¢ÙˆÙÙ†ÙŽØ©Ù‹ ÙŠÙŽØ§ Ø­ÙØ³Ù’Ù†ÙŽÙ‡Ù Ù…ÙÙ†Ù’ Ù‚ÙŽÙˆÙŽØ§Ù…Ù Ù…ÙŽØ§ ÙˆÙŽÙ…ÙÙ†Ù’ØªÙŽÙ‚ÙŽØ¨ÙŽØ§\n",
      "s: http://www.tafsirtabari.com/ontology#P_019, n: Ø¬Ø²Ù‰ Ø§Ù„Ù„Ù‡ Ø§Ø¨Ù† Ø¹Ø±ÙˆØ© Ø­ÙŠØ« Ø£Ù…Ø³Ù‰ Ø¹Ù‚ÙˆÙ‚Ø§ ÙˆØ§Ù„Ø¹Ù‚ÙˆÙ‚ Ù„Ù‡ Ø£Ø«Ø§Ù…\n",
      "s: http://www.tafsirtabari.com/ontology#P_020, n: Ù…ØªÙ‰ ØªØ£ØªÙ‡ ØªØ¹Ø´Ùˆ Ø¥Ù„Ù‰ Ø¶ÙˆØ¡ Ù†Ø§Ø±Ù‡ ØªØ¬Ø¯ Ø®ÙŠØ± Ù†Ø§Ø± Ø¹Ù†Ø¯Ù‡Ø§ Ø®ÙŠØ± Ù…ÙˆÙ‚Ø¯\n",
      "s: http://www.tafsirtabari.com/ontology#P_021, n: Ù„Ø§ ÙŠÙÙ‚Ù’Ù†ÙØ¹Ù Ø§Ù„Ù’Ø¬ÙŽØ§Ø±ÙÙŠÙŽØ©ÙŽ Ø§Ù„Ù’Ø®ÙØ¶ÙŽØ§Ø¨Ù ÙˆÙŽÙ„Ø§ Ø§Ù„Ù’ÙˆÙØ´ÙŽØ§Ø­ÙŽØ§Ù†Ù ÙˆÙŽÙ„Ø§ Ø§Ù„Ù’Ø¬ÙÙ„Ù’Ø¨ÙŽØ§Ø¨Ù Ù…ÙÙ†Ù’ Ø¯ÙÙˆÙ†Ù Ø£ÙŽÙ†Ù’ ØªÙŽÙ„Ù’ØªÙŽÙ‚ÙÙŠÙŽ Ø§Ù„Ø£ÙŽØ±Ù’ÙƒÙŽØ§Ø¨Ù ÙˆÙŽÙŠÙŽÙ‚Ù’Ø¹ÙØ¯ÙŽ Ø§Ù„Ø£ÙŽÙŠÙ’Ø±Ù Ù„ÙŽÙ‡Ù Ù„ÙØ¹ÙŽØ§Ø¨Ù\n",
      "s: http://www.tafsirtabari.com/ontology#P_022, n: ÙˆÙŽÙŠÙŽÙ‚Ù’Ø¹ÙØ¯Ù Ø§Ù„Ù’Ù‡ÙŽÙ†Ù Ù„ÙŽÙ‡Ù Ù„ÙØ¹ÙŽØ§Ø¨Ù\n",
      "s: http://www.tafsirtabari.com/ontology#P_023, n: ÙŠÙŽØ§ Ø¹ÙŽØ§Ø°ÙÙ„Ø§ØªÙÙŠ Ù„Ø§ ØªÙØ±ÙØ¯Ù’Ù†ÙŽ Ù…ÙŽÙ„Ø§Ù…ÙŽØªÙÙŠ Ø¥ÙÙ†Ù‘ÙŽ Ø§Ù„Ù’Ø¹ÙŽÙˆÙŽØ§Ø°ÙÙ„ÙŽ Ù„ÙŽØ³Ù’Ù†ÙŽ Ù„ÙÙŠ Ø¨ÙØ£ÙŽÙ…ÙÙŠØ±Ù\n",
      "s: http://www.tafsirtabari.com/ontology#P_024, n: ÙƒØ£Ù† Ø¨Ù†Ø­Ø±Ù‡ ÙˆØ¨Ù…Ù†ÙƒØ¨ÙŠÙ‡ Ø¹Ø¨ÙŠØ±Ø§ Ø¨Ø§Øª ÙŠØ¹Ø¨Ø¤Ù‡ Ø¹Ø±ÙˆØ³\n",
      "s: http://www.tafsirtabari.com/ontology#P_025, n: ÙÙØ§Ø¬Ø£Ø© Ø¨Ø¹Ø§Ø¯ÙŠØ© Ù„ÙØ²Ø§Ù…Ù ÙƒÙ…Ø§ ÙŠØªÙØ¬Ø± Ø§Ù„Ø­ÙˆØ¶ Ø§Ù„Ù„Ù‚ÙŠÙ\n",
      "s: http://www.tafsirtabari.com/ontology#P_026, n: Ø¥ÙØ°ÙŽØ§ ÙƒÙŽØ§Ù†ÙŽ Ø·ÙŽØ¹Ù’Ù†Ù‹Ø§ Ø¨ÙŽÙŠÙ’Ù†ÙŽÙ‡ÙÙ…Ù’ ÙˆÙŽÙ‚ÙØªÙŽØ§Ù„Ø§\n",
      "<quote type=\"poetry\"> <lg> <l>Ø¥Ø° Ø£Ø¬Ø§Ø±ÙŠ <seg ana=\"samiyat\"><name role=\"other\">Ø§Ù„Ø´ÙŠØ·Ø§Ù†</name></seg> ÙÙŠ Ø³Ù†Ù† Ø§Ù„ØºØ³ÙŠ </l> <l>ÙˆÙ…Ù† Ù…Ø§Ù„ Ù…ÙŠÙ„Ù‡ Ù…Ø«Ø¨ÙˆØ± </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>ÙŠØ§ Ø±Ø³ÙˆÙ„ Ø§Ù„Ù…Ù„ÙŠÙƒ Ø¥Ù† Ù„Ø³Ø§Ù†ÙŠ </l> <l>Ø±Ø§ØªÙ‚ Ù…Ø§ ÙØªÙ‚Øª Ø¥Ø° Ø£Ù†Ø§ Ø¨ÙˆØ± </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>Ø­Ù†Øª Ø¥Ù„ÙŠ Ø§Ù„Ù†Ø®Ù„Ø© Ø§Ù„Ù‚ØµÙˆÙ‰ ÙÙ‚Ù„Øª Ù„Ù‡Ø§ </l> <l>Ø­Ø¬Ø± Ø­Ø±Ø§Ù… Ø£Ù„Ø§ ØªÙ„Ùƒ <name role=\"time\">Ø§Ù„Ø¯Ù‡Ø§Ø±ÙŠØ³</name> </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>ÙÙ‡Ù…Ù…Øª Ø£Ù† Ø£Ù„Ù‚ÙŠ Ø¥Ù„ÙŠÙ‡Ø§ Ù…Ø­Ø¬Ø±Ø§ </l> <l>ÙÙ„Ù…Ø«Ù„Ù‡Ø§ ÙŠÙ„Ù‚ÙŠ Ø¥Ù„ÙŠÙ‡ Ø§Ù„Ù…Ø­Ø¬Ø± </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <l> ÙˆÙ‚Ø¯Ù… <seg ana=\"yes firaq\">Ø§Ù„Ø®ÙˆØ§Ø±Ø¬</seg> <seg ana=\"kalamterm\">Ø§Ù„Ø¶Ù„Ø§Ù„</seg> </l> <l> Ø¥Ù„Ù‰ Ø¹Ø¨Ø§Ø¯ Ø±Ø¨Ù‡Ù… ÙˆÙ‚Ø§Ù„ÙˆØ§ </l> <l> Ø¥Ù† Ø¯Ù…Ø§Ø¡ÙƒÙ… Ù„Ù†Ø§ Ø­Ù„Ø§Ù„ </l> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>Ø³Ø¨Ù‚Øª Ø¥Ù„Ù‰ ÙØ±Ø· Ù†Ø§Ù‡Ù„ </l> <l>ØªÙ†Ø§Ø¨Ù„Ø© ÙŠØ­ÙØ±ÙˆÙ† Ø§Ù„Ø±Ø³Ø§Ø³Ø§ </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>Ø­ØªÙ‰ ÙŠÙ‚ÙˆÙ„ Ø§Ù„Ù†Ø§Ø³ Ù…Ù…Ø§ Ø±Ø£ÙˆØ§ </l> <l>ÙŠØ§ Ø¹Ø¬Ø¨Ø§ Ù„Ù„Ù…ÙŠØª Ø§Ù„Ù†Ø§Ø´Ø± </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <l> Ø±Ø¹Ù‰ Ø¨Ù‡Ø§ Ù…Ø±Ø¬ Ø±Ø¨ÙŠØ¹ Ù…Ù…Ø±Ø¬Ø§ </l> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>Ø£Ù„Ù… ÙŠØ­Ø²Ù†Ùƒ Ø£Ù† Ø­Ø¨Ø§Ù„ Ù‚ÙŠØ³ </l> <l>ÙˆØªØºÙ„Ø¨ Ù‚Ø¯ ØªØ¨Ø§ÙŠÙ†ØªØ§ Ø§Ù†Ù‚Ø·Ø§Ø¹Ø§ </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>ÙƒØ£Ù†Ù‡Ø§ Ø¨Ø±Ø¬ <name role=\"organization\">Ø±ÙˆÙ…ÙŠ</name> ÙŠØ´ÙŠØ¯Ù‡ </l> <l>Ø¨Ø§Ù† Ø¨Ø¬Øµ ÙˆØ¢Ø¬Ø± ÙˆØ£Ø­Ø¬Ø§Ø± </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>ÙˆÙ„Ù‡Ø§ Ø¨Ø§Ù„Ù…Ø§Ø·Ø±ÙˆÙ† Ø¥Ø°Ø§ </l> <l>Ø£ÙƒÙ„ Ø§Ù„Ù†Ù…Ù„ Ø§Ù„Ø°ÙŠ Ø¬Ù…Ø¹Ø§ </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>Ø®Ù„ÙØ© Ø­ØªÙ‰ Ø¥Ø°Ø§ Ø§Ø±ØªØ¨Ø¹Øª </l> <l>Ø³ÙƒÙ†Øª Ù…Ù† Ø¬Ù„Ù‚ Ø¨ÙŠØ¹Ø§ </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>Ø¨Ù‡Ø§ Ø§Ù„Ø¹ÙŠÙ† ÙˆØ§Ù„Ø¢Ø±Ø§Ù… ÙŠÙ…Ø´ÙŠÙ† Ø®Ù„ÙØ© </l> <l>ÙˆØ£Ø·Ù„Ø§Ø¤Ù‡Ø§ ÙŠÙ†Ù‡Ø¶Ù† Ù…Ù† ÙƒÙ„ Ù…Ø¬Ø«Ù… </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>Ø¥Ù† ÙŠØ¹Ø§Ù‚Ø¨ ÙŠÙƒÙ† ØºØ±Ø§Ù…Ø§ ÙˆØ¥Ù† ÙŠØ¹ </l> <l>Ø· Ø¬Ø²ÙŠÙ„Ø§ ÙØ¥Ù†Ù‡ Ù„Ø§ ÙŠØ¨Ø§Ù„Ù‰ </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>Ùˆ<name role=\"time\">ÙŠÙˆÙ… Ø§Ù„Ù†Ø³Ø§Ø±</name> Ùˆ<name role=\"time\">ÙŠÙˆÙ… Ø§Ù„Ø¬ÙØ§</name> </l> <l>Ø± ÙƒØ§Ù†Ø§ Ø¹Ù‚Ø§Ø¨Ø§ ÙˆÙƒØ§Ù†Ø§ ØºØ±Ø§Ù…Ø§ </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>ÙŠÙˆÙ…Ø§Ù† ÙŠÙˆÙ… Ù…Ù‚Ø§Ù…Ø§Øª ÙˆØ£Ù†Ø¯ÙŠØ© </l> <l>ÙˆÙŠÙˆÙ… Ø³ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¡ ØªØ£ÙˆÙŠØ¨ </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>ÙØ£ÙŠÙŠ Ù…Ø§ ÙˆØ£ÙŠÙƒ ÙƒØ§Ù† Ø´Ø±Ø§ </l> <l>ÙÙ‚ÙŠØ¯ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ù…Ø© Ù„Ø§ ÙŠØ±Ø§Ù‡Ø§ </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>Ø·Ø§ÙØª <name role=\"person\">Ø£Ù…Ø§Ù…Ø©</name> Ø¨Ø§Ù„Ø±ÙƒØ¨Ø§Ù† Ø¢ÙˆÙ†Ø© </l> <l>ÙŠØ§ Ø­Ø³Ù†Ù‡ Ù…Ù† Ù‚ÙˆØ§Ù… Ù…Ø§ ÙˆÙ…Ù†ØªÙ‚Ø¨Ø§ </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>Ø¬Ø²Ù‰ Ø§Ù„Ù„Ù‡ <name role=\"person\">Ø§Ø¨Ù† Ø¹Ø±ÙˆØ©</name> Ø­ÙŠØ« Ø£Ù…Ø³Ù‰ </l> <l>Ø¹Ù‚ÙˆÙ‚Ø§ ÙˆØ§Ù„Ø¹Ù‚ÙˆÙ‚ Ù„Ù‡ Ø£Ø«Ø§Ù… </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>Ù…ØªÙ‰ ØªØ£ØªÙ‡ ØªØ¹Ø´Ùˆ Ø¥Ù„Ù‰ Ø¶ÙˆØ¡ Ù†Ø§Ø±Ù‡ </l> <l>ØªØ¬Ø¯ Ø®ÙŠØ± Ù†Ø§Ø± Ø¹Ù†Ø¯Ù‡Ø§ Ø®ÙŠØ± Ù…ÙˆÙ‚Ø¯ </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <l> Ù„Ø§ ÙŠÙ‚Ù†Ø¹ Ø§Ù„Ø¬Ø§Ø±ÙŠØ© Ø§Ù„Ø®Ø¶Ø§Ø¨ </l> <l> ÙˆÙ„Ø§ Ø§Ù„ÙˆØ´Ø§Ø­Ø§Ù† ÙˆÙ„Ø§ Ø§Ù„Ø¬Ù„Ø¨Ø§Ø¨ </l> <l> Ù…Ù† Ø¯ÙˆÙ† Ø£Ù† ØªÙ„ØªÙ‚ÙŠ Ø§Ù„Ø£Ø±ÙƒØ§Ø¨ </l> <l> ÙˆÙŠÙ‚Ø¹Ø¯ Ø§Ù„Ø£ÙŠØ± Ù„Ù‡ Ù„Ø¹Ø§Ø¨ </l> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <l> ÙˆÙŠÙ‚Ø¹Ø¯ Ø§Ù„Ù‡Ù† Ù„Ù‡ Ù„Ø¹Ø§Ø¨ </l> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>ÙŠØ§ Ø¹Ø§Ø°Ù„Ø§ØªÙŠ Ù„Ø§ ØªØ±Ø¯Ù† Ù…Ù„Ø§Ù…ØªÙŠ </l> <l>Ø¥Ù† Ø§Ù„Ø¹ÙˆØ§Ø°Ù„ Ù„Ø³Ù† Ù„ÙŠ Ø¨Ø£Ù…ÙŠØ± </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>ÙƒØ£Ù† Ø¨Ù†Ø­Ø±Ù‡ ÙˆØ¨Ù…Ù†ÙƒØ¨ÙŠÙ‡ </l> <l>Ø¹Ø¨ÙŠØ±Ø§ Ø¨Ø§Øª ÙŠØ¹Ø¨Ø¤Ù‡ Ø¹Ø±ÙˆØ³ </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <lg> <l>ÙÙØ§Ø¬Ø£Ø© Ø¨Ø¹Ø§Ø¯ÙŠØ© Ù„Ø²Ø§Ù… </l> <l>ÙƒÙ…Ø§ ÙŠØªÙØ¬Ø± Ø§Ù„Ø­ÙˆØ¶ Ø§Ù„Ù„Ù‚ÙŠÙ </l> </lg> </quote>\n",
      "\n",
      "\n",
      "\n",
      "<quote type=\"poetry\"> <l> Ø¥Ø°Ø§ ÙƒØ§Ù† Ø·Ø¹Ù†Ø§ Ø¨ÙŠÙ†Ù‡Ù… ÙˆÙ‚ØªØ§Ù„Ø§ </l> </quote>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'remove_numbers_and_slashes_hadith' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [73]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUniversity\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msemester6\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mKRR_WORK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFYP-AUTOKG-Narrator_fixed\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mS025N\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msure_25_section_25.1-25.46.xml\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUniversity\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msemester6\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mKRR_WORK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFYP-AUTOKG-Narrator_fixed\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mS025N\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msure_25_section_25.47-25.52.xml\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUniversity\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msemester6\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mKRR_WORK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFYP-AUTOKG-Narrator_fixed\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mA-Box\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mTafsir Al-Tabari A-Box S25.owl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mValidate_Poetry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36mValidate_Poetry\u001b[1;34m(graph_file, xml_file)\u001b[0m\n\u001b[0;32m     73\u001b[0m cleaned_poetry_from_graph \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m poetry_from_graph:\n\u001b[1;32m---> 75\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mremove_numbers_and_slashes_hadith\u001b[49m(i)\n\u001b[0;32m     76\u001b[0m     cleaned_poetry_from_graph\u001b[38;5;241m.\u001b[39mappend(removeDiacritics(a))\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# xml\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'remove_numbers_and_slashes_hadith' is not defined"
     ]
    }
   ],
   "source": [
    "a = ['D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\Dataset\\\\S025N\\\\sure_25_section_25.1-25.46.xml','D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\Dataset\\\\S025N\\\\sure_25_section_25.47-25.52.xml']\n",
    "g = 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\A-Box\\\\Tafsir Al-Tabari A-Box S25.owl'\n",
    "Validate_Poetry(g,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e41e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9ae1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab31ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f02ce05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "30e222c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total verses: 6\n",
      "['Ù‚ÙÙ„Ù’ Ø£ÙŽØ¹ÙÙˆØ°Ù Ø¨ÙØ±ÙŽØ¨Ù‘Ù Ø§Ù„Ù†Ù‘ÙŽØ§Ø³Ù ', ' Ù…ÙŽÙ„ÙÙƒÙ Ø§Ù„Ù†Ù‘ÙŽØ§Ø³Ù ', ' Ø¥ÙÙ„ÙŽÙ‡Ù Ø§Ù„Ù†Ù‘ÙŽØ§Ø³Ù ', ' Ù…ÙÙ†Ù’ Ø´ÙŽØ±Ù‘Ù Ø§Ù„Ù’ÙˆÙŽØ³Ù’ÙˆÙŽØ§Ø³Ù Ø§Ù„Ù’Ø®ÙŽÙ†Ù‘ÙŽØ§Ø³Ù ', ' Ø§Ù„Ù‘ÙŽØ°ÙÙŠ ÙŠÙÙˆÙŽØ³Ù’ÙˆÙØ³Ù ÙÙÙŠ ØµÙØ¯ÙÙˆØ±Ù Ø§Ù„Ù†Ù‘ÙŽØ§Ø³Ù ', ' Ù…ÙÙ†ÙŽ Ø§Ù„Ù’Ø¬ÙÙ†Ù‘ÙŽØ©Ù ÙˆÙŽØ§Ù„Ù†Ù‘ÙŽØ§Ø³Ù ']\n"
     ]
    }
   ],
   "source": [
    "list_i = []\n",
    "import re\n",
    "# D:\\University\\semester6\\KRR_WORK\\FYP-AUTOKG-Narrator_fixed\\Dataset\\S114Ù˜M\n",
    "# a = ['D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\Dataset\\\\S025N\\\\sure_25_section_25.1-25.46.xml','D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\Dataset\\\\S025N\\\\sure_25_section_25.47-25.52.xml']\n",
    "a = ['D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\Dataset\\\\S114Ù˜M\\\\sure_114_section_114.1-114.1.xml']\n",
    "verses = []\n",
    "\n",
    "for xml_f in a:\n",
    "    # extracting info from xml\n",
    "    with open(xml_f, 'r', encoding='utf8') as f:     \n",
    "        xml_file_as_string = f.read()\n",
    "        Bs_data = BeautifulSoup(xml_file_as_string, 'xml')\n",
    "\n",
    "    tags_with_quotes = Bs_data.find_all(\"head\")\n",
    "    \n",
    "    for tag in tags_with_quotes:\n",
    "        quote_tag = tag.find(\"quote\")\n",
    "        if quote_tag:\n",
    "            l_tags = quote_tag.find_all(\"l\")  # Find nested <l> tags\n",
    "            if l_tags:\n",
    "                for l_tag in l_tags:\n",
    "                    l_text = l_tag.get_text()\n",
    "                    verses.append(l_text)    \n",
    "            else:        \n",
    "                quote_text = quote_tag.get_text()  # Get the text within the quote tag\n",
    "                verses.append(quote_text)\n",
    "print(\"Total verses:\", len(verses))\n",
    "\n",
    "            \n",
    "print(verses)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5735c090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7058e0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: http://www.tafsirtabari.com/ontology#C_114, n: http://www.tafsirtabari.com/ontology#V114:002, t: Ù…ÙŽÙ„ÙÙƒÙ Ø§Ù„Ù†Ù‘ÙŽØ§Ø³Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_114, n: http://www.tafsirtabari.com/ontology#V114:003, t: Ø¥ÙÙ„ÙŽÙ°Ù‡Ù Ø§Ù„Ù†Ù‘ÙŽØ§Ø³Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_114, n: http://www.tafsirtabari.com/ontology#V114:004, t: Ù…ÙÙ†Ù’ Ø´ÙŽØ±Ù‘Ù Ø§Ù„Ù’ÙˆÙŽØ³Ù’ÙˆÙŽØ§Ø³Ù Ø§Ù„Ù’Ø®ÙŽÙ†Ù‘ÙŽØ§Ø³Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_114, n: http://www.tafsirtabari.com/ontology#V114:005, t: Ø§Ù„Ù‘ÙŽØ°ÙÙŠ ÙŠÙÙˆÙŽØ³Ù’ÙˆÙØ³Ù ÙÙÙŠ ØµÙØ¯ÙÙˆØ±Ù Ø§Ù„Ù†Ù‘ÙŽØ§Ø³Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_114, n: http://www.tafsirtabari.com/ontology#V114:006, t: Ù…ÙÙ†ÙŽ Ø§Ù„Ù’Ø¬ÙÙ†Ù‘ÙŽØ©Ù ÙˆÙŽØ§Ù„Ù†Ù‘ÙŽØ§Ø³Ù\n",
      "s: http://www.tafsirtabari.com/ontology#C_114, n: http://www.tafsirtabari.com/ontology#V114:001, t: Ø¨ÙØ³Ù’Ù…Ù Ø§Ù„Ù„Ù‘ÙŽÙ‡Ù Ø§Ù„Ø±Ù‘ÙŽØ­Ù’Ù…ÙŽÙ°Ù†Ù Ø§Ù„Ø±Ù‘ÙŽØ­ÙÙŠÙ…Ù Ù‚ÙÙ„Ù’ Ø£ÙŽØ¹ÙÙˆØ°Ù Ø¨ÙØ±ÙŽØ¨Ù‘Ù Ø§Ù„Ù†Ù‘ÙŽØ§Ø³Ù\n"
     ]
    }
   ],
   "source": [
    "# querying from graph\n",
    "    \n",
    "# Create a new RDF graph\n",
    "graph = Graph()\n",
    "\n",
    "graph_file = 'D:\\\\University\\\\semester6\\\\KRR_WORK\\\\FYP-AUTOKG-Narrator_fixed\\\\A-Box\\\\Tafsir Al-Tabari A-Box S114.owl'\n",
    "\n",
    "\n",
    "# Parse the RDF data from a file or URL\n",
    "graph.parse(graph_file)\n",
    "\n",
    "# Define the SPARQL query\n",
    "query = \"\"\"\n",
    "PREFIX : <http://www.tafsirtabari.com/ontology#>\n",
    "PREFIX W3:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "SELECT DISTINCT *\n",
    "WHERE {\n",
    "\n",
    " ?k rdf:type :Chapter.\n",
    "?k :containsVerse ?v.\n",
    "?v :hasText ?t.\n",
    "}\n",
    "\"\"\"\n",
    "R = []\n",
    "# Execute the SPARQL query\n",
    "results = graph.query(query)\n",
    "# Process the query results\n",
    "for row in results:\n",
    "    # Access the query variables\n",
    "    s = row[\"k\"]\n",
    "    n = row[\"v\"]\n",
    "    t = row[\"t\"]\n",
    "    \n",
    "    print(f\"s: {s}, n: {n}, t: {t}\")\n",
    "    R.append(str(t))\n",
    "\n",
    "hadiths_from_graph = list(set(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d048d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8da357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d182f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
